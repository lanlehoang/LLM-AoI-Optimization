[
     {
          "algorithm": "Combine min-max normalized feature scores with a logistic sharpening, shift weights to favor arc length and processing rate, apply a milder queue penalty only for queues above a high-quantile threshold, center across valid neighbours and scale to small clipped offsets so the adjustments refine but do not overpower learned Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths : numpy arrays of same shape\n    Output:\n      - offsets : numpy array of same shape (float)\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    if not (distances.shape == arc_lengths.shape == processing_rates.shape == queue_lengths.shape):\n        raise ValueError(\"All inputs must have the same shape\")\n\n    # detect padded neighbours (all-zero feature rows)\n    padded = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n    valid = ~padded\n    offsets = np.zeros_like(distances, dtype=float)\n\n    if not np.any(valid):\n        return offsets\n\n    eps = 1e-8\n\n    def minmax_norm(x):\n        xv = x[valid]\n        mn = np.min(xv)\n        mx = np.max(xv)\n        if mx - mn < eps:\n            out = np.full_like(x, 0.5, dtype=float)\n            return out\n        out = np.zeros_like(x, dtype=float)\n        out[valid] = (x[valid] - mn) / (mx - mn + eps)\n        return out\n\n    # normalized such that higher is better\n    proc_score = minmax_norm(processing_rates)             # higher proc -> better\n    arc_score = 1.0 - minmax_norm(arc_lengths)             # smaller arc -> better\n    dist_score = 1.0 - minmax_norm(distances)              # smaller distance -> better\n    queue_score = 1.0 - minmax_norm(queue_lengths)         # smaller queue -> better\n\n    # weights: emphasize arc and processing rate more than queue (different from original)\n    w_queue = 0.35\n    w_arc = 0.35\n    w_proc = 0.20\n    w_dist = 0.10\n\n    combined = np.zeros_like(distances, dtype=float)\n    combined[valid] = (w_queue * queue_score[valid]\n                       + w_arc * arc_score[valid]\n                       + w_proc * proc_score[valid]\n                       + w_dist * dist_score[valid])\n\n    # logistic sharpening around center to increase contrast but keep range (0..1)\n    k = 3.0\n    s = np.zeros_like(combined)\n    # apply logistic only on valid entries\n    s[valid] = 1.0 / (1.0 + np.exp(-k * (combined[valid] - 0.5)))\n\n    # center across valid neighbours to produce relative preferences\n    mean_valid = np.mean(s[valid])\n    rel_score = np.zeros_like(s)\n    rel_score[valid] = s[valid] - mean_valid\n\n    # modest scale so offsets refine, not dominate\n    scale = 0.14\n    offsets[valid] = rel_score[valid] * scale\n\n    # extra penalty applied only when queue is high relative to high quantile baseline\n    q_valid = queue_lengths[valid]\n    q_thresh = max(1.0, np.percentile(q_valid, 85))\n    q_max = np.max(q_valid)\n    extra_penalty = np.zeros_like(offsets)\n    if q_max > q_thresh + eps:\n        # fractional excess mapped nonlinearly; stronger penalty for larger excesses\n        frac = np.zeros_like(q_valid, dtype=float)\n        frac_vals = (q_valid - q_thresh) / (q_max - q_thresh + eps)\n        frac_vals = np.clip(frac_vals, 0.0, 1.0)\n        # apply only where queue > threshold\n        frac[ q_valid > q_thresh ] = frac_vals[ q_valid > q_thresh ]\n        penalty_strength = 0.22\n        # exponent <2 to avoid overly aggressive punishments on moderate excess\n        extra_penalty_vals = -penalty_strength * (frac ** 1.5)\n        # map back to full array\n        extra_penalty_arr = np.zeros_like(offsets)\n        extra_penalty_arr[valid] = extra_penalty_vals\n        extra_penalty = extra_penalty_arr\n\n    offsets[valid] += extra_penalty[valid]\n\n    # final clamp to safe bounds\n    np.clip(offsets, -0.25, 0.25, out=offsets)\n\n    # ensure padded entries remain zero\n    offsets[padded] = 0.0\n\n    return offsets",
          "objective": 1.5606,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6079597192202714,
               "avg_dropped_ratio": 0.05120481927710843
          }
     },
     {
          "algorithm": "Combine normalized, validity-masked feature scores (favoring low queue, short arc, high processing rate, and short distance) with tuned weights, center them across available neighbours, and scale to small offsets so they refine \u2014 not overpower \u2014 learned Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths : numpy arrays of same shape\n    Output:\n      - offsets : numpy array of same shape (float)\n    \"\"\"\n    # Ensure arrays\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n    \n    # Shape check\n    if not (distances.shape == arc_lengths.shape == processing_rates.shape == queue_lengths.shape):\n        raise ValueError(\"All inputs must have the same shape\")\n    \n    # Detect padded/absent neighbours (all-zero feature rows)\n    padded = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n    valid = ~padded\n    offsets = np.zeros_like(distances, dtype=float)\n    \n    if not np.any(valid):\n        return offsets  # all padded -> zero offsets\n    \n    eps = 1e-8\n    \n    # Helper to compute normalized (0..1) with robust min/max over valid entries\n    def norm_positive_better(x):\n        xv = x[valid]\n        mn = np.min(xv)\n        mx = np.max(xv)\n        if mx - mn < eps:\n            return np.ones_like(x) * 0.5  # neutral when no variation\n        out = np.zeros_like(x, dtype=float)\n        out[valid] = (x[valid] - mn) / (mx - mn + eps)\n        return out\n    \n    def norm_negative_better(x):\n        # smaller values are better -> invert normalized positive-better\n        n = norm_positive_better(x)\n        return 1.0 - n\n    \n    # Compute normalized feature scores in [0,1], where higher is better\n    # - queue: smaller is better (strong effect)\n    queue_score = norm_negative_better(queue_lengths)\n    # - arc: smaller arc_length closer to destination -> better\n    arc_score = norm_negative_better(arc_lengths)\n    # - processing rate: higher is better\n    proc_score = norm_positive_better(processing_rates)\n    # - distance: smaller distance reduces propagation delay -> better (weaker effect)\n    dist_score = norm_negative_better(distances)\n    \n    # Weights (sum to 1) emphasizing queue, then arc, proc, dist\n    w_queue = 0.50\n    w_arc = 0.25\n    w_proc = 0.15\n    w_dist = 0.10\n    \n    # Combined score only for valid entries\n    combined = np.zeros_like(distances, dtype=float)\n    combined[valid] = (w_queue * queue_score[valid]\n                       + w_arc * arc_score[valid]\n                       + w_proc * proc_score[valid]\n                       + w_dist * dist_score[valid])\n    \n    # Center combined scores across valid neighbours to create relative preference\n    mean_valid = np.mean(combined[valid])\n    rel_score = np.zeros_like(combined)\n    rel_score[valid] = combined[valid] - mean_valid\n    \n    # Scale to modest offsets so we refine but don't overpower the agent Q-values\n    scale = 0.20  # typical offset magnitude (~[-0.2,0.2])\n    offsets[valid] = rel_score[valid] * scale\n    \n    # Extra nonlinear penalty for very large queues (to avoid drops) using squared term\n    # Determine a soft queue capacity baseline from observed valid queues\n    q_valid = queue_lengths[valid]\n    q_baseline = max(1.0, np.percentile(q_valid, 90))  # aggressive if queue near its 90th percentile\n    extra_penalty = np.zeros_like(offsets)\n    extra_penalty[valid] = -0.15 * ((q_valid / (q_baseline + eps)) ** 2)\n    # Apply penalty but keep offsets within reasonable bounds\n    offsets[valid] += extra_penalty[valid]\n    \n    # Clamp offsets to a safe range\n    np.clip(offsets, -0.3, 0.3, out=offsets)\n    \n    # Ensure padded entries remain zero (neutral); Q-agent should be -inf for them anyway\n    offsets[padded] = 0.0\n    \n    return offsets",
          "objective": 1.5035,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6130308463407801,
               "avg_dropped_ratio": 0.0783132530120482
          }
     },
     {
          "algorithm": "Combine smooth sigmoid-based per-feature utilities (favoring low queue and arc, high processing rate, and short distance), fuse them by weighted sum, center across valid neighbours to get relative preferences, scale modestly and apply a nonlinear queue overflow penalty to yield small offsets that refine agent Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths : numpy arrays of same shape\n    Output:\n      - offsets : numpy array of same shape (float)\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n    \n    if not (distances.shape == arc_lengths.shape == processing_rates.shape == queue_lengths.shape):\n        raise ValueError(\"All inputs must have the same shape\")\n    \n    # identify padded/absent neighbours (rows where all features are zero)\n    padded = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n    valid = ~padded\n    offsets = np.zeros_like(distances, dtype=float)\n    \n    if not np.any(valid):\n        return offsets\n    \n    eps = 1e-9\n    # Helper: robust center/scale for sigmoid transforms\n    def center_scale(xv):\n        # use median and a scale based on IQR to be robust\n        med = np.median(xv)\n        q75, q25 = np.percentile(xv, [75, 25])\n        iqr = q75 - q25\n        scale = iqr / 1.349  # approximate std from IQR\n        if scale < eps:\n            scale = np.std(xv) + eps\n        if scale < eps:\n            scale = 1.0\n        return med, scale\n    \n    # Sigmoid mapping functions:\n    # negative-better -> higher score when smaller: s = 1/(1+exp((x-center)/scale))\n    # positive-better -> higher score when larger: s = 1/(1+exp(-(x-center)/scale))\n    def neg_better_score(x):\n        out = np.zeros_like(x, dtype=float)\n        xv = x[valid]\n        c, s = center_scale(xv)\n        out[valid] = 1.0 / (1.0 + np.exp((xv - c) / (s + eps)))\n        return out\n    \n    def pos_better_score(x):\n        out = np.zeros_like(x, dtype=float)\n        xv = x[valid]\n        c, s = center_scale(xv)\n        out[valid] = 1.0 / (1.0 + np.exp(-(xv - c) / (s + eps)))\n        return out\n    \n    # Compute per-feature scores in (0,1)\n    queue_score = neg_better_score(queue_lengths)           # low queue -> high score\n    arc_score = neg_better_score(arc_lengths)               # small arc -> high score\n    proc_score = pos_better_score(processing_rates)         # high processing -> high score\n    dist_score = neg_better_score(distances)                # smaller distance -> high score\n    \n    # Weights emphasizing queue and arc length more, but not identical to previous\n    w_queue = 0.46\n    w_arc = 0.28\n    w_proc = 0.18\n    w_dist = 0.08\n    \n    # Weighted aggregate utility for valid entries\n    utility = np.zeros_like(distances, dtype=float)\n    utility[valid] = (w_queue * queue_score[valid]\n                      + w_arc * arc_score[valid]\n                      + w_proc * proc_score[valid]\n                      + w_dist * dist_score[valid])\n    \n    # Center to create relative preference (zero-mean across valid neighbours)\n    mean_util = np.mean(utility[valid])\n    rel = np.zeros_like(utility)\n    rel[valid] = utility[valid] - mean_util\n    \n    # Scale to modest offsets so we refine agent Q-values (keeps magnitude small)\n    base_scale = 0.22\n    offsets[valid] = rel[valid] * base_scale\n    \n    # Nonlinear extra penalty for large queues (avoid drop risk)\n    q_valid = queue_lengths[valid]\n    # Soft baseline capacity: median + 1.5*std (reasonable heuristic)\n    q_med = np.median(q_valid)\n    q_std = np.std(q_valid)\n    q_baseline = max(1.0, q_med + 1.5 * q_std)\n    # Penalty grows quadratically once queue exceeds baseline\n    penalty = np.zeros_like(offsets)\n    over = np.zeros_like(q_valid, dtype=bool)\n    if q_baseline > 0:\n        over = q_valid > q_baseline\n        rel_over = np.zeros_like(q_valid)\n        rel_over[over] = (q_valid[over] - q_baseline) / (q_baseline + eps)\n        penalty_vals = -0.20 * (rel_over ** 2)  # tuned magnitude small but significant\n        penalty[valid] = penalty_vals\n        offsets[valid] += penalty_vals\n    \n    # Small bias toward routes with zero queue (prefer empty queues slightly)\n    zero_queue_bonus = np.zeros_like(offsets)\n    zero_idx = (queue_lengths == 0) & valid\n    zero_queue_bonus[zero_idx] = +0.03\n    offsets += zero_queue_bonus\n    \n    # Clamp to safe bounds so offsets won't overpower agent Qs\n    np.clip(offsets, -0.35, 0.35, out=offsets)\n    \n    # Ensure padded entries remain zero\n    offsets[padded] = 0.0\n    \n    return offsets",
          "objective": 1.4976,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6174703893995901,
               "avg_dropped_ratio": 0.0753012048192771
          }
     },
     {
          "algorithm": "Combine robustly-normalized feature scores with adjusted weights, apply a tempered scaling, percentile-based sigmoid queue penalty, and a small closest-arc bonus, then center and clip to produce modest offsets that refine agent Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths : numpy arrays of same shape\n    Output:\n      - offsets : numpy array of same shape (float)\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n    \n    if not (distances.shape == arc_lengths.shape == processing_rates.shape == queue_lengths.shape):\n        raise ValueError(\"All inputs must have the same shape\")\n    \n    # Identify padded/absent neighbours (all-zero feature rows)\n    padded = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n    valid = ~padded\n    offsets = np.zeros_like(distances, dtype=float)\n    \n    if not np.any(valid):\n        return offsets\n    \n    eps = 1e-8\n    \n    # Normalizers: return arrays same shape, valid entries in [0,1], neutral 0.5 if no variation\n    def norm_positive_better(x):\n        out = np.zeros_like(x, dtype=float)\n        xv = x[valid]\n        mn = np.min(xv)\n        mx = np.max(xv)\n        if mx - mn < eps:\n            out[valid] = 0.5\n        else:\n            out[valid] = (x[valid] - mn) / (mx - mn + eps)\n        return out\n    \n    def norm_negative_better(x):\n        return 1.0 - norm_positive_better(x)\n    \n    # Compute scores in [0,1] where higher is better\n    queue_score = norm_negative_better(queue_lengths)    # lower queue better\n    arc_score = norm_negative_better(arc_lengths)        # closer to dest better\n    proc_score = norm_positive_better(processing_rates)  # higher proc better\n    dist_score = norm_negative_better(distances)         # closer neighbor better (weaker)\n    \n    # New weights (different from original): emphasize arc and processing a bit more\n    w_queue = 0.40\n    w_arc = 0.30\n    w_proc = 0.20\n    w_dist = 0.10\n    \n    combined = np.zeros_like(distances, dtype=float)\n    combined[valid] = (w_queue * queue_score[valid]\n                       + w_arc * arc_score[valid]\n                       + w_proc * proc_score[valid]\n                       + w_dist * dist_score[valid])\n    \n    # Center across valid neighbours to preserve relative preferences\n    mean_valid = np.mean(combined[valid])\n    rel_score = np.zeros_like(combined)\n    rel_score[valid] = combined[valid] - mean_valid\n    \n    # Tempered scaling so offsets are modest (smaller than original)\n    base_scale = 0.12  # typical offset magnitude ~[-0.12,0.12] before penalties/bonuses\n    offsets[valid] = rel_score[valid] * base_scale\n    \n    # Queue penalty: use percentile-based soft-capacity and a sigmoid to penalize aggressively near capacity\n    q_valid = queue_lengths[valid]\n    q_baseline = max(1.0, np.percentile(q_valid, 80))  # different percentile (80th)\n    # sigmoid shape param\n    k = 4.0\n    # compute sigmoid penalty in (0,1), then scale to negative offset\n    sig = 1.0 / (1.0 + np.exp(-k * ((q_valid / (q_baseline + eps)) - 1.0)))\n    extra_penalty = -0.18 * sig  # stronger negative when queue > baseline\n    offsets[valid] += extra_penalty\n    \n    # Small positive bonus for the neighbour(s) with minimum arc_length (closest to destination)\n    min_arc = np.min(arc_lengths[valid])\n    is_closest = (arc_lengths == min_arc) & valid\n    # If multiple equally closest, distribute small bonus among them\n    n_closest = max(1, np.sum(is_closest))\n    bonus_total = 0.03\n    offsets[is_closest] += (bonus_total / float(n_closest))\n    \n    # Ensure padded entries remain zero\n    offsets[padded] = 0.0\n    \n    # Clip to safe bounds so we don't overpower learned Q-values\n    np.clip(offsets, -0.25, 0.25, out=offsets)\n    \n    return offsets",
          "objective": 1.4969,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6197651901056138,
               "avg_dropped_ratio": 0.07228915662650602
          }
     }
]