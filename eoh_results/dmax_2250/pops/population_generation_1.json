[
     {
          "algorithm": "Combine normalized, validity-masked feature scores (favoring low queue, short arc, high processing rate, and short distance) with tuned weights, center them across available neighbours, and scale to small offsets so they refine \u2014 not overpower \u2014 learned Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths : numpy arrays of same shape\n    Output:\n      - offsets : numpy array of same shape (float)\n    \"\"\"\n    # Ensure arrays\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n    \n    # Shape check\n    if not (distances.shape == arc_lengths.shape == processing_rates.shape == queue_lengths.shape):\n        raise ValueError(\"All inputs must have the same shape\")\n    \n    # Detect padded/absent neighbours (all-zero feature rows)\n    padded = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n    valid = ~padded\n    offsets = np.zeros_like(distances, dtype=float)\n    \n    if not np.any(valid):\n        return offsets  # all padded -> zero offsets\n    \n    eps = 1e-8\n    \n    # Helper to compute normalized (0..1) with robust min/max over valid entries\n    def norm_positive_better(x):\n        xv = x[valid]\n        mn = np.min(xv)\n        mx = np.max(xv)\n        if mx - mn < eps:\n            return np.ones_like(x) * 0.5  # neutral when no variation\n        out = np.zeros_like(x, dtype=float)\n        out[valid] = (x[valid] - mn) / (mx - mn + eps)\n        return out\n    \n    def norm_negative_better(x):\n        # smaller values are better -> invert normalized positive-better\n        n = norm_positive_better(x)\n        return 1.0 - n\n    \n    # Compute normalized feature scores in [0,1], where higher is better\n    # - queue: smaller is better (strong effect)\n    queue_score = norm_negative_better(queue_lengths)\n    # - arc: smaller arc_length closer to destination -> better\n    arc_score = norm_negative_better(arc_lengths)\n    # - processing rate: higher is better\n    proc_score = norm_positive_better(processing_rates)\n    # - distance: smaller distance reduces propagation delay -> better (weaker effect)\n    dist_score = norm_negative_better(distances)\n    \n    # Weights (sum to 1) emphasizing queue, then arc, proc, dist\n    w_queue = 0.50\n    w_arc = 0.25\n    w_proc = 0.15\n    w_dist = 0.10\n    \n    # Combined score only for valid entries\n    combined = np.zeros_like(distances, dtype=float)\n    combined[valid] = (w_queue * queue_score[valid]\n                       + w_arc * arc_score[valid]\n                       + w_proc * proc_score[valid]\n                       + w_dist * dist_score[valid])\n    \n    # Center combined scores across valid neighbours to create relative preference\n    mean_valid = np.mean(combined[valid])\n    rel_score = np.zeros_like(combined)\n    rel_score[valid] = combined[valid] - mean_valid\n    \n    # Scale to modest offsets so we refine but don't overpower the agent Q-values\n    scale = 0.20  # typical offset magnitude (~[-0.2,0.2])\n    offsets[valid] = rel_score[valid] * scale\n    \n    # Extra nonlinear penalty for very large queues (to avoid drops) using squared term\n    # Determine a soft queue capacity baseline from observed valid queues\n    q_valid = queue_lengths[valid]\n    q_baseline = max(1.0, np.percentile(q_valid, 90))  # aggressive if queue near its 90th percentile\n    extra_penalty = np.zeros_like(offsets)\n    extra_penalty[valid] = -0.15 * ((q_valid / (q_baseline + eps)) ** 2)\n    # Apply penalty but keep offsets within reasonable bounds\n    offsets[valid] += extra_penalty[valid]\n    \n    # Clamp offsets to a safe range\n    np.clip(offsets, -0.3, 0.3, out=offsets)\n    \n    # Ensure padded entries remain zero (neutral); Q-agent should be -inf for them anyway\n    offsets[padded] = 0.0\n    \n    return offsets",
          "objective": 1.5035,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6130308463407801,
               "avg_dropped_ratio": 0.0783132530120482
          }
     },
     {
          "algorithm": "Combine smooth sigmoid-based per-feature utilities (favoring low queue and arc, high processing rate, and short distance), fuse them by weighted sum, center across valid neighbours to get relative preferences, scale modestly and apply a nonlinear queue overflow penalty to yield small offsets that refine agent Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths : numpy arrays of same shape\n    Output:\n      - offsets : numpy array of same shape (float)\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n    \n    if not (distances.shape == arc_lengths.shape == processing_rates.shape == queue_lengths.shape):\n        raise ValueError(\"All inputs must have the same shape\")\n    \n    # identify padded/absent neighbours (rows where all features are zero)\n    padded = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n    valid = ~padded\n    offsets = np.zeros_like(distances, dtype=float)\n    \n    if not np.any(valid):\n        return offsets\n    \n    eps = 1e-9\n    # Helper: robust center/scale for sigmoid transforms\n    def center_scale(xv):\n        # use median and a scale based on IQR to be robust\n        med = np.median(xv)\n        q75, q25 = np.percentile(xv, [75, 25])\n        iqr = q75 - q25\n        scale = iqr / 1.349  # approximate std from IQR\n        if scale < eps:\n            scale = np.std(xv) + eps\n        if scale < eps:\n            scale = 1.0\n        return med, scale\n    \n    # Sigmoid mapping functions:\n    # negative-better -> higher score when smaller: s = 1/(1+exp((x-center)/scale))\n    # positive-better -> higher score when larger: s = 1/(1+exp(-(x-center)/scale))\n    def neg_better_score(x):\n        out = np.zeros_like(x, dtype=float)\n        xv = x[valid]\n        c, s = center_scale(xv)\n        out[valid] = 1.0 / (1.0 + np.exp((xv - c) / (s + eps)))\n        return out\n    \n    def pos_better_score(x):\n        out = np.zeros_like(x, dtype=float)\n        xv = x[valid]\n        c, s = center_scale(xv)\n        out[valid] = 1.0 / (1.0 + np.exp(-(xv - c) / (s + eps)))\n        return out\n    \n    # Compute per-feature scores in (0,1)\n    queue_score = neg_better_score(queue_lengths)           # low queue -> high score\n    arc_score = neg_better_score(arc_lengths)               # small arc -> high score\n    proc_score = pos_better_score(processing_rates)         # high processing -> high score\n    dist_score = neg_better_score(distances)                # smaller distance -> high score\n    \n    # Weights emphasizing queue and arc length more, but not identical to previous\n    w_queue = 0.46\n    w_arc = 0.28\n    w_proc = 0.18\n    w_dist = 0.08\n    \n    # Weighted aggregate utility for valid entries\n    utility = np.zeros_like(distances, dtype=float)\n    utility[valid] = (w_queue * queue_score[valid]\n                      + w_arc * arc_score[valid]\n                      + w_proc * proc_score[valid]\n                      + w_dist * dist_score[valid])\n    \n    # Center to create relative preference (zero-mean across valid neighbours)\n    mean_util = np.mean(utility[valid])\n    rel = np.zeros_like(utility)\n    rel[valid] = utility[valid] - mean_util\n    \n    # Scale to modest offsets so we refine agent Q-values (keeps magnitude small)\n    base_scale = 0.22\n    offsets[valid] = rel[valid] * base_scale\n    \n    # Nonlinear extra penalty for large queues (avoid drop risk)\n    q_valid = queue_lengths[valid]\n    # Soft baseline capacity: median + 1.5*std (reasonable heuristic)\n    q_med = np.median(q_valid)\n    q_std = np.std(q_valid)\n    q_baseline = max(1.0, q_med + 1.5 * q_std)\n    # Penalty grows quadratically once queue exceeds baseline\n    penalty = np.zeros_like(offsets)\n    over = np.zeros_like(q_valid, dtype=bool)\n    if q_baseline > 0:\n        over = q_valid > q_baseline\n        rel_over = np.zeros_like(q_valid)\n        rel_over[over] = (q_valid[over] - q_baseline) / (q_baseline + eps)\n        penalty_vals = -0.20 * (rel_over ** 2)  # tuned magnitude small but significant\n        penalty[valid] = penalty_vals\n        offsets[valid] += penalty_vals\n    \n    # Small bias toward routes with zero queue (prefer empty queues slightly)\n    zero_queue_bonus = np.zeros_like(offsets)\n    zero_idx = (queue_lengths == 0) & valid\n    zero_queue_bonus[zero_idx] = +0.03\n    offsets += zero_queue_bonus\n    \n    # Clamp to safe bounds so offsets won't overpower agent Qs\n    np.clip(offsets, -0.35, 0.35, out=offsets)\n    \n    # Ensure padded entries remain zero\n    offsets[padded] = 0.0\n    \n    return offsets",
          "objective": 1.4976,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6174703893995901,
               "avg_dropped_ratio": 0.0753012048192771
          }
     },
     {
          "algorithm": "Combine robustly-normalized feature scores with adjusted weights, apply a tempered scaling, percentile-based sigmoid queue penalty, and a small closest-arc bonus, then center and clip to produce modest offsets that refine agent Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths : numpy arrays of same shape\n    Output:\n      - offsets : numpy array of same shape (float)\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n    \n    if not (distances.shape == arc_lengths.shape == processing_rates.shape == queue_lengths.shape):\n        raise ValueError(\"All inputs must have the same shape\")\n    \n    # Identify padded/absent neighbours (all-zero feature rows)\n    padded = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n    valid = ~padded\n    offsets = np.zeros_like(distances, dtype=float)\n    \n    if not np.any(valid):\n        return offsets\n    \n    eps = 1e-8\n    \n    # Normalizers: return arrays same shape, valid entries in [0,1], neutral 0.5 if no variation\n    def norm_positive_better(x):\n        out = np.zeros_like(x, dtype=float)\n        xv = x[valid]\n        mn = np.min(xv)\n        mx = np.max(xv)\n        if mx - mn < eps:\n            out[valid] = 0.5\n        else:\n            out[valid] = (x[valid] - mn) / (mx - mn + eps)\n        return out\n    \n    def norm_negative_better(x):\n        return 1.0 - norm_positive_better(x)\n    \n    # Compute scores in [0,1] where higher is better\n    queue_score = norm_negative_better(queue_lengths)    # lower queue better\n    arc_score = norm_negative_better(arc_lengths)        # closer to dest better\n    proc_score = norm_positive_better(processing_rates)  # higher proc better\n    dist_score = norm_negative_better(distances)         # closer neighbor better (weaker)\n    \n    # New weights (different from original): emphasize arc and processing a bit more\n    w_queue = 0.40\n    w_arc = 0.30\n    w_proc = 0.20\n    w_dist = 0.10\n    \n    combined = np.zeros_like(distances, dtype=float)\n    combined[valid] = (w_queue * queue_score[valid]\n                       + w_arc * arc_score[valid]\n                       + w_proc * proc_score[valid]\n                       + w_dist * dist_score[valid])\n    \n    # Center across valid neighbours to preserve relative preferences\n    mean_valid = np.mean(combined[valid])\n    rel_score = np.zeros_like(combined)\n    rel_score[valid] = combined[valid] - mean_valid\n    \n    # Tempered scaling so offsets are modest (smaller than original)\n    base_scale = 0.12  # typical offset magnitude ~[-0.12,0.12] before penalties/bonuses\n    offsets[valid] = rel_score[valid] * base_scale\n    \n    # Queue penalty: use percentile-based soft-capacity and a sigmoid to penalize aggressively near capacity\n    q_valid = queue_lengths[valid]\n    q_baseline = max(1.0, np.percentile(q_valid, 80))  # different percentile (80th)\n    # sigmoid shape param\n    k = 4.0\n    # compute sigmoid penalty in (0,1), then scale to negative offset\n    sig = 1.0 / (1.0 + np.exp(-k * ((q_valid / (q_baseline + eps)) - 1.0)))\n    extra_penalty = -0.18 * sig  # stronger negative when queue > baseline\n    offsets[valid] += extra_penalty\n    \n    # Small positive bonus for the neighbour(s) with minimum arc_length (closest to destination)\n    min_arc = np.min(arc_lengths[valid])\n    is_closest = (arc_lengths == min_arc) & valid\n    # If multiple equally closest, distribute small bonus among them\n    n_closest = max(1, np.sum(is_closest))\n    bonus_total = 0.03\n    offsets[is_closest] += (bonus_total / float(n_closest))\n    \n    # Ensure padded entries remain zero\n    offsets[padded] = 0.0\n    \n    # Clip to safe bounds so we don't overpower learned Q-values\n    np.clip(offsets, -0.25, 0.25, out=offsets)\n    \n    return offsets",
          "objective": 1.4969,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6197651901056138,
               "avg_dropped_ratio": 0.07228915662650602
          }
     },
     {
          "algorithm": "",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths : numpy arrays of same shape\n    Output:\n      - offsets : numpy array of same shape (float)\n    \"\"\"\n    # Ensure arrays\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n    \n    # Shape check\n    if not (distances.shape == arc_lengths.shape == processing_rates.shape == queue_lengths.shape):\n        raise ValueError(\"All inputs must have the same shape\")\n    \n    # Detect padded/absent neighbours (all-zero feature rows)\n    padded = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n    valid = ~padded\n    offsets = np.zeros_like(distances, dtype=float)\n    \n    if not np.any(valid):\n        return offsets  # all padded -> zero offsets\n    \n    eps = 1e-8\n    \n    # Use simple, monotonic transforms that do not depend on per-call min/max\n    # Map features to (0,1], higher is better for all scores\n    # - small queue is better -> 1/(1+q)\n    # - small arc_length is better -> 1/(1+arc)\n    # - high processing rate is better -> r/(1+r)\n    # - small distance is better -> 1/(1+d)\n    q = np.maximum(queue_lengths, 0.0)\n    a = np.maximum(arc_lengths, 0.0)\n    r = np.maximum(processing_rates, 0.0)\n    d = np.maximum(distances, 0.0)\n    \n    score_queue = np.zeros_like(q, dtype=float)\n    score_arc = np.zeros_like(a, dtype=float)\n    score_proc = np.zeros_like(r, dtype=float)\n    score_dist = np.zeros_like(d, dtype=float)\n    \n    score_queue[valid] = 1.0 / (1.0 + q[valid])\n    score_arc[valid] = 1.0 / (1.0 + a[valid])\n    score_proc[valid] = r[valid] / (1.0 + r[valid] + eps)\n    score_dist[valid] = 1.0 / (1.0 + d[valid])\n    \n    # Fixed weights (sum to 1) without per-call re-centering\n    w_queue = 0.50\n    w_arc = 0.25\n    w_proc = 0.15\n    w_dist = 0.10\n    \n    combined = np.zeros_like(distances, dtype=float)\n    combined[valid] = (w_queue * score_queue[valid]\n                       + w_arc * score_arc[valid]\n                       + w_proc * score_proc[valid]\n                       + w_dist * score_dist[valid])\n    \n    # Map combined score in [0,1] to symmetric offset around 0 with fixed scale\n    # combined == 0.5 -> offset 0; extremes map to +/- scale_limit\n    base_scale = 0.15  # yields possible range approximately +/-0.3 after factor 2\n    offsets[valid] = (combined[valid] - 0.5) * 2.0 * base_scale\n    \n    # Clamp offsets to a safe fixed range\n    np.clip(offsets, -0.3, 0.3, out=offsets)\n    \n    # Ensure padded entries remain zero\n    offsets[padded] = 0.0\n    \n    return offsets",
          "objective": 1.4846,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6096691782276525,
               "avg_dropped_ratio": 0.09487951807228916
          }
     }
]