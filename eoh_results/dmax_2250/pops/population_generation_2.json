[
     {
          "algorithm": "Combine normalized, validity-masked feature scores (favoring low queue, short arc, high processing rate, and short distance) with tuned weights, center them across available neighbours, and scale to small offsets so they refine \u2014 not overpower \u2014 learned Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths : numpy arrays of same shape\n    Output:\n      - offsets : numpy array of same shape (float)\n    \"\"\"\n    # Ensure arrays\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n    \n    # Shape check\n    if not (distances.shape == arc_lengths.shape == processing_rates.shape == queue_lengths.shape):\n        raise ValueError(\"All inputs must have the same shape\")\n    \n    # Detect padded/absent neighbours (all-zero feature rows)\n    padded = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n    valid = ~padded\n    offsets = np.zeros_like(distances, dtype=float)\n    \n    if not np.any(valid):\n        return offsets  # all padded -> zero offsets\n    \n    eps = 1e-8\n    \n    # Helper to compute normalized (0..1) with robust min/max over valid entries\n    def norm_positive_better(x):\n        xv = x[valid]\n        mn = np.min(xv)\n        mx = np.max(xv)\n        if mx - mn < eps:\n            return np.ones_like(x) * 0.5  # neutral when no variation\n        out = np.zeros_like(x, dtype=float)\n        out[valid] = (x[valid] - mn) / (mx - mn + eps)\n        return out\n    \n    def norm_negative_better(x):\n        # smaller values are better -> invert normalized positive-better\n        n = norm_positive_better(x)\n        return 1.0 - n\n    \n    # Compute normalized feature scores in [0,1], where higher is better\n    # - queue: smaller is better (strong effect)\n    queue_score = norm_negative_better(queue_lengths)\n    # - arc: smaller arc_length closer to destination -> better\n    arc_score = norm_negative_better(arc_lengths)\n    # - processing rate: higher is better\n    proc_score = norm_positive_better(processing_rates)\n    # - distance: smaller distance reduces propagation delay -> better (weaker effect)\n    dist_score = norm_negative_better(distances)\n    \n    # Weights (sum to 1) emphasizing queue, then arc, proc, dist\n    w_queue = 0.50\n    w_arc = 0.25\n    w_proc = 0.15\n    w_dist = 0.10\n    \n    # Combined score only for valid entries\n    combined = np.zeros_like(distances, dtype=float)\n    combined[valid] = (w_queue * queue_score[valid]\n                       + w_arc * arc_score[valid]\n                       + w_proc * proc_score[valid]\n                       + w_dist * dist_score[valid])\n    \n    # Center combined scores across valid neighbours to create relative preference\n    mean_valid = np.mean(combined[valid])\n    rel_score = np.zeros_like(combined)\n    rel_score[valid] = combined[valid] - mean_valid\n    \n    # Scale to modest offsets so we refine but don't overpower the agent Q-values\n    scale = 0.20  # typical offset magnitude (~[-0.2,0.2])\n    offsets[valid] = rel_score[valid] * scale\n    \n    # Extra nonlinear penalty for very large queues (to avoid drops) using squared term\n    # Determine a soft queue capacity baseline from observed valid queues\n    q_valid = queue_lengths[valid]\n    q_baseline = max(1.0, np.percentile(q_valid, 90))  # aggressive if queue near its 90th percentile\n    extra_penalty = np.zeros_like(offsets)\n    extra_penalty[valid] = -0.15 * ((q_valid / (q_baseline + eps)) ** 2)\n    # Apply penalty but keep offsets within reasonable bounds\n    offsets[valid] += extra_penalty[valid]\n    \n    # Clamp offsets to a safe range\n    np.clip(offsets, -0.3, 0.3, out=offsets)\n    \n    # Ensure padded entries remain zero (neutral); Q-agent should be -inf for them anyway\n    offsets[padded] = 0.0\n    \n    return offsets",
          "objective": 1.5035,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6130308463407801,
               "avg_dropped_ratio": 0.0783132530120482
          }
     },
     {
          "algorithm": "Combine smooth sigmoid-based per-feature utilities (favoring low queue and arc, high processing rate, and short distance), fuse them by weighted sum, center across valid neighbours to get relative preferences, scale modestly and apply a nonlinear queue overflow penalty to yield small offsets that refine agent Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths : numpy arrays of same shape\n    Output:\n      - offsets : numpy array of same shape (float)\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n    \n    if not (distances.shape == arc_lengths.shape == processing_rates.shape == queue_lengths.shape):\n        raise ValueError(\"All inputs must have the same shape\")\n    \n    # identify padded/absent neighbours (rows where all features are zero)\n    padded = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n    valid = ~padded\n    offsets = np.zeros_like(distances, dtype=float)\n    \n    if not np.any(valid):\n        return offsets\n    \n    eps = 1e-9\n    # Helper: robust center/scale for sigmoid transforms\n    def center_scale(xv):\n        # use median and a scale based on IQR to be robust\n        med = np.median(xv)\n        q75, q25 = np.percentile(xv, [75, 25])\n        iqr = q75 - q25\n        scale = iqr / 1.349  # approximate std from IQR\n        if scale < eps:\n            scale = np.std(xv) + eps\n        if scale < eps:\n            scale = 1.0\n        return med, scale\n    \n    # Sigmoid mapping functions:\n    # negative-better -> higher score when smaller: s = 1/(1+exp((x-center)/scale))\n    # positive-better -> higher score when larger: s = 1/(1+exp(-(x-center)/scale))\n    def neg_better_score(x):\n        out = np.zeros_like(x, dtype=float)\n        xv = x[valid]\n        c, s = center_scale(xv)\n        out[valid] = 1.0 / (1.0 + np.exp((xv - c) / (s + eps)))\n        return out\n    \n    def pos_better_score(x):\n        out = np.zeros_like(x, dtype=float)\n        xv = x[valid]\n        c, s = center_scale(xv)\n        out[valid] = 1.0 / (1.0 + np.exp(-(xv - c) / (s + eps)))\n        return out\n    \n    # Compute per-feature scores in (0,1)\n    queue_score = neg_better_score(queue_lengths)           # low queue -> high score\n    arc_score = neg_better_score(arc_lengths)               # small arc -> high score\n    proc_score = pos_better_score(processing_rates)         # high processing -> high score\n    dist_score = neg_better_score(distances)                # smaller distance -> high score\n    \n    # Weights emphasizing queue and arc length more, but not identical to previous\n    w_queue = 0.46\n    w_arc = 0.28\n    w_proc = 0.18\n    w_dist = 0.08\n    \n    # Weighted aggregate utility for valid entries\n    utility = np.zeros_like(distances, dtype=float)\n    utility[valid] = (w_queue * queue_score[valid]\n                      + w_arc * arc_score[valid]\n                      + w_proc * proc_score[valid]\n                      + w_dist * dist_score[valid])\n    \n    # Center to create relative preference (zero-mean across valid neighbours)\n    mean_util = np.mean(utility[valid])\n    rel = np.zeros_like(utility)\n    rel[valid] = utility[valid] - mean_util\n    \n    # Scale to modest offsets so we refine agent Q-values (keeps magnitude small)\n    base_scale = 0.22\n    offsets[valid] = rel[valid] * base_scale\n    \n    # Nonlinear extra penalty for large queues (avoid drop risk)\n    q_valid = queue_lengths[valid]\n    # Soft baseline capacity: median + 1.5*std (reasonable heuristic)\n    q_med = np.median(q_valid)\n    q_std = np.std(q_valid)\n    q_baseline = max(1.0, q_med + 1.5 * q_std)\n    # Penalty grows quadratically once queue exceeds baseline\n    penalty = np.zeros_like(offsets)\n    over = np.zeros_like(q_valid, dtype=bool)\n    if q_baseline > 0:\n        over = q_valid > q_baseline\n        rel_over = np.zeros_like(q_valid)\n        rel_over[over] = (q_valid[over] - q_baseline) / (q_baseline + eps)\n        penalty_vals = -0.20 * (rel_over ** 2)  # tuned magnitude small but significant\n        penalty[valid] = penalty_vals\n        offsets[valid] += penalty_vals\n    \n    # Small bias toward routes with zero queue (prefer empty queues slightly)\n    zero_queue_bonus = np.zeros_like(offsets)\n    zero_idx = (queue_lengths == 0) & valid\n    zero_queue_bonus[zero_idx] = +0.03\n    offsets += zero_queue_bonus\n    \n    # Clamp to safe bounds so offsets won't overpower agent Qs\n    np.clip(offsets, -0.35, 0.35, out=offsets)\n    \n    # Ensure padded entries remain zero\n    offsets[padded] = 0.0\n    \n    return offsets",
          "objective": 1.4976,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6174703893995901,
               "avg_dropped_ratio": 0.0753012048192771
          }
     },
     {
          "algorithm": "Combine robustly-normalized feature scores with adjusted weights, apply a tempered scaling, percentile-based sigmoid queue penalty, and a small closest-arc bonus, then center and clip to produce modest offsets that refine agent Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths : numpy arrays of same shape\n    Output:\n      - offsets : numpy array of same shape (float)\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n    \n    if not (distances.shape == arc_lengths.shape == processing_rates.shape == queue_lengths.shape):\n        raise ValueError(\"All inputs must have the same shape\")\n    \n    # Identify padded/absent neighbours (all-zero feature rows)\n    padded = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n    valid = ~padded\n    offsets = np.zeros_like(distances, dtype=float)\n    \n    if not np.any(valid):\n        return offsets\n    \n    eps = 1e-8\n    \n    # Normalizers: return arrays same shape, valid entries in [0,1], neutral 0.5 if no variation\n    def norm_positive_better(x):\n        out = np.zeros_like(x, dtype=float)\n        xv = x[valid]\n        mn = np.min(xv)\n        mx = np.max(xv)\n        if mx - mn < eps:\n            out[valid] = 0.5\n        else:\n            out[valid] = (x[valid] - mn) / (mx - mn + eps)\n        return out\n    \n    def norm_negative_better(x):\n        return 1.0 - norm_positive_better(x)\n    \n    # Compute scores in [0,1] where higher is better\n    queue_score = norm_negative_better(queue_lengths)    # lower queue better\n    arc_score = norm_negative_better(arc_lengths)        # closer to dest better\n    proc_score = norm_positive_better(processing_rates)  # higher proc better\n    dist_score = norm_negative_better(distances)         # closer neighbor better (weaker)\n    \n    # New weights (different from original): emphasize arc and processing a bit more\n    w_queue = 0.40\n    w_arc = 0.30\n    w_proc = 0.20\n    w_dist = 0.10\n    \n    combined = np.zeros_like(distances, dtype=float)\n    combined[valid] = (w_queue * queue_score[valid]\n                       + w_arc * arc_score[valid]\n                       + w_proc * proc_score[valid]\n                       + w_dist * dist_score[valid])\n    \n    # Center across valid neighbours to preserve relative preferences\n    mean_valid = np.mean(combined[valid])\n    rel_score = np.zeros_like(combined)\n    rel_score[valid] = combined[valid] - mean_valid\n    \n    # Tempered scaling so offsets are modest (smaller than original)\n    base_scale = 0.12  # typical offset magnitude ~[-0.12,0.12] before penalties/bonuses\n    offsets[valid] = rel_score[valid] * base_scale\n    \n    # Queue penalty: use percentile-based soft-capacity and a sigmoid to penalize aggressively near capacity\n    q_valid = queue_lengths[valid]\n    q_baseline = max(1.0, np.percentile(q_valid, 80))  # different percentile (80th)\n    # sigmoid shape param\n    k = 4.0\n    # compute sigmoid penalty in (0,1), then scale to negative offset\n    sig = 1.0 / (1.0 + np.exp(-k * ((q_valid / (q_baseline + eps)) - 1.0)))\n    extra_penalty = -0.18 * sig  # stronger negative when queue > baseline\n    offsets[valid] += extra_penalty\n    \n    # Small positive bonus for the neighbour(s) with minimum arc_length (closest to destination)\n    min_arc = np.min(arc_lengths[valid])\n    is_closest = (arc_lengths == min_arc) & valid\n    # If multiple equally closest, distribute small bonus among them\n    n_closest = max(1, np.sum(is_closest))\n    bonus_total = 0.03\n    offsets[is_closest] += (bonus_total / float(n_closest))\n    \n    # Ensure padded entries remain zero\n    offsets[padded] = 0.0\n    \n    # Clip to safe bounds so we don't overpower learned Q-values\n    np.clip(offsets, -0.25, 0.25, out=offsets)\n    \n    return offsets",
          "objective": 1.4969,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6197651901056138,
               "avg_dropped_ratio": 0.07228915662650602
          }
     },
     {
          "algorithm": "Combine robust sigmoid-normalized feature utilities into a weighted preference, center them across available neighbours, add a steep cubic penalty for extremely congested queues, and scale/clamp the result to produce small offsets that refine the agent's Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths : numpy arrays of same shape\n    Output:\n      - offsets : numpy array of same shape (float)\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n    \n    if not (distances.shape == arc_lengths.shape == processing_rates.shape == queue_lengths.shape):\n        raise ValueError(\"All inputs must have the same shape\")\n    \n    # detect padded neighbours (all features zero)\n    padded = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n    valid = ~padded\n    offsets = np.zeros_like(distances, dtype=float)\n    \n    if not np.any(valid):\n        return offsets  # nothing valid\n    \n    num_valid = int(np.sum(valid))\n    # If only one valid neighbour, no meaningful preference -> zero offsets\n    if num_valid <= 1:\n        offsets[valid] = 0.0\n        return offsets\n    \n    eps = 1e-8\n    \n    # Robust scale estimate: IQR/1.349 approximates std for normal; fallback to max-min\n    def robust_scale(xv):\n        xv = np.asarray(xv, dtype=float)\n        q75, q25 = np.percentile(xv, [75, 25])\n        iqr = q75 - q25\n        scale = iqr / 1.349\n        if scale < eps:\n            scale = max(np.std(xv), (np.max(xv) - np.min(xv)) / 2.0, eps)\n        return scale\n    \n    # Sigmoid utility mapping to (0,1): higher is better\n    def sigmoid_util(x, higher_is_better=True):\n        xv = x[valid]\n        med = np.median(xv)\n        scale = robust_scale(xv)\n        # choose sign so that larger utility is better\n        sign = 1.0 if higher_is_better else -1.0\n        # Shift so that median maps to ~0.5, scale controls steepness\n        z = sign * (x - med) / (scale + eps)\n        # numerically stable logistic\n        util = np.zeros_like(x, dtype=float)\n        util[valid] = 1.0 / (1.0 + np.exp(-z[valid]))\n        return util\n    \n    # Compute utilities where larger is better\n    queue_util = sigmoid_util(queue_lengths, higher_is_better=False)   # smaller queue => higher util\n    arc_util   = sigmoid_util(arc_lengths, higher_is_better=False)     # smaller arc => higher util\n    proc_util  = sigmoid_util(processing_rates, higher_is_better=True) # higher processing => higher util\n    dist_util  = sigmoid_util(distances, higher_is_better=False)       # smaller distance => higher util (weaker)\n    \n    # Weights emphasizing queue and arc, but allow interaction\n    w_queue = 0.45\n    w_arc   = 0.30\n    w_proc  = 0.15\n    w_dist  = 0.10\n    \n    combined = np.zeros_like(distances, dtype=float)\n    combined[valid] = (w_queue * queue_util[valid] +\n                       w_arc   * arc_util[valid] +\n                       w_proc  * proc_util[valid] +\n                       w_dist  * dist_util[valid])\n    \n    # Center across valid neighbours to form relative preference\n    mean_val = np.mean(combined[valid])\n    rel = np.zeros_like(combined, dtype=float)\n    rel[valid] = combined[valid] - mean_val\n    \n    # Base scaling to keep offsets modest (tunable)\n    base_scale = 0.18  # typical resulting offsets ~[-0.18,0.18] before extra penalties\n    \n    offsets[valid] = rel[valid] * base_scale\n    \n    # Strong nonlinear penalty for excessively large queues relative to observed baseline\n    q_valid = queue_lengths[valid]\n    q_baseline = max(1.0, np.percentile(q_valid,  eighty_five:=85))  # 85th percentile baseline\n    # cubic penalty shape for heavy congestion (only applies when queue > baseline)\n    overload = np.clip((q_valid - q_baseline) / (q_baseline + eps), 0.0, None)\n    extra_penalty = -0.14 * (overload ** 3)  # strongly negative for extreme overloads\n    # apply penalty to valid entries\n    offsets[valid] += extra_penalty\n    \n    # If variance of combined is extremely small, fall back to zero offsets (no discrimination)\n    if np.std(combined[valid]) < 1e-4:\n        offsets[valid] = 0.0\n    \n    # Clamp to safe bounds so offsets refine but do not overpower learned Q-values\n    np.clip(offsets, -0.28, 0.28, out=offsets)\n    \n    # Ensure padded remain exactly zero\n    offsets[padded] = 0.0\n    \n    return offsets",
          "objective": 1.4896,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6318882871569873,
               "avg_dropped_ratio": 0.058734939759036146
          }
     }
]