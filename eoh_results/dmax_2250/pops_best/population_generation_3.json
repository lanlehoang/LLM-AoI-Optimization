{
     "algorithm": "Combine min-max normalized feature scores with a logistic sharpening, shift weights to favor arc length and processing rate, apply a milder queue penalty only for queues above a high-quantile threshold, center across valid neighbours and scale to small clipped offsets so the adjustments refine but do not overpower learned Q-values.",
     "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths : numpy arrays of same shape\n    Output:\n      - offsets : numpy array of same shape (float)\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    if not (distances.shape == arc_lengths.shape == processing_rates.shape == queue_lengths.shape):\n        raise ValueError(\"All inputs must have the same shape\")\n\n    # detect padded neighbours (all-zero feature rows)\n    padded = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n    valid = ~padded\n    offsets = np.zeros_like(distances, dtype=float)\n\n    if not np.any(valid):\n        return offsets\n\n    eps = 1e-8\n\n    def minmax_norm(x):\n        xv = x[valid]\n        mn = np.min(xv)\n        mx = np.max(xv)\n        if mx - mn < eps:\n            out = np.full_like(x, 0.5, dtype=float)\n            return out\n        out = np.zeros_like(x, dtype=float)\n        out[valid] = (x[valid] - mn) / (mx - mn + eps)\n        return out\n\n    # normalized such that higher is better\n    proc_score = minmax_norm(processing_rates)             # higher proc -> better\n    arc_score = 1.0 - minmax_norm(arc_lengths)             # smaller arc -> better\n    dist_score = 1.0 - minmax_norm(distances)              # smaller distance -> better\n    queue_score = 1.0 - minmax_norm(queue_lengths)         # smaller queue -> better\n\n    # weights: emphasize arc and processing rate more than queue (different from original)\n    w_queue = 0.35\n    w_arc = 0.35\n    w_proc = 0.20\n    w_dist = 0.10\n\n    combined = np.zeros_like(distances, dtype=float)\n    combined[valid] = (w_queue * queue_score[valid]\n                       + w_arc * arc_score[valid]\n                       + w_proc * proc_score[valid]\n                       + w_dist * dist_score[valid])\n\n    # logistic sharpening around center to increase contrast but keep range (0..1)\n    k = 3.0\n    s = np.zeros_like(combined)\n    # apply logistic only on valid entries\n    s[valid] = 1.0 / (1.0 + np.exp(-k * (combined[valid] - 0.5)))\n\n    # center across valid neighbours to produce relative preferences\n    mean_valid = np.mean(s[valid])\n    rel_score = np.zeros_like(s)\n    rel_score[valid] = s[valid] - mean_valid\n\n    # modest scale so offsets refine, not dominate\n    scale = 0.14\n    offsets[valid] = rel_score[valid] * scale\n\n    # extra penalty applied only when queue is high relative to high quantile baseline\n    q_valid = queue_lengths[valid]\n    q_thresh = max(1.0, np.percentile(q_valid, 85))\n    q_max = np.max(q_valid)\n    extra_penalty = np.zeros_like(offsets)\n    if q_max > q_thresh + eps:\n        # fractional excess mapped nonlinearly; stronger penalty for larger excesses\n        frac = np.zeros_like(q_valid, dtype=float)\n        frac_vals = (q_valid - q_thresh) / (q_max - q_thresh + eps)\n        frac_vals = np.clip(frac_vals, 0.0, 1.0)\n        # apply only where queue > threshold\n        frac[ q_valid > q_thresh ] = frac_vals[ q_valid > q_thresh ]\n        penalty_strength = 0.22\n        # exponent <2 to avoid overly aggressive punishments on moderate excess\n        extra_penalty_vals = -penalty_strength * (frac ** 1.5)\n        # map back to full array\n        extra_penalty_arr = np.zeros_like(offsets)\n        extra_penalty_arr[valid] = extra_penalty_vals\n        extra_penalty = extra_penalty_arr\n\n    offsets[valid] += extra_penalty[valid]\n\n    # final clamp to safe bounds\n    np.clip(offsets, -0.25, 0.25, out=offsets)\n\n    # ensure padded entries remain zero\n    offsets[padded] = 0.0\n\n    return offsets",
     "objective": 1.5606,
     "other_inf": null,
     "eval_metrics": {
          "avg_aoi": 0.6079597192202714,
          "avg_dropped_ratio": 0.05120481927710843
     }
}