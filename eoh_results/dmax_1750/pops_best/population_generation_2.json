{
     "algorithm": "Create a bounded offset by combining a multiplicative \"close-and-fast\" bonus with separate nonlinear penalties for queue congestion and propagation distance, using robust scale estimates (medians/max) so the offset remains modest and well-behaved across zero-padded neighbors.",
     "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n        distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n        offsets: numpy array of same shape with small signed offsets to add to agent Q-values\n    \"\"\"\n    eps = 1e-9\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    # Robust scale estimates (ignore exact zeros when appropriate to handle padded neighbors)\n    pos_dist = distances[distances > 0]\n    pos_arc = arc_lengths[arc_lengths > 0]\n    pos_proc = processing_rates[processing_rates > 0]\n    pos_queue = queue_lengths[queue_lengths > 0]\n\n    # Fallback scales to avoid division by zero; choose typical magnitudes (km, pkts/s, pkts)\n    dist_scale = float(np.median(pos_dist)) if pos_dist.size > 0 else 1000.0\n    arc_scale = float(np.median(pos_arc)) if pos_arc.size > 0 else 2000.0\n    proc_max = float(np.max(processing_rates)) if processing_rates.size > 0 else 1.0\n    proc_max = max(proc_max, 1.0)\n    queue_scale = float(np.median(pos_queue)) if pos_queue.size > 0 else 4.0\n    queue_scale = max(queue_scale, 1.0)\n\n    # Nonlinear feature transforms giving values in [0,1] (higher = better for arc/proc, worse for queue/dist)\n    # Closeness to destination: exponential decay (1 when very close, ->0 when far)\n    arc_score = np.exp(-arc_lengths / (arc_scale + eps))\n\n    # Processing: diminishing returns via log1p, normalized by log1p(max)\n    proc_score = np.log1p(processing_rates) / np.log1p(proc_max + eps)\n\n    # Queue congestion: saturating penalty in [0,1]\n    queue_score = queue_lengths / (queue_lengths + queue_scale + eps)\n\n    # Propagation distance penalty: fraction in [0,1]\n    dist_score = distances / (distances + dist_scale + eps)\n\n    # Interaction: prefer neighbours that are both close (to dest) and fast\n    interaction = arc_score * proc_score\n\n    # Weights emphasize interaction and queue penalty but keep influence modest\n    w_int = 0.55\n    w_arc = 0.15\n    w_proc = 0.15\n    w_queue = 0.95\n    w_dist = 0.25\n\n    combined = (w_int * interaction) + (w_arc * arc_score) + (w_proc * proc_score) - (w_queue * queue_score) - (w_dist * dist_score)\n\n    # Normalize combined to roughly [-1,1] by dividing by sum of absolute positive/negative weight contributions\n    # Compute an approximate normalizer using maxima of component ranges\n    max_pos = (w_int * 1.0) + (w_arc * 1.0) + (w_proc * 1.0)  # when positive terms are max\n    max_neg = (w_queue * 1.0) + (w_dist * 1.0)               # when penalties are max\n    normalizer = max(max_pos, max_neg, eps)\n\n    normalized = combined / (normalizer + eps)  # now roughly in [-1,1]\n\n    # Final scaling: keep offsets modest relative to typical Q-values (~\u00b10.5), and clip to safe bounds\n    scale = 0.07\n    offsets = scale * normalized\n    offsets = np.clip(offsets, -0.12, 0.12)\n\n    return offsets",
     "objective": 1.217,
     "other_inf": null,
     "eval_metrics": {
          "avg_aoi": 0.6076311033087556,
          "avg_dropped_ratio": 0.26054216867469876
     }
}