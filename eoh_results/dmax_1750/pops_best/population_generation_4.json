{
     "algorithm": "Combine normalized closeness and service-speed bonuses with stronger nonlinear queue and propagation penalties using median-based robust scaling and modest clipping to keep offsets small.",
     "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n        distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n        offsets: numpy array of same shape with small signed offsets to add to agent Q-values\n    \"\"\"\n    eps = 1e-9\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    # Mask positive (non-zero) entries for robust statistics (handle zero-padding)\n    pos_dist = distances[distances > 0]\n    pos_arc = arc_lengths[arc_lengths > 0]\n    pos_proc = processing_rates[processing_rates > 0]\n    pos_queue = queue_lengths[queue_lengths > 0]\n\n    # Robust fallback scales\n    dist_scale = float(np.median(pos_dist)) if pos_dist.size > 0 else 1000.0\n    arc_scale = float(np.median(pos_arc)) if pos_arc.size > 0 else 2000.0\n    proc_max = float(np.max(processing_rates)) if processing_rates.size > 0 else 1.0\n    proc_max = max(proc_max, 1.0)\n    queue_scale = float(np.median(pos_queue)) if pos_queue.size > 0 else 4.0\n    queue_scale = max(queue_scale, 1.0)\n\n    # Propagation delay (seconds) using speed of light (km/s)\n    c_km_s = 299792.458\n    prop_delay = distances / (c_km_s + eps)\n    median_delay = dist_scale / (c_km_s + eps)\n\n    # Feature transforms mapped roughly to [0,1]\n    # Closeness to destination: reciprocal form (1 when arc is tiny)\n    arc_score = 1.0 / (1.0 + (arc_lengths / (arc_scale + eps)))\n\n    # Processing: normalized then square-root for diminishing returns\n    proc_score = np.sqrt(processing_rates / (proc_max + eps))\n\n    # Queue congestion: saturating penalty in [0,1] (higher = worse)\n    queue_score = queue_lengths / (queue_lengths + (queue_scale * 1.5) + eps)\n\n    # Delay (distance) penalty in [0,1]\n    delay_score = prop_delay / (prop_delay + (median_delay + eps))\n\n    # Interaction bonus: prefer close-to-destination AND fast service, down-weighted by queue\n    interaction = arc_score * proc_score\n    interaction = interaction * (1.0 - queue_score)  # damp interaction if queue is bad\n\n    # Weights (different from original): emphasize interaction and queue penalty, moderate delay\n    w_int = 0.60\n    w_arc = 0.20\n    w_proc = 0.10\n    w_queue = 1.10\n    w_delay = 0.35\n\n    combined = (w_int * interaction) + (w_arc * arc_score) + (w_proc * proc_score) - (w_queue * queue_score) - (w_delay * delay_score)\n\n    # Normalize to keep output roughly in [-1,1] using max possible positive/negative contributions\n    max_pos = (w_int * 1.0) + (w_arc * 1.0) + (w_proc * 1.0)\n    max_neg = (w_queue * 1.0) + (w_delay * 1.0)\n    normalizer = max(max_pos, max_neg, eps)\n\n    normalized = combined / (normalizer + eps)\n\n    # Final scaling and clipping (modest offsets relative to Q-values)\n    scale = 0.09\n    offsets = scale * normalized\n    offsets = np.clip(offsets, -0.15, 0.15)\n\n    return offsets",
     "objective": 1.22,
     "other_inf": null,
     "eval_metrics": {
          "avg_aoi": 0.6073459326235029,
          "avg_dropped_ratio": 0.25903614457831325
     }
}