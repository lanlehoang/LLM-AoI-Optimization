[
     {
          "algorithm": "Create a bounded offset by combining a multiplicative \"close-and-fast\" bonus with separate nonlinear penalties for queue congestion and propagation distance, using robust scale estimates (medians/max) so the offset remains modest and well-behaved across zero-padded neighbors.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n        distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n        offsets: numpy array of same shape with small signed offsets to add to agent Q-values\n    \"\"\"\n    eps = 1e-9\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    # Robust scale estimates (ignore exact zeros when appropriate to handle padded neighbors)\n    pos_dist = distances[distances > 0]\n    pos_arc = arc_lengths[arc_lengths > 0]\n    pos_proc = processing_rates[processing_rates > 0]\n    pos_queue = queue_lengths[queue_lengths > 0]\n\n    # Fallback scales to avoid division by zero; choose typical magnitudes (km, pkts/s, pkts)\n    dist_scale = float(np.median(pos_dist)) if pos_dist.size > 0 else 1000.0\n    arc_scale = float(np.median(pos_arc)) if pos_arc.size > 0 else 2000.0\n    proc_max = float(np.max(processing_rates)) if processing_rates.size > 0 else 1.0\n    proc_max = max(proc_max, 1.0)\n    queue_scale = float(np.median(pos_queue)) if pos_queue.size > 0 else 4.0\n    queue_scale = max(queue_scale, 1.0)\n\n    # Nonlinear feature transforms giving values in [0,1] (higher = better for arc/proc, worse for queue/dist)\n    # Closeness to destination: exponential decay (1 when very close, ->0 when far)\n    arc_score = np.exp(-arc_lengths / (arc_scale + eps))\n\n    # Processing: diminishing returns via log1p, normalized by log1p(max)\n    proc_score = np.log1p(processing_rates) / np.log1p(proc_max + eps)\n\n    # Queue congestion: saturating penalty in [0,1]\n    queue_score = queue_lengths / (queue_lengths + queue_scale + eps)\n\n    # Propagation distance penalty: fraction in [0,1]\n    dist_score = distances / (distances + dist_scale + eps)\n\n    # Interaction: prefer neighbours that are both close (to dest) and fast\n    interaction = arc_score * proc_score\n\n    # Weights emphasize interaction and queue penalty but keep influence modest\n    w_int = 0.55\n    w_arc = 0.15\n    w_proc = 0.15\n    w_queue = 0.95\n    w_dist = 0.25\n\n    combined = (w_int * interaction) + (w_arc * arc_score) + (w_proc * proc_score) - (w_queue * queue_score) - (w_dist * dist_score)\n\n    # Normalize combined to roughly [-1,1] by dividing by sum of absolute positive/negative weight contributions\n    # Compute an approximate normalizer using maxima of component ranges\n    max_pos = (w_int * 1.0) + (w_arc * 1.0) + (w_proc * 1.0)  # when positive terms are max\n    max_neg = (w_queue * 1.0) + (w_dist * 1.0)               # when penalties are max\n    normalizer = max(max_pos, max_neg, eps)\n\n    normalized = combined / (normalizer + eps)  # now roughly in [-1,1]\n\n    # Final scaling: keep offsets modest relative to typical Q-values (~\u00b10.5), and clip to safe bounds\n    scale = 0.07\n    offsets = scale * normalized\n    offsets = np.clip(offsets, -0.12, 0.12)\n\n    return offsets",
          "objective": 1.217,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6076311033087556,
               "avg_dropped_ratio": 0.26054216867469876
          }
     },
     {
          "algorithm": "Combine small, normalized bonuses for proximity and processing speed with penalties for queue congestion and propagation distance to produce a modest, bounded offset that refines (but does not overpower) the agent Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n        distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n        offsets: numpy array of same shape with small signed offsets to add to agent Q-values\n    \"\"\"\n    eps = 1e-6\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    def normize(x):\n        xmin = np.nanmin(x)\n        xmax = np.nanmax(x)\n        if np.isclose(xmax, xmin):\n            # If no variation, return neutral 0.5 for all elements\n            return np.full_like(x, 0.5, dtype=float)\n        return (x - xmin) / (xmax - xmin + eps)\n\n    # Normalized features in [0,1]\n    d_norm = normize(distances)         # larger => worse (prop delay)\n    a_norm = normize(arc_lengths)       # larger => farther from destination\n    p_norm = normize(processing_rates)  # larger => better (faster service)\n    q_norm = normize(queue_lengths)     # larger => worse (congestion)\n\n    # Terms: make smaller arc_length and higher processing positive; queues and distance negative\n    arc_term = 1.0 - a_norm      # in [0,1], higher is better (closer to dest)\n    proc_term = p_norm           # in [0,1], higher is better\n    queue_term = q_norm          # in [0,1], higher is worse\n    dist_term = d_norm           # in [0,1], higher is worse\n\n    # Weights chosen to give modest influence (do not overpower learned Qs)\n    w_arc = 0.60\n    w_proc = 0.40\n    w_queue = 0.90\n    w_dist = 0.20\n\n    # Base combined score (positive good, negative bad)\n    combined = (w_arc * arc_term) + (w_proc * proc_term) - (w_queue * queue_term) - (w_dist * dist_term)\n\n    # Scale down to keep offsets small relative to typical Q-values (~\u00b10.5)\n    scale = 0.08\n    offsets = scale * combined\n\n    # Clip to a modest symmetric bound so offsets never dominate\n    offsets = np.clip(offsets, -0.12, 0.12)\n\n    return offsets",
          "objective": 1.2148,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6161331881391877,
               "avg_dropped_ratio": 0.25150602409638556
          }
     },
     {
          "algorithm": "Combine a multiplicative nonlinear \"close-and-fast\" bonus with a logistic queue penalty and a square-root distance penalty, using robust medians for scaling and conservative weights so offsets remain modest and zero for padded neighbors.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n        distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n        offsets: numpy array of same shape with small signed offsets to add to agent Q-values\n    \"\"\"\n    eps = 1e-9\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    # Identify padded/missing neighbors (all-zero)\n    mask_missing = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n\n    # Robust scale estimates (ignore exact zeros when appropriate to handle padded neighbors)\n    pos_dist = distances[distances > 0]\n    pos_arc = arc_lengths[arc_lengths > 0]\n    pos_proc = processing_rates[processing_rates > 0]\n    pos_queue = queue_lengths[queue_lengths > 0]\n\n    dist_scale = float(np.median(pos_dist)) if pos_dist.size > 0 else 1000.0\n    arc_scale = float(np.median(pos_arc)) if pos_arc.size > 0 else 2000.0\n    proc_max = float(np.max(processing_rates)) if processing_rates.size > 0 else 1.0\n    proc_max = max(proc_max, 1.0)\n    queue_scale = float(np.median(pos_queue)) if pos_queue.size > 0 else 4.0\n    queue_scale = max(queue_scale, 1.0)\n\n    # Feature transforms\n    # Arc: strong nonlinear closeness (exponential), slightly sharpened\n    arc_score = np.exp(-arc_lengths / (arc_scale + eps))\n    arc_score = np.power(arc_score, 1.2)\n\n    # Processing: diminishing returns via log1p normalized\n    proc_score = np.log1p(processing_rates) / (np.log1p(proc_max + eps) + eps)\n\n    # Queue: logistic-like saturating penalty using tanh\n    queue_pen = np.tanh(queue_lengths / (queue_scale + eps))\n\n    # Distance: sublinear penalty via sqrt fraction\n    dist_pen = np.sqrt(distances / (distances + dist_scale + eps))\n\n    # Interaction: prefer neighbors that are both close (to dest) and reasonably fast\n    interaction = arc_score * np.power(proc_score, 0.9)\n\n    # Weights (conservative / different from prior)\n    w_bonus = 0.50\n    w_arc_linear = 0.12\n    w_proc = 0.18\n    w_queue = 1.20\n    w_dist = 0.28\n\n    combined = (w_bonus * interaction) + (w_arc_linear * arc_score) + (w_proc * proc_score) - (w_queue * queue_pen) - (w_dist * dist_pen)\n\n    # Normalize combined to keep values roughly in [-1,1]\n    max_pos = (w_bonus * 1.0) + (w_arc_linear * 1.0) + (w_proc * 1.0)\n    max_neg = (w_queue * 1.0) + (w_dist * 1.0)\n    normalizer = max(max_pos, max_neg, eps)\n    normalized = combined / (normalizer + eps)\n\n    # Final scaling and clipping to remain modest relative to Q-values\n    scale = 0.05\n    offsets = scale * normalized\n    offsets = np.clip(offsets, -0.10, 0.10)\n\n    # Ensure missing/padded neighbors have zero offset\n    if np.any(mask_missing):\n        offsets = offsets.copy()\n        offsets[mask_missing] = 0.0\n\n    return offsets",
          "objective": 1.1819,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6065151095720476,
               "avg_dropped_ratio": 0.2831325301204819
          }
     },
     {
          "algorithm": "Combine normalized, bounded contributions from queue congestion (negative), proximity to destination (positive), processing speed (positive), and propagation distance (negative) into a small weighted offset per neighbour, and set fully zero-padded neighbours to -inf so they remain unselectable.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n      - offsets: numpy array of same shape (float), with -np.inf for fully zero-padded neighbours\n    \"\"\"\n    # Convert to float arrays\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    # Identify missing (zero-padded) neighbours: all features zero\n    missing = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n\n    eps = 1e-6\n\n    # Normalize queue lengths: higher -> worse (we want 0..1)\n    max_q = queue_lengths.max()\n    if max_q > 0:\n        q_norm = queue_lengths / (max_q + eps)\n    else:\n        q_norm = np.zeros_like(queue_lengths)\n\n    # Normalize arc lengths: smaller arc -> better, so invert to make larger-is-better in 0..1\n    min_a = arc_lengths.min()\n    max_a = arc_lengths.max()\n    if max_a - min_a > eps:\n        a_norm = 1.0 - (arc_lengths - min_a) / (max_a - min_a + eps)\n    else:\n        # fallback: if all equal (or all zeros), prefer non-zero shorter arcs slightly\n        a_norm = np.zeros_like(arc_lengths)\n        nonzero_mask = arc_lengths > eps\n        if nonzero_mask.any():\n            a_norm[nonzero_mask] = 1.0\n\n    # Normalize processing rates: higher -> better\n    max_p = processing_rates.max()\n    if max_p > 0:\n        p_norm = processing_rates / (max_p + eps)\n    else:\n        p_norm = np.zeros_like(processing_rates)\n\n    # Normalize distances: larger -> worse\n    max_d = distances.max()\n    if max_d > 0:\n        d_norm = distances / (max_d + eps)\n    else:\n        d_norm = np.zeros_like(distances)\n\n    # Weights chosen to be small so offsets refine but do not overpower learned Q-values\n    w_q = 0.12   # penalty for congestion\n    w_a = 0.08   # reward for being closer to destination\n    w_p = 0.06   # reward for faster processing\n    w_d = 0.03   # penalty for longer propagation distance\n\n    offsets = (-w_q * q_norm) + (w_a * a_norm) + (w_p * p_norm) - (w_d * d_norm)\n\n    # Bound offsets to a small range to avoid overwhelming agent Q-values\n    offsets = np.clip(offsets, -0.15, 0.15)\n\n    # Ensure missing neighbours remain unselectable\n    offsets = offsets.astype(float)\n    offsets[missing] = -np.inf\n\n    return offsets",
          "objective": 1.173,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6149788581295117,
               "avg_dropped_ratio": 0.27861445783132527
          }
     }
]