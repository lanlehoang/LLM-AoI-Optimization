[
     {
          "algorithm": "Combine normalized closeness and service-speed bonuses with stronger nonlinear queue and propagation penalties using median-based robust scaling and modest clipping to keep offsets small.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n        distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n        offsets: numpy array of same shape with small signed offsets to add to agent Q-values\n    \"\"\"\n    eps = 1e-9\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    # Mask positive (non-zero) entries for robust statistics (handle zero-padding)\n    pos_dist = distances[distances > 0]\n    pos_arc = arc_lengths[arc_lengths > 0]\n    pos_proc = processing_rates[processing_rates > 0]\n    pos_queue = queue_lengths[queue_lengths > 0]\n\n    # Robust fallback scales\n    dist_scale = float(np.median(pos_dist)) if pos_dist.size > 0 else 1000.0\n    arc_scale = float(np.median(pos_arc)) if pos_arc.size > 0 else 2000.0\n    proc_max = float(np.max(processing_rates)) if processing_rates.size > 0 else 1.0\n    proc_max = max(proc_max, 1.0)\n    queue_scale = float(np.median(pos_queue)) if pos_queue.size > 0 else 4.0\n    queue_scale = max(queue_scale, 1.0)\n\n    # Propagation delay (seconds) using speed of light (km/s)\n    c_km_s = 299792.458\n    prop_delay = distances / (c_km_s + eps)\n    median_delay = dist_scale / (c_km_s + eps)\n\n    # Feature transforms mapped roughly to [0,1]\n    # Closeness to destination: reciprocal form (1 when arc is tiny)\n    arc_score = 1.0 / (1.0 + (arc_lengths / (arc_scale + eps)))\n\n    # Processing: normalized then square-root for diminishing returns\n    proc_score = np.sqrt(processing_rates / (proc_max + eps))\n\n    # Queue congestion: saturating penalty in [0,1] (higher = worse)\n    queue_score = queue_lengths / (queue_lengths + (queue_scale * 1.5) + eps)\n\n    # Delay (distance) penalty in [0,1]\n    delay_score = prop_delay / (prop_delay + (median_delay + eps))\n\n    # Interaction bonus: prefer close-to-destination AND fast service, down-weighted by queue\n    interaction = arc_score * proc_score\n    interaction = interaction * (1.0 - queue_score)  # damp interaction if queue is bad\n\n    # Weights (different from original): emphasize interaction and queue penalty, moderate delay\n    w_int = 0.60\n    w_arc = 0.20\n    w_proc = 0.10\n    w_queue = 1.10\n    w_delay = 0.35\n\n    combined = (w_int * interaction) + (w_arc * arc_score) + (w_proc * proc_score) - (w_queue * queue_score) - (w_delay * delay_score)\n\n    # Normalize to keep output roughly in [-1,1] using max possible positive/negative contributions\n    max_pos = (w_int * 1.0) + (w_arc * 1.0) + (w_proc * 1.0)\n    max_neg = (w_queue * 1.0) + (w_delay * 1.0)\n    normalizer = max(max_pos, max_neg, eps)\n\n    normalized = combined / (normalizer + eps)\n\n    # Final scaling and clipping (modest offsets relative to Q-values)\n    scale = 0.09\n    offsets = scale * normalized\n    offsets = np.clip(offsets, -0.15, 0.15)\n\n    return offsets",
          "objective": 1.22,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6073459326235029,
               "avg_dropped_ratio": 0.25903614457831325
          }
     },
     {
          "algorithm": "Create a bounded offset by combining a multiplicative \"close-and-fast\" bonus with separate nonlinear penalties for queue congestion and propagation distance, using robust scale estimates (medians/max) so the offset remains modest and well-behaved across zero-padded neighbors.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n        distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n        offsets: numpy array of same shape with small signed offsets to add to agent Q-values\n    \"\"\"\n    eps = 1e-9\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    # Robust scale estimates (ignore exact zeros when appropriate to handle padded neighbors)\n    pos_dist = distances[distances > 0]\n    pos_arc = arc_lengths[arc_lengths > 0]\n    pos_proc = processing_rates[processing_rates > 0]\n    pos_queue = queue_lengths[queue_lengths > 0]\n\n    # Fallback scales to avoid division by zero; choose typical magnitudes (km, pkts/s, pkts)\n    dist_scale = float(np.median(pos_dist)) if pos_dist.size > 0 else 1000.0\n    arc_scale = float(np.median(pos_arc)) if pos_arc.size > 0 else 2000.0\n    proc_max = float(np.max(processing_rates)) if processing_rates.size > 0 else 1.0\n    proc_max = max(proc_max, 1.0)\n    queue_scale = float(np.median(pos_queue)) if pos_queue.size > 0 else 4.0\n    queue_scale = max(queue_scale, 1.0)\n\n    # Nonlinear feature transforms giving values in [0,1] (higher = better for arc/proc, worse for queue/dist)\n    # Closeness to destination: exponential decay (1 when very close, ->0 when far)\n    arc_score = np.exp(-arc_lengths / (arc_scale + eps))\n\n    # Processing: diminishing returns via log1p, normalized by log1p(max)\n    proc_score = np.log1p(processing_rates) / np.log1p(proc_max + eps)\n\n    # Queue congestion: saturating penalty in [0,1]\n    queue_score = queue_lengths / (queue_lengths + queue_scale + eps)\n\n    # Propagation distance penalty: fraction in [0,1]\n    dist_score = distances / (distances + dist_scale + eps)\n\n    # Interaction: prefer neighbours that are both close (to dest) and fast\n    interaction = arc_score * proc_score\n\n    # Weights emphasize interaction and queue penalty but keep influence modest\n    w_int = 0.55\n    w_arc = 0.15\n    w_proc = 0.15\n    w_queue = 0.95\n    w_dist = 0.25\n\n    combined = (w_int * interaction) + (w_arc * arc_score) + (w_proc * proc_score) - (w_queue * queue_score) - (w_dist * dist_score)\n\n    # Normalize combined to roughly [-1,1] by dividing by sum of absolute positive/negative weight contributions\n    # Compute an approximate normalizer using maxima of component ranges\n    max_pos = (w_int * 1.0) + (w_arc * 1.0) + (w_proc * 1.0)  # when positive terms are max\n    max_neg = (w_queue * 1.0) + (w_dist * 1.0)               # when penalties are max\n    normalizer = max(max_pos, max_neg, eps)\n\n    normalized = combined / (normalizer + eps)  # now roughly in [-1,1]\n\n    # Final scaling: keep offsets modest relative to typical Q-values (~\u00b10.5), and clip to safe bounds\n    scale = 0.07\n    offsets = scale * normalized\n    offsets = np.clip(offsets, -0.12, 0.12)\n\n    return offsets",
          "objective": 1.217,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6076311033087556,
               "avg_dropped_ratio": 0.26054216867469876
          }
     },
     {
          "algorithm": "Combine small, normalized bonuses for proximity and processing speed with penalties for queue congestion and propagation distance to produce a modest, bounded offset that refines (but does not overpower) the agent Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n        distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n        offsets: numpy array of same shape with small signed offsets to add to agent Q-values\n    \"\"\"\n    eps = 1e-6\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    def normize(x):\n        xmin = np.nanmin(x)\n        xmax = np.nanmax(x)\n        if np.isclose(xmax, xmin):\n            # If no variation, return neutral 0.5 for all elements\n            return np.full_like(x, 0.5, dtype=float)\n        return (x - xmin) / (xmax - xmin + eps)\n\n    # Normalized features in [0,1]\n    d_norm = normize(distances)         # larger => worse (prop delay)\n    a_norm = normize(arc_lengths)       # larger => farther from destination\n    p_norm = normize(processing_rates)  # larger => better (faster service)\n    q_norm = normize(queue_lengths)     # larger => worse (congestion)\n\n    # Terms: make smaller arc_length and higher processing positive; queues and distance negative\n    arc_term = 1.0 - a_norm      # in [0,1], higher is better (closer to dest)\n    proc_term = p_norm           # in [0,1], higher is better\n    queue_term = q_norm          # in [0,1], higher is worse\n    dist_term = d_norm           # in [0,1], higher is worse\n\n    # Weights chosen to give modest influence (do not overpower learned Qs)\n    w_arc = 0.60\n    w_proc = 0.40\n    w_queue = 0.90\n    w_dist = 0.20\n\n    # Base combined score (positive good, negative bad)\n    combined = (w_arc * arc_term) + (w_proc * proc_term) - (w_queue * queue_term) - (w_dist * dist_term)\n\n    # Scale down to keep offsets small relative to typical Q-values (~\u00b10.5)\n    scale = 0.08\n    offsets = scale * combined\n\n    # Clip to a modest symmetric bound so offsets never dominate\n    offsets = np.clip(offsets, -0.12, 0.12)\n\n    return offsets",
          "objective": 1.2148,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6161331881391877,
               "avg_dropped_ratio": 0.25150602409638556
          }
     },
     {
          "algorithm": "Combine a strengthened queue-penalty with a moderate \"close-and-fast\" multiplicative bonus, using alternate nonlinear transforms (reciprocal arc decay, root-normalized processing, power-law queue penalty, exponential distance penalty) and robust medians to produce modest bounded offsets that refine agent Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n        distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n        offsets: numpy array of same shape with small signed offsets to add to agent Q-values\n    \"\"\"\n    eps = 1e-9\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    # Determine valid neighbours (non-padded)\n    valid = (distances > 0) | (arc_lengths > 0) | (processing_rates > 0) | (queue_lengths > 0)\n\n    # Robust scales ignoring zero-padded entries\n    pos_dist = distances[distances > 0]\n    pos_arc = arc_lengths[arc_lengths > 0]\n    pos_proc = processing_rates[processing_rates > 0]\n    pos_queue = queue_lengths[queue_lengths > 0]\n\n    dist_scale = float(np.median(pos_dist)) if pos_dist.size > 0 else 1200.0\n    arc_scale = float(np.median(pos_arc)) if pos_arc.size > 0 else 1800.0\n    proc_max = float(np.max(processing_rates)) if processing_rates.size > 0 else 1.0\n    proc_max = max(proc_max, 1.0)\n    queue_scale = float(np.median(pos_queue)) if pos_queue.size > 0 else 3.0\n    queue_scale = max(queue_scale, 1.0)\n\n    # Scores in [0,1] for valid entries; initialize arrays\n    arc_score = np.zeros_like(arc_lengths, dtype=float)\n    proc_score = np.zeros_like(processing_rates, dtype=float)\n    queue_score = np.zeros_like(queue_lengths, dtype=float)\n    dist_score = np.zeros_like(distances, dtype=float)\n\n    # Arc closeness: reciprocal decay (1 when arc=0, ->0 when far)\n    arc_denom = (1.0 + (arc_lengths / (arc_scale + eps)))\n    arc_score[valid] = 1.0 / (arc_denom[valid] + eps)\n\n    # Processing: diminishing returns via square-root normalization\n    proc_score[valid] = np.sqrt(np.clip(processing_rates[valid], 0.0, None)) / (np.sqrt(proc_max) + eps)\n\n    # Queue congestion: power-law saturating penalty (stronger penalty for larger queues)\n    qpow = 1.5\n    queue_score[valid] = (np.power(queue_lengths[valid], qpow)) / (np.power(queue_lengths[valid], qpow) + np.power(queue_scale, qpow) + eps)\n\n    # Distance penalty: exponential approach to 1 for large distances (higher = worse)\n    dist_score[valid] = 1.0 - np.exp(-distances[valid] / (1.4 * dist_scale + eps))\n\n    # Interaction: prefer neighbours both close to destination and fast (multiplicative)\n    interaction = np.zeros_like(arc_score)\n    interaction[valid] = arc_score[valid] * np.power(proc_score[valid], 0.85)\n\n    # Weights (different from original): emphasize queue penalty more, keep offsets modest\n    w_int = 0.45\n    w_arc = 0.10\n    w_proc = 0.10\n    w_queue = 1.15\n    w_dist = 0.30\n\n    combined = (w_int * interaction) + (w_arc * arc_score) + (w_proc * proc_score) - (w_queue * queue_score) - (w_dist * dist_score)\n\n    # Normalizer using positive/negative maxima to keep range balanced\n    max_pos = (w_int * 1.0) + (w_arc * 1.0) + (w_proc * 1.0)\n    max_neg = (w_queue * 1.0) + (w_dist * 1.0)\n    normalizer = max(max_pos, max_neg, eps)\n\n    normalized = combined / (normalizer + eps)\n\n    # Final scaling and clipping: modest offsets relative to typical Q-values\n    scale = 0.085\n    offsets = scale * normalized\n\n    # Clip to safe bounds and zero out padded neighbours\n    offsets = np.clip(offsets, -0.10, 0.10)\n    offsets[~valid] = 0.0\n\n    return offsets",
          "objective": 1.203,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6171792080495462,
               "avg_dropped_ratio": 0.2575301204819277
          }
     }
]