[
     {
          "algorithm": "Combine small, normalized bonuses for proximity and processing speed with penalties for queue congestion and propagation distance to produce a modest, bounded offset that refines (but does not overpower) the agent Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n        distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n        offsets: numpy array of same shape with small signed offsets to add to agent Q-values\n    \"\"\"\n    eps = 1e-6\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    def normize(x):\n        xmin = np.nanmin(x)\n        xmax = np.nanmax(x)\n        if np.isclose(xmax, xmin):\n            # If no variation, return neutral 0.5 for all elements\n            return np.full_like(x, 0.5, dtype=float)\n        return (x - xmin) / (xmax - xmin + eps)\n\n    # Normalized features in [0,1]\n    d_norm = normize(distances)         # larger => worse (prop delay)\n    a_norm = normize(arc_lengths)       # larger => farther from destination\n    p_norm = normize(processing_rates)  # larger => better (faster service)\n    q_norm = normize(queue_lengths)     # larger => worse (congestion)\n\n    # Terms: make smaller arc_length and higher processing positive; queues and distance negative\n    arc_term = 1.0 - a_norm      # in [0,1], higher is better (closer to dest)\n    proc_term = p_norm           # in [0,1], higher is better\n    queue_term = q_norm          # in [0,1], higher is worse\n    dist_term = d_norm           # in [0,1], higher is worse\n\n    # Weights chosen to give modest influence (do not overpower learned Qs)\n    w_arc = 0.60\n    w_proc = 0.40\n    w_queue = 0.90\n    w_dist = 0.20\n\n    # Base combined score (positive good, negative bad)\n    combined = (w_arc * arc_term) + (w_proc * proc_term) - (w_queue * queue_term) - (w_dist * dist_term)\n\n    # Scale down to keep offsets small relative to typical Q-values (~\u00b10.5)\n    scale = 0.08\n    offsets = scale * combined\n\n    # Clip to a modest symmetric bound so offsets never dominate\n    offsets = np.clip(offsets, -0.12, 0.12)\n\n    return offsets",
          "objective": 1.2148,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6161331881391877,
               "avg_dropped_ratio": 0.25150602409638556
          }
     },
     {
          "algorithm": "Combine normalized, bounded contributions from queue congestion (negative), proximity to destination (positive), processing speed (positive), and propagation distance (negative) into a small weighted offset per neighbour, and set fully zero-padded neighbours to -inf so they remain unselectable.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n      - offsets: numpy array of same shape (float), with -np.inf for fully zero-padded neighbours\n    \"\"\"\n    # Convert to float arrays\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    # Identify missing (zero-padded) neighbours: all features zero\n    missing = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n\n    eps = 1e-6\n\n    # Normalize queue lengths: higher -> worse (we want 0..1)\n    max_q = queue_lengths.max()\n    if max_q > 0:\n        q_norm = queue_lengths / (max_q + eps)\n    else:\n        q_norm = np.zeros_like(queue_lengths)\n\n    # Normalize arc lengths: smaller arc -> better, so invert to make larger-is-better in 0..1\n    min_a = arc_lengths.min()\n    max_a = arc_lengths.max()\n    if max_a - min_a > eps:\n        a_norm = 1.0 - (arc_lengths - min_a) / (max_a - min_a + eps)\n    else:\n        # fallback: if all equal (or all zeros), prefer non-zero shorter arcs slightly\n        a_norm = np.zeros_like(arc_lengths)\n        nonzero_mask = arc_lengths > eps\n        if nonzero_mask.any():\n            a_norm[nonzero_mask] = 1.0\n\n    # Normalize processing rates: higher -> better\n    max_p = processing_rates.max()\n    if max_p > 0:\n        p_norm = processing_rates / (max_p + eps)\n    else:\n        p_norm = np.zeros_like(processing_rates)\n\n    # Normalize distances: larger -> worse\n    max_d = distances.max()\n    if max_d > 0:\n        d_norm = distances / (max_d + eps)\n    else:\n        d_norm = np.zeros_like(distances)\n\n    # Weights chosen to be small so offsets refine but do not overpower learned Q-values\n    w_q = 0.12   # penalty for congestion\n    w_a = 0.08   # reward for being closer to destination\n    w_p = 0.06   # reward for faster processing\n    w_d = 0.03   # penalty for longer propagation distance\n\n    offsets = (-w_q * q_norm) + (w_a * a_norm) + (w_p * p_norm) - (w_d * d_norm)\n\n    # Bound offsets to a small range to avoid overwhelming agent Q-values\n    offsets = np.clip(offsets, -0.15, 0.15)\n\n    # Ensure missing neighbours remain unselectable\n    offsets = offsets.astype(float)\n    offsets[missing] = -np.inf\n\n    return offsets",
          "objective": 1.173,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6149788581295117,
               "avg_dropped_ratio": 0.27861445783132527
          }
     },
     {
          "algorithm": "Combine normalized, clipped scores for (low) queue length, (short) arc length, (high) processing rate, and (short) distance with tuned weights and a small overall scale to produce a modest offset (\u00b1~0.2) that nudges the agent away from congested/long links and toward fast/closer ones.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths : numpy arrays (same shape)\n    Returns:\n      - offsets : numpy array (same shape)\n    \"\"\"\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n\n    # Mask padded/absent neighbours (all-zero feature vectors) -> keep offset 0\n    pad_mask = (d == 0) & (a == 0) & (p == 0) & (q == 0)\n\n    # Avoid division by zero when normalizing\n    eps = 1e-6\n    max_d = max(d.max(), eps)\n    max_a = max(a.max(), eps)\n    max_p = max(p.max(), eps)\n    max_q = max(q.max(), eps)\n\n    # Normalize features to [0,1]\n    dist_norm = d / max_d\n    arc_norm = a / max_a\n    proc_norm = p / max_p\n    queue_norm = q / max_q\n\n    # Convert to \"desirability\" scores in [0,1] (higher is better)\n    # - closer distance -> higher score\n    dist_score = np.clip(1.0 - dist_norm, 0.0, 1.0)\n    # - smaller arc length -> higher score\n    arc_score = np.clip(1.0 - arc_norm, 0.0, 1.0)\n    # - higher processing rate -> higher score\n    proc_score = np.clip(proc_norm, 0.0, 1.0)\n    # - smaller queue -> higher score; amplify penalty for larger queues with exponent\n    queue_score = np.clip(1.0 - np.power(queue_norm, 1.5), 0.0, 1.0)\n\n    # Weights (queue is strongest, then arc to destination, then processing, then distance)\n    w_q, w_a, w_p, w_d = 0.40, 0.30, 0.20, 0.10\n\n    raw = (w_q * queue_score) + (w_a * arc_score) + (w_p * proc_score) + (w_d * dist_score)\n\n    # Center around 0.5 and scale to keep offsets modest (roughly in [-0.2, 0.2])\n    scale = 0.4\n    offsets = (raw - 0.5) * scale\n\n    # Ensure padded neighbours receive zero offset\n    offsets = offsets.astype(float)\n    offsets[pad_mask] = 0.0\n\n    return offsets",
          "objective": 1.1163,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6435067993904627,
               "avg_dropped_ratio": 0.28162650602409645
          }
     },
     {
          "algorithm": "Combine normalized proximity, processing-rate, queue-length and propagation-delay signals into a small bounded offset that rewards closer and faster neighbours and penalizes congestion and long links.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n      offsets: numpy array of same shape\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    eps = 1e-6\n    offsets = np.zeros_like(distances, dtype=float)\n\n    # valid neighbor mask (zero-padded neighbours are treated as invalid)\n    valid = (distances != 0) | (arc_lengths != 0) | (processing_rates != 0) | (queue_lengths != 0)\n\n    if not np.any(valid):\n        return offsets\n\n    # safe statistics over valid neighbours\n    arc_max = max(np.max(arc_lengths[valid]), eps)\n    dist_max = max(np.max(distances[valid]), eps)\n    proc_max = np.max(processing_rates[valid])\n    proc_min = np.min(processing_rates[valid])\n    proc_range = max(proc_max - proc_min, eps)\n    queue_max = max(np.max(queue_lengths[valid]), eps)\n\n    # normalized signals (0..1)\n    proximity = 1.0 - (arc_lengths / (arc_max + eps))            # higher = closer to destination\n    proc_score = (processing_rates - proc_min) / proc_range     # higher = faster service\n    queue_score = queue_lengths / (queue_max + eps)             # higher = more congested\n    dist_score = distances / (dist_max + eps)                   # higher = longer propagation delay\n\n    # weights chosen to refine (not overpower) learned Q-values; magnitudes ~0.1\n    w_prox = 0.07\n    w_proc = 0.04\n    w_queue = 0.14\n    w_dist = 0.02\n\n    raw_offset = w_prox * proximity + w_proc * proc_score - w_queue * queue_score - w_dist * dist_score\n\n    # apply only to valid neighbours; keep padded slots at zero\n    offsets[valid] = raw_offset[valid]\n\n    # bound offsets to a small range so they don't overpower Q-values\n    offsets = np.clip(offsets, -0.15, 0.15)\n\n    return offsets",
          "objective": 1.1135,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6532794711446224,
               "avg_dropped_ratio": 0.2725903614457832
          }
     }
]