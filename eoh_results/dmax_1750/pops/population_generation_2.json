[
     {
          "algorithm": "Create a bounded offset by combining a multiplicative \"close-and-fast\" bonus with separate nonlinear penalties for queue congestion and propagation distance, using robust scale estimates (medians/max) so the offset remains modest and well-behaved across zero-padded neighbors.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n        distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n        offsets: numpy array of same shape with small signed offsets to add to agent Q-values\n    \"\"\"\n    eps = 1e-9\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    # Robust scale estimates (ignore exact zeros when appropriate to handle padded neighbors)\n    pos_dist = distances[distances > 0]\n    pos_arc = arc_lengths[arc_lengths > 0]\n    pos_proc = processing_rates[processing_rates > 0]\n    pos_queue = queue_lengths[queue_lengths > 0]\n\n    # Fallback scales to avoid division by zero; choose typical magnitudes (km, pkts/s, pkts)\n    dist_scale = float(np.median(pos_dist)) if pos_dist.size > 0 else 1000.0\n    arc_scale = float(np.median(pos_arc)) if pos_arc.size > 0 else 2000.0\n    proc_max = float(np.max(processing_rates)) if processing_rates.size > 0 else 1.0\n    proc_max = max(proc_max, 1.0)\n    queue_scale = float(np.median(pos_queue)) if pos_queue.size > 0 else 4.0\n    queue_scale = max(queue_scale, 1.0)\n\n    # Nonlinear feature transforms giving values in [0,1] (higher = better for arc/proc, worse for queue/dist)\n    # Closeness to destination: exponential decay (1 when very close, ->0 when far)\n    arc_score = np.exp(-arc_lengths / (arc_scale + eps))\n\n    # Processing: diminishing returns via log1p, normalized by log1p(max)\n    proc_score = np.log1p(processing_rates) / np.log1p(proc_max + eps)\n\n    # Queue congestion: saturating penalty in [0,1]\n    queue_score = queue_lengths / (queue_lengths + queue_scale + eps)\n\n    # Propagation distance penalty: fraction in [0,1]\n    dist_score = distances / (distances + dist_scale + eps)\n\n    # Interaction: prefer neighbours that are both close (to dest) and fast\n    interaction = arc_score * proc_score\n\n    # Weights emphasize interaction and queue penalty but keep influence modest\n    w_int = 0.55\n    w_arc = 0.15\n    w_proc = 0.15\n    w_queue = 0.95\n    w_dist = 0.25\n\n    combined = (w_int * interaction) + (w_arc * arc_score) + (w_proc * proc_score) - (w_queue * queue_score) - (w_dist * dist_score)\n\n    # Normalize combined to roughly [-1,1] by dividing by sum of absolute positive/negative weight contributions\n    # Compute an approximate normalizer using maxima of component ranges\n    max_pos = (w_int * 1.0) + (w_arc * 1.0) + (w_proc * 1.0)  # when positive terms are max\n    max_neg = (w_queue * 1.0) + (w_dist * 1.0)               # when penalties are max\n    normalizer = max(max_pos, max_neg, eps)\n\n    normalized = combined / (normalizer + eps)  # now roughly in [-1,1]\n\n    # Final scaling: keep offsets modest relative to typical Q-values (~\u00b10.5), and clip to safe bounds\n    scale = 0.07\n    offsets = scale * normalized\n    offsets = np.clip(offsets, -0.12, 0.12)\n\n    return offsets",
          "objective": 1.217,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6076311033087556,
               "avg_dropped_ratio": 0.26054216867469876
          }
     },
     {
          "algorithm": "Combine small, normalized bonuses for proximity and processing speed with penalties for queue congestion and propagation distance to produce a modest, bounded offset that refines (but does not overpower) the agent Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n        distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n        offsets: numpy array of same shape with small signed offsets to add to agent Q-values\n    \"\"\"\n    eps = 1e-6\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    def normize(x):\n        xmin = np.nanmin(x)\n        xmax = np.nanmax(x)\n        if np.isclose(xmax, xmin):\n            # If no variation, return neutral 0.5 for all elements\n            return np.full_like(x, 0.5, dtype=float)\n        return (x - xmin) / (xmax - xmin + eps)\n\n    # Normalized features in [0,1]\n    d_norm = normize(distances)         # larger => worse (prop delay)\n    a_norm = normize(arc_lengths)       # larger => farther from destination\n    p_norm = normize(processing_rates)  # larger => better (faster service)\n    q_norm = normize(queue_lengths)     # larger => worse (congestion)\n\n    # Terms: make smaller arc_length and higher processing positive; queues and distance negative\n    arc_term = 1.0 - a_norm      # in [0,1], higher is better (closer to dest)\n    proc_term = p_norm           # in [0,1], higher is better\n    queue_term = q_norm          # in [0,1], higher is worse\n    dist_term = d_norm           # in [0,1], higher is worse\n\n    # Weights chosen to give modest influence (do not overpower learned Qs)\n    w_arc = 0.60\n    w_proc = 0.40\n    w_queue = 0.90\n    w_dist = 0.20\n\n    # Base combined score (positive good, negative bad)\n    combined = (w_arc * arc_term) + (w_proc * proc_term) - (w_queue * queue_term) - (w_dist * dist_term)\n\n    # Scale down to keep offsets small relative to typical Q-values (~\u00b10.5)\n    scale = 0.08\n    offsets = scale * combined\n\n    # Clip to a modest symmetric bound so offsets never dominate\n    offsets = np.clip(offsets, -0.12, 0.12)\n\n    return offsets",
          "objective": 1.2148,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6161331881391877,
               "avg_dropped_ratio": 0.25150602409638556
          }
     },
     {
          "algorithm": "Combine a strengthened queue-penalty with a moderate \"close-and-fast\" multiplicative bonus, using alternate nonlinear transforms (reciprocal arc decay, root-normalized processing, power-law queue penalty, exponential distance penalty) and robust medians to produce modest bounded offsets that refine agent Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n        distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n        offsets: numpy array of same shape with small signed offsets to add to agent Q-values\n    \"\"\"\n    eps = 1e-9\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    # Determine valid neighbours (non-padded)\n    valid = (distances > 0) | (arc_lengths > 0) | (processing_rates > 0) | (queue_lengths > 0)\n\n    # Robust scales ignoring zero-padded entries\n    pos_dist = distances[distances > 0]\n    pos_arc = arc_lengths[arc_lengths > 0]\n    pos_proc = processing_rates[processing_rates > 0]\n    pos_queue = queue_lengths[queue_lengths > 0]\n\n    dist_scale = float(np.median(pos_dist)) if pos_dist.size > 0 else 1200.0\n    arc_scale = float(np.median(pos_arc)) if pos_arc.size > 0 else 1800.0\n    proc_max = float(np.max(processing_rates)) if processing_rates.size > 0 else 1.0\n    proc_max = max(proc_max, 1.0)\n    queue_scale = float(np.median(pos_queue)) if pos_queue.size > 0 else 3.0\n    queue_scale = max(queue_scale, 1.0)\n\n    # Scores in [0,1] for valid entries; initialize arrays\n    arc_score = np.zeros_like(arc_lengths, dtype=float)\n    proc_score = np.zeros_like(processing_rates, dtype=float)\n    queue_score = np.zeros_like(queue_lengths, dtype=float)\n    dist_score = np.zeros_like(distances, dtype=float)\n\n    # Arc closeness: reciprocal decay (1 when arc=0, ->0 when far)\n    arc_denom = (1.0 + (arc_lengths / (arc_scale + eps)))\n    arc_score[valid] = 1.0 / (arc_denom[valid] + eps)\n\n    # Processing: diminishing returns via square-root normalization\n    proc_score[valid] = np.sqrt(np.clip(processing_rates[valid], 0.0, None)) / (np.sqrt(proc_max) + eps)\n\n    # Queue congestion: power-law saturating penalty (stronger penalty for larger queues)\n    qpow = 1.5\n    queue_score[valid] = (np.power(queue_lengths[valid], qpow)) / (np.power(queue_lengths[valid], qpow) + np.power(queue_scale, qpow) + eps)\n\n    # Distance penalty: exponential approach to 1 for large distances (higher = worse)\n    dist_score[valid] = 1.0 - np.exp(-distances[valid] / (1.4 * dist_scale + eps))\n\n    # Interaction: prefer neighbours both close to destination and fast (multiplicative)\n    interaction = np.zeros_like(arc_score)\n    interaction[valid] = arc_score[valid] * np.power(proc_score[valid], 0.85)\n\n    # Weights (different from original): emphasize queue penalty more, keep offsets modest\n    w_int = 0.45\n    w_arc = 0.10\n    w_proc = 0.10\n    w_queue = 1.15\n    w_dist = 0.30\n\n    combined = (w_int * interaction) + (w_arc * arc_score) + (w_proc * proc_score) - (w_queue * queue_score) - (w_dist * dist_score)\n\n    # Normalizer using positive/negative maxima to keep range balanced\n    max_pos = (w_int * 1.0) + (w_arc * 1.0) + (w_proc * 1.0)\n    max_neg = (w_queue * 1.0) + (w_dist * 1.0)\n    normalizer = max(max_pos, max_neg, eps)\n\n    normalized = combined / (normalizer + eps)\n\n    # Final scaling and clipping: modest offsets relative to typical Q-values\n    scale = 0.085\n    offsets = scale * normalized\n\n    # Clip to safe bounds and zero out padded neighbours\n    offsets = np.clip(offsets, -0.10, 0.10)\n    offsets[~valid] = 0.0\n\n    return offsets",
          "objective": 1.203,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6171792080495462,
               "avg_dropped_ratio": 0.2575301204819277
          }
     },
     {
          "algorithm": "This algorithm computes ordinal ranks for proximity, service-capacity, congestion and propagation distance, combines these centered ranks with tuned weights, and applies a tanh compressor to produce small, bounded offsets that subtly refine the agent Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    offsets = np.zeros_like(distances, dtype=float)\n\n    # Consider neighbors that are actually present (non-padded). Use distance>0 as presence indicator.\n    mask = distances > 0\n    idxs = np.nonzero(mask)[0]\n    n = idxs.size\n    if n == 0:\n        return offsets\n\n    # Raw signals (higher is better)\n    prox_score = -arc_lengths[idxs]                      # smaller arc_length -> larger score\n    cap_score = processing_rates[idxs] / (1.0 + queue_lengths[idxs])  # effective service per queued packet\n    cong_score = -queue_lengths[idxs]                    # smaller queue -> larger score\n    dist_score = -distances[idxs]                        # smaller distance -> larger score\n\n    def centered_ranks(x):\n        \"\"\"Return ranks normalized to [-0.5, 0.5] (higher x -> positive).\"\"\"\n        order = np.argsort(x)\n        ranks = np.empty_like(order, dtype=float)\n        ranks[order] = np.arange(x.size, dtype=float)\n        if x.size == 1:\n            return np.zeros_like(ranks)\n        return (ranks / (x.size - 1.0)) - 0.5\n\n    r_prox = centered_ranks(prox_score)\n    r_cap = centered_ranks(cap_score)\n    r_cong = centered_ranks(cong_score)\n    r_dist = centered_ranks(dist_score)\n\n    # Weights chosen to prefer moving closer to destination and to favor capacity while penalizing congestion/distance\n    w_prox = 0.35\n    w_cap = 0.30\n    w_cong = 0.25\n    w_dist = 0.10\n\n    combined = (w_prox * r_prox) + (w_cap * r_cap) + (w_cong * r_cong) + (w_dist * r_dist)\n\n    # Nonlinear compression to keep offsets modest and robust to outliers\n    # tanh compresses to (-1,1); scale chosen so offsets are smaller than typical Q-value magnitudes\n    scale = 0.12\n    gain = 3.0\n    offsets_valid = scale * np.tanh(gain * combined)\n\n    # Safety clip (keeps influence modest)\n    offsets_valid = np.clip(offsets_valid, -scale, scale)\n\n    offsets[idxs] = offsets_valid\n    return offsets",
          "objective": 1.185,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6100546780037672,
               "avg_dropped_ratio": 0.27710843373493976
          }
     }
]