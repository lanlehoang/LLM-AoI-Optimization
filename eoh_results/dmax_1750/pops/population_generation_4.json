[
     {
          "algorithm": "Compute a small, bounded heuristic offset by combining (positive) proximity-to-destination and processing-rate signals with (negative) queue-congestion and propagation-delay penalties, normalized per-sample so offsets refine but do not overwhelm learned Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs: distances, arc_lengths, processing_rates, queue_lengths (numpy arrays, same shape)\n    Output: offsets (numpy array, same shape)\n    \"\"\"\n    distances = np.asarray(distances, dtype=np.float64)\n    arc_lengths = np.asarray(arc_lengths, dtype=np.float64)\n    processing_rates = np.asarray(processing_rates, dtype=np.float64)\n    queue_lengths = np.asarray(queue_lengths, dtype=np.float64)\n    \n    # Prevent division by zero in normalization\n    eps = 1e-8\n    \n    # Mask invalid / padded neighbours (all-zero feature rows)\n    invalid = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n    \n    # 1) Proximity to destination: use inverse arc length, give a stronger bonus for very small arc lengths (including zero)\n    inv_arc = 1.0 / (arc_lengths + eps)\n    # Normalize to [0,1]\n    max_inv_arc = np.max(inv_arc) if np.max(inv_arc) > 0 else 1.0\n    arc_score = inv_arc / (max_inv_arc + eps)\n    \n    # 2) Processing rate: higher is better\n    max_proc = np.max(processing_rates) if np.max(processing_rates) > 0 else 1.0\n    proc_score = processing_rates / (max_proc + eps)\n    \n    # 3) Queue congestion: higher is worse; use a slightly superlinear penalty so high queues are penalized more\n    max_queue = np.max(queue_lengths)\n    # If all queues zero, avoid division by zero; use denom >=1\n    denom_q = max(1.0, max_queue)\n    q_norm = queue_lengths / (denom_q + eps)\n    q_score = np.power(q_norm, 1.2)  # sharpen penalty for larger queues\n    \n    # 4) Distance penalty (propagation delay): longer distance slightly worse\n    max_dist = np.max(distances) if np.max(distances) > 0 else 1.0\n    dist_score = distances / (max_dist + eps)\n    \n    # Weights chosen so offsets are modest (do not overwhelm learned Q-values)\n    w_arc = 0.12    # positive weight for closeness to destination\n    w_proc = 0.08   # positive weight for higher processing rate\n    w_queue = 0.14  # negative weight for queue congestion\n    w_dist = 0.02   # negative weight for propagation distance\n    \n    offsets = (w_arc * arc_score) + (w_proc * proc_score) - (w_queue * q_score) - (w_dist * dist_score)\n    \n    # Encourage direct delivery (arc_length == 0) a bit more (explicit boost) but keep within bounds\n    direct_mask = (arc_lengths == 0) & (~invalid)\n    offsets[direct_mask] += 0.04\n    \n    # Clip offsets to a small range so they refine but don't dominate learned Q-values\n    offsets = np.clip(offsets, -0.16, 0.16)\n    \n    # Set offsets for invalid/padded neighbours to zero (Q-agent is typically -inf there anyway)\n    offsets[invalid] = 0.0\n    \n    return offsets",
          "objective": 1.5415,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6008486373997836,
               "avg_dropped_ratio": 0.07379518072289157
          }
     },
     {
          "algorithm": "",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs: distances, arc_lengths, processing_rates, queue_lengths (numpy arrays, same shape)\n    Output: offsets (numpy array, same shape)\n    \"\"\"\n    distances = np.asarray(distances, dtype=np.float64)\n    arc_lengths = np.asarray(arc_lengths, dtype=np.float64)\n    processing_rates = np.asarray(processing_rates, dtype=np.float64)\n    queue_lengths = np.asarray(queue_lengths, dtype=np.float64)\n    \n    eps = 1e-8\n\n    # Mask invalid / padded neighbours (all-zero feature rows)\n    invalid = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n\n    # Use monotonic, bounded transforms that don't rely on dataset-wide maxima:\n    # - proximity: 1 / (1 + arc_length)  -> in (0,1], arc_length=0 gives 1\n    # - processing rate: proc / (1 + proc) -> in [0,1)\n    # - queue: queue / (1 + queue) -> in [0,1)\n    # - distance: dist / (1 + dist) -> in [0,1)\n    arc_score = 1.0 / (1.0 + arc_lengths + eps)\n    proc_score = processing_rates / (1.0 + processing_rates + eps)\n    q_score = queue_lengths / (1.0 + queue_lengths + eps)\n    dist_score = distances / (1.0 + distances + eps)\n\n    # Simple, modest weights to keep offsets as small refinements\n    w_arc = 0.12\n    w_proc = 0.08\n    w_queue = 0.12\n    w_dist = 0.02\n\n    offsets = (w_arc * arc_score) + (w_proc * proc_score) - (w_queue * q_score) - (w_dist * dist_score)\n\n    # Keep offsets bounded so they refine Q-values without dominating\n    offsets = np.clip(offsets, -0.16, 0.16)\n\n    # Zero out invalid / padded neighbours\n    offsets = np.where(invalid, 0.0, offsets)\n\n    return offsets",
          "objective": 1.5397,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.5995944721262487,
               "avg_dropped_ratio": 0.07680722891566265
          }
     },
     {
          "algorithm": "A normalized, bounded heuristic that blends an inverse-logarithmic proximity-to-destination score, a log-scaled processing-rate bonus, a superlinear queue congestion penalty, and a distance penalty using adjusted weights so offsets modestly refine but do not dominate learned Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs: distances, arc_lengths, processing_rates, queue_lengths (numpy arrays, same shape)\n    Output: offsets (numpy array, same shape)\n    \"\"\"\n    distances = np.asarray(distances, dtype=np.float64)\n    arc_lengths = np.asarray(arc_lengths, dtype=np.float64)\n    processing_rates = np.asarray(processing_rates, dtype=np.float64)\n    queue_lengths = np.asarray(queue_lengths, dtype=np.float64)\n\n    eps = 1e-8\n\n    # Mask invalid / padded neighbours (all-zero feature rows)\n    invalid = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n\n    # 1) Proximity to destination: inverse-log scaling to reduce dominance of very large arc lengths\n    inv_log_arc = 1.0 / (1.0 + np.log1p(arc_lengths + eps))\n    max_inv_log_arc = np.max(inv_log_arc) if np.max(inv_log_arc) > 0 else 1.0\n    arc_score = inv_log_arc / (max_inv_log_arc + eps)\n\n    # 2) Processing rate: log-scaled to compress large rate differences\n    proc_log = np.log1p(processing_rates)\n    max_proc_log = np.max(proc_log) if np.max(proc_log) > 0 else 1.0\n    proc_score = proc_log / (max_proc_log + eps)\n\n    # 3) Queue congestion: normalized then sharpened with superlinear exponent to penalize high queues\n    max_queue = np.max(queue_lengths)\n    denom_q = max(1.0, max_queue)\n    q_norm = queue_lengths / (denom_q + eps)\n    q_score = np.power(q_norm, 1.6)  # stronger penalty for larger queues\n\n    # 4) Distance penalty (propagation delay): normalized linear penalty\n    max_dist = np.max(distances) if np.max(distances) > 0 else 1.0\n    dist_score = distances / (max_dist + eps)\n\n    # Weights chosen to provide modest adjustments (different from previous settings)\n    w_arc = 0.10    # positive weight for closeness to destination (reduced slightly)\n    w_proc = 0.12   # positive weight for higher processing rate (increased)\n    w_queue = 0.20  # negative weight for queue congestion (increased)\n    w_dist = 0.03   # negative weight for propagation distance (increased a bit)\n\n    offsets = (w_arc * arc_score) + (w_proc * proc_score) - (w_queue * q_score) - (w_dist * dist_score)\n\n    # Extra encouragement for direct delivery (arc_length == 0) but slightly larger than before\n    direct_mask = (arc_lengths == 0) & (~invalid)\n    offsets[direct_mask] += 0.05\n\n    # Clip offsets to a small range so they refine but don't dominate learned Q-values (narrower range)\n    offsets = np.clip(offsets, -0.12, 0.12)\n\n    # Ensure invalid/padded neighbours have zero offset\n    offsets[invalid] = 0.0\n\n    return offsets",
          "objective": 1.5318,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.605626708584327,
               "avg_dropped_ratio": 0.07228915662650602
          }
     },
     {
          "algorithm": "Components: input conversion, invalid mask, feature scoring (arc inverse, processing rate, queue, distance), weighted combination, clipping, invalid zeroing.\nOverfitting risks: global max normalization, superlinear exponent, explicit direct boost and many tuned scalars can overfit to training-range values.\nSimplifications applied: per-row (axis=-1) normalization, remove exponent and direct-boost, simpler stable denominators, conservative clipping.\n\n",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs: distances, arc_lengths, processing_rates, queue_lengths (numpy arrays, same shape)\n    Output: offsets (numpy array, same shape)\n    \"\"\"\n    distances = np.asarray(distances, dtype=np.float64)\n    arc_lengths = np.asarray(arc_lengths, dtype=np.float64)\n    processing_rates = np.asarray(processing_rates, dtype=np.float64)\n    queue_lengths = np.asarray(queue_lengths, dtype=np.float64)\n    \n    eps = 1e-8\n\n    # Mask invalid / padded neighbours (all-zero feature rows)\n    invalid = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n    \n    # Per-row (last-axis) normalizations to avoid global-stat overfitting\n    # 1) Proximity to destination: inverse arc length, normalized per-row\n    inv_arc = 1.0 / (arc_lengths + eps)\n    max_inv_arc = np.max(inv_arc, axis=-1, keepdims=True)\n    arc_score = inv_arc / np.maximum(max_inv_arc, eps)\n    \n    # 2) Processing rate: higher is better, normalized per-row\n    max_proc = np.max(processing_rates, axis=-1, keepdims=True)\n    proc_score = processing_rates / np.maximum(max_proc, eps)\n    \n    # 3) Queue congestion: higher is worse, linear normalized per-row\n    max_queue = np.max(queue_lengths, axis=-1, keepdims=True)\n    q_score = queue_lengths / np.maximum(max_queue, eps)\n    \n    # 4) Distance penalty: longer distance slightly worse, normalized per-row\n    max_dist = np.max(distances, axis=-1, keepdims=True)\n    dist_score = distances / np.maximum(max_dist, eps)\n    \n    # Conservative, simple weights\n    w_arc = 0.12\n    w_proc = 0.08\n    w_queue = 0.14\n    w_dist = 0.02\n    \n    offsets = (w_arc * arc_score) + (w_proc * proc_score) - (w_queue * q_score) - (w_dist * dist_score)\n    \n    # Clip offsets to a small symmetric range\n    offsets = np.clip(offsets, -0.12, 0.12)\n    \n    # Set offsets for invalid/padded neighbours to zero\n    offsets[invalid] = 0.0\n    \n    return offsets",
          "objective": 1.5289,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6028291085309414,
               "avg_dropped_ratio": 0.0783132530120482
          }
     }
]