[
     {
          "algorithm": "A robust percentile-normalized offset combining a steep exponential reward for short arc lengths, diminishing returns for high processing rates, inverse-distance preference, and a strong sigmoid queue penalty, with tuned weights and a modest tanh scaling (scale=0.12) so offsets refine \u2014 not overpower \u2014 learned Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    def robust_minmax(x, lowp=5.0, highp=95.0):\n        # Percentile-based min-max to reduce outlier influence\n        try:\n            p_low = np.nanpercentile(x, lowp)\n            p_high = np.nanpercentile(x, highp)\n        except IndexError:\n            p_low = np.nanmin(x)\n            p_high = np.nanmax(x)\n        rng = p_high - p_low\n        if rng < 1e-8:\n            # Fallback to global min-max\n            mn = np.nanmin(x)\n            mx = np.nanmax(x)\n            rng2 = mx - mn\n            if rng2 < 1e-8:\n                return np.zeros_like(x, dtype=float)\n            return np.clip((x - mn) / rng2, 0.0, 1.0)\n        return np.clip((x - p_low) / rng, 0.0, 1.0)\n\n    n_dist = robust_minmax(distances)\n    n_arc = robust_minmax(arc_lengths)\n    n_proc = robust_minmax(processing_rates)\n    n_queue = robust_minmax(queue_lengths)\n\n    # Transforms (all produce values roughly in [0,1])\n    s_arc = np.exp(-5.0 * n_arc)                 # strongly prefer small arc_length\n    s_proc = np.sqrt(n_proc)                     # reward higher processing rates with diminishing returns\n    s_dist = 1.0 / (1.0 + 4.0 * n_dist)          # prefer shorter physical distance (inverse-like)\n    p_queue = 1.0 / (1.0 + np.exp(-8.0 * (n_queue - 0.4)))  # steep sigmoid penalty for moderate+ queues\n\n    # Tuned weights (kept modest so offsets refine Q-values)\n    w_arc = 0.40\n    w_proc = 0.12\n    w_dist = 0.08\n    w_queue = 0.90\n\n    raw = (w_arc * s_arc) + (w_proc * s_proc) + (w_dist * s_dist) - (w_queue * p_queue)\n\n    # Remove global bias and bound to modest magnitude\n    raw_centered = raw - np.median(raw)\n    scale = 0.12\n    offsets = scale * np.tanh(raw_centered)\n\n    return offsets",
          "objective": 1.5636,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.5884834103597613,
               "avg_dropped_ratio": 0.07981927710843373
          }
     },
     {
          "algorithm": "A nonlinear, rank-aware offset that uses robust min-max normalization followed by exponential/sigmoid-like transforms to reward short arc, high processing rate, and short distance while strongly penalizing high queue lengths, then centers and bounds the combined score with tanh so offsets remain modest.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    def minmax_norm(x):\n        mn = np.nanmin(x)\n        mx = np.nanmax(x)\n        rng = mx - mn\n        if rng < 1e-8:\n            return np.zeros_like(x, dtype=float)\n        return (x - mn) / rng\n\n    n_dist = minmax_norm(distances)\n    n_arc = minmax_norm(arc_lengths)\n    n_proc = minmax_norm(processing_rates)\n    n_queue = minmax_norm(queue_lengths)\n\n    # Nonlinear desirability transforms (range roughly [0,1])\n    s_arc = np.exp(-3.0 * n_arc)                   # strongly prefer small arc_length\n    s_proc = 1.0 - np.exp(-3.0 * n_proc)           # reward higher processing rates (saturates)\n    s_dist = 1.0 - np.power(n_dist, 1.2)           # prefer shorter physical distance (mild nonlinearity)\n    p_queue = np.power(n_queue, 2.0)               # penalize large queues (quadratic penalty)\n\n    # Tuned weights (kept modest so offsets refine Q-values)\n    w_arc = 0.50\n    w_proc = 0.20\n    w_dist = 0.10\n    w_queue = 0.80\n\n    raw = (w_arc * s_arc) + (w_proc * s_proc) + (w_dist * s_dist) - (w_queue * p_queue)\n\n    # Center to remove global bias and bound to modest magnitude\n    raw_centered = raw - np.median(raw)\n    scale = 0.20\n    offsets = scale * np.tanh(raw_centered)\n\n    return offsets",
          "objective": 1.4891,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6230038219577101,
               "avg_dropped_ratio": 0.07228915662650602
          }
     },
     {
          "algorithm": "A robust, percentile-clipped min-max normalized nonlinear scorer that more strongly penalizes queue occupancy, more sharply rewards short arc and high processing rate via exponential/sigmoid transforms, then centers and bounds the combined score with a smaller tanh scale to produce modest offsets.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    def robust_minmax_norm(x, lowp=5.0, highp=95.0):\n        # Clip to [p5, p95] to reduce outlier influence, then min-max normalize\n        p_low = np.nanpercentile(x, lowp)\n        p_high = np.nanpercentile(x, highp)\n        if np.isclose(p_high, p_low):\n            mn = np.nanmin(x)\n            mx = np.nanmax(x)\n            rng = mx - mn\n            if rng < 1e-8:\n                return np.zeros_like(x, dtype=float)\n            return (x - mn) / rng\n        x_clipped = np.clip(x, p_low, p_high)\n        return (x_clipped - p_low) / (p_high - p_low)\n\n    n_dist = robust_minmax_norm(distances)\n    n_arc = robust_minmax_norm(arc_lengths)\n    n_proc = robust_minmax_norm(processing_rates)\n    n_queue = robust_minmax_norm(queue_lengths)\n\n    # Nonlinear desirability transforms\n    s_arc = np.exp(-4.0 * n_arc)                   # stronger preference for small arc_length\n    s_proc = 1.0 / (1.0 + np.exp(-6.0 * (n_proc - 0.6)))  # sigmoid favoring higher processing rate\n    s_dist = np.exp(-2.5 * n_dist)                 # prefer shorter physical distance\n    p_queue = np.power(n_queue, 2.5)               # stronger penalty for larger queues\n\n    # Tuned weights (kept modest so offsets refine Q-values)\n    w_arc = 0.60\n    w_proc = 0.15\n    w_dist = 0.10\n    w_queue = 1.00\n\n    raw = (w_arc * s_arc) + (w_proc * s_proc) + (w_dist * s_dist) - (w_queue * p_queue)\n\n    # Remove global bias and bound to modest magnitude\n    raw_centered = raw - np.mean(raw)\n    scale = 0.12\n    offsets = scale * np.tanh(raw_centered)\n\n    return offsets",
          "objective": 1.4865,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6220696356088694,
               "avg_dropped_ratio": 0.0753012048192771
          }
     },
     {
          "algorithm": "",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    def robust_unit_scale(x):\n        # Robust median/MAD based scaling to [0,1], fallback to min-max if needed\n        med = np.nanmedian(x)\n        mad = np.nanmedian(np.abs(x - med))\n        scale = 1.4826 * mad  # approximate std\n        if scale < 1e-8:\n            mn = np.nanmin(x)\n            mx = np.nanmax(x)\n            if mx - mn < 1e-8:\n                return np.zeros_like(x, dtype=float)\n            return np.clip((x - mn) / (mx - mn), 0.0, 1.0)\n        z = (x - med) / scale\n        z = np.clip(z, -5.0, 5.0)         # limit influence of extreme outliers\n        return (z + 5.0) / 10.0           # map to [0,1]\n\n    n_dist = robust_unit_scale(distances)\n    n_arc = robust_unit_scale(arc_lengths)\n    n_proc = robust_unit_scale(processing_rates)\n    n_queue = robust_unit_scale(queue_lengths)\n\n    # Simple, less-tuned transforms that generalize better\n    s_arc = 1.0 / (1.0 + n_arc)          # prefer smaller arc lengths\n    s_dist = 1.0 / (1.0 + n_dist)        # prefer shorter distances\n    s_proc = n_proc                      # higher processing rate is better (already in [0,1])\n    p_queue = n_queue                    # higher queue is worse (already in [0,1])\n\n    # Combine with balanced contributions (no heavily tuned weights)\n    raw = (s_arc + s_dist + s_proc - p_queue) / 4.0\n\n    # Center and bound to modest magnitude\n    raw_centered = raw - np.nanmedian(raw)\n    scale = 0.10\n    offsets = scale * np.tanh(raw_centered)\n\n    return offsets",
          "objective": 1.4843,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6158754382823302,
               "avg_dropped_ratio": 0.0858433734939759
          }
     }
]