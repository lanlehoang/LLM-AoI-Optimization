[
     {
          "algorithm": "A robust percentile-normalized offset combining a steep exponential reward for short arc lengths, diminishing returns for high processing rates, inverse-distance preference, and a strong sigmoid queue penalty, with tuned weights and a modest tanh scaling (scale=0.12) so offsets refine \u2014 not overpower \u2014 learned Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    def robust_minmax(x, lowp=5.0, highp=95.0):\n        # Percentile-based min-max to reduce outlier influence\n        try:\n            p_low = np.nanpercentile(x, lowp)\n            p_high = np.nanpercentile(x, highp)\n        except IndexError:\n            p_low = np.nanmin(x)\n            p_high = np.nanmax(x)\n        rng = p_high - p_low\n        if rng < 1e-8:\n            # Fallback to global min-max\n            mn = np.nanmin(x)\n            mx = np.nanmax(x)\n            rng2 = mx - mn\n            if rng2 < 1e-8:\n                return np.zeros_like(x, dtype=float)\n            return np.clip((x - mn) / rng2, 0.0, 1.0)\n        return np.clip((x - p_low) / rng, 0.0, 1.0)\n\n    n_dist = robust_minmax(distances)\n    n_arc = robust_minmax(arc_lengths)\n    n_proc = robust_minmax(processing_rates)\n    n_queue = robust_minmax(queue_lengths)\n\n    # Transforms (all produce values roughly in [0,1])\n    s_arc = np.exp(-5.0 * n_arc)                 # strongly prefer small arc_length\n    s_proc = np.sqrt(n_proc)                     # reward higher processing rates with diminishing returns\n    s_dist = 1.0 / (1.0 + 4.0 * n_dist)          # prefer shorter physical distance (inverse-like)\n    p_queue = 1.0 / (1.0 + np.exp(-8.0 * (n_queue - 0.4)))  # steep sigmoid penalty for moderate+ queues\n\n    # Tuned weights (kept modest so offsets refine Q-values)\n    w_arc = 0.40\n    w_proc = 0.12\n    w_dist = 0.08\n    w_queue = 0.90\n\n    raw = (w_arc * s_arc) + (w_proc * s_proc) + (w_dist * s_dist) - (w_queue * p_queue)\n\n    # Remove global bias and bound to modest magnitude\n    raw_centered = raw - np.median(raw)\n    scale = 0.12\n    offsets = scale * np.tanh(raw_centered)\n\n    return offsets",
          "objective": 1.5636,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.5884834103597613,
               "avg_dropped_ratio": 0.07981927710843373
          }
     },
     {
          "algorithm": "A robust MAD-percentile-normalized offset that multiplicatively blends a strong Gaussian-like bonus for very short arc lengths, a log-diminishing benefit for processing rate, a superlinear inverse-distance preference, and a steep superlinear queue penalty, median-centers the combined score and applies a small tanh scaling so offsets refine (not overpower) learned Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n    \n    def robust_percentile_scale(x, lowp=10.0, highp=90.0):\n        # Percentile-based min-max scaling to [0,1] robust to outliers\n        try:\n            p_low = np.nanpercentile(x, lowp)\n            p_high = np.nanpercentile(x, highp)\n        except Exception:\n            p_low = np.nanmin(x)\n            p_high = np.nanmax(x)\n        rng = p_high - p_low\n        if rng < 1e-8:\n            mn = np.nanmin(x)\n            mx = np.nanmax(x)\n            rng2 = mx - mn\n            if rng2 < 1e-8:\n                return np.zeros_like(x, dtype=float)\n            return np.clip((x - mn) / rng2, 0.0, 1.0)\n        return np.clip((x - p_low) / rng, 0.0, 1.0)\n    \n    n_dist = robust_percentile_scale(distances, lowp=5.0, highp=95.0)\n    n_arc = robust_percentile_scale(arc_lengths, lowp=5.0, highp=95.0)\n    n_proc = robust_percentile_scale(processing_rates, lowp=5.0, highp=95.0)\n    n_queue = robust_percentile_scale(queue_lengths, lowp=5.0, highp=95.0)\n    \n    # Feature transforms (range roughly [0,1])\n    # Strong preference for very short arc lengths (mixture of broad and very sharp Gaussian-like terms)\n    s_arc = np.exp(-4.5 * n_arc) + 0.22 * np.exp(-28.0 * n_arc)\n    \n    # Diminishing returns for processing rate (log-like)\n    s_proc = np.log1p(5.5 * n_proc) / np.log1p(5.5)\n    \n    # Inverse-distance with superlinear emphasis on short links\n    s_dist = np.maximum(0.0, 1.0 - np.power(n_dist, 1.35))\n    \n    # Steep superlinear penalty for queue (makes moderate+ queues costly)\n    p_queue = np.clip(1.2 * np.power(n_queue, 1.9), 0.0, 1.0)\n    \n    # Tuned modest weights so offsets refine learned Q-values\n    w_arc = 0.36\n    w_proc = 0.14\n    w_dist = 0.10\n    w_queue = 0.80\n    \n    raw = (w_arc * s_arc) + (w_proc * s_proc) + (w_dist * s_dist) - (w_queue * p_queue)\n    \n    # Remove global bias using median to be robust to a few extreme neighbors\n    raw_centered = raw - np.median(raw)\n    \n    # Small scaling so offsets are modest compared to Q-values observed in examples\n    scale = 0.10\n    # Slight sharpening before tanh to preserve sign and relative differences\n    offsets = scale * np.tanh(1.1 * raw_centered)\n    \n    # If an entry is clearly a padded/missing neighbour (all features zero), keep offset zero\n    pad_mask = (distances == 0.0) & (arc_lengths == 0.0) & (processing_rates == 0.0) & (queue_lengths == 0.0)\n    if np.any(pad_mask):\n        offsets = offsets.copy()\n        offsets[pad_mask] = 0.0\n    \n    return offsets",
          "objective": 1.5073,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6124678669576987,
               "avg_dropped_ratio": 0.07680722891566265
          }
     },
     {
          "algorithm": "A robust robust-zscore-and-sigmoid based offset that maps each feature to a bounded utility (favoring short arc lengths, high processing rates, short distances, and penalizing large queues) combines them with tuned weights, recenters the result, and applies a modest tanh scaling so offsets gently refine the agent Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    def robust_zscore(x):\n        # median-based zscore with MAD; fallback to std if MAD is zero\n        med = np.nanmedian(x)\n        mad = np.nanmedian(np.abs(x - med))\n        # convert MAD to consistent std estimate\n        denom = 1.4826 * mad\n        if denom < 1e-8:\n            denom = np.nanstd(x)\n        if denom < 1e-8:\n            # everything identical -> return zeros\n            return np.zeros_like(x, dtype=float)\n        return (x - med) / denom\n\n    def sigmoid(x):\n        return 1.0 / (1.0 + np.exp(-x))\n\n    # Robust standardized features\n    z_arc = robust_zscore(arc_lengths)\n    # processing rates benefit from log scaling (diminishing returns)\n    z_proc = robust_zscore(np.log1p(processing_rates))\n    z_dist = robust_zscore(distances)\n    z_queue = robust_zscore(queue_lengths)\n\n    # Map standardized features to utility in [-1, 1]\n    # Shorter arc -> higher utility\n    u_arc = 2.0 * sigmoid(-1.6 * z_arc) - 1.0\n\n    # Higher processing rate -> positive but diminishing returns\n    u_proc = 2.0 * sigmoid(1.1 * z_proc) - 1.0\n\n    # Shorter physical distance preferred (inverse-ish)\n    u_dist = 2.0 * sigmoid(-0.9 * z_dist) - 1.0\n\n    # Queue: strong penalty for moderate+ queues, mild bonus for very low queues\n    u_queue = -(2.0 * sigmoid(3.2 * (z_queue - 0.25)) - 1.0)\n\n    # Tuned weights that keep offsets modest relative to learned Q-values\n    w_arc = 0.42\n    w_proc = 0.11\n    w_dist = 0.07\n    w_queue = 0.95\n\n    raw = (w_arc * u_arc) + (w_proc * u_proc) + (w_dist * u_dist) + (w_queue * u_queue)\n\n    # Remove global bias so offsets are relative across neighbours\n    raw_centered = raw - np.nanmedian(raw)\n\n    # Final gentle scaling so offsets refine but do not overpower Q-values\n    scale = 0.11\n    offsets = scale * np.tanh(raw_centered)\n\n    return offsets",
          "objective": 1.5027,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6033166528784635,
               "avg_dropped_ratio": 0.09337349397590362
          }
     },
     {
          "algorithm": "A percentile-robust scorer that uses milder exponential preference for short arcs, stronger reward for processing rate via a log transform, an inverse-distance power preference, and a softened sigmoid queue penalty with re-tuned weights and a reduced tanh scaling so offsets refine but differ from the original settings.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    def robust_minmax(x, lowp=2.0, highp=98.0):\n        # Percentile-based min-max to reduce outlier influence\n        try:\n            p_low = np.nanpercentile(x, lowp)\n            p_high = np.nanpercentile(x, highp)\n        except Exception:\n            p_low = np.nanmin(x)\n            p_high = np.nanmax(x)\n        rng = p_high - p_low\n        if rng < 1e-8:\n            mn = np.nanmin(x)\n            mx = np.nanmax(x)\n            rng2 = mx - mn\n            if rng2 < 1e-8:\n                return np.zeros_like(x, dtype=float)\n            return np.clip((x - mn) / rng2, 0.0, 1.0)\n        return np.clip((x - p_low) / rng, 0.0, 1.0)\n\n    n_dist = robust_minmax(distances)\n    n_arc = robust_minmax(arc_lengths)\n    n_proc = robust_minmax(processing_rates)\n    n_queue = robust_minmax(queue_lengths)\n\n    # Transforms (producing values roughly in [0,1])\n    s_arc = np.exp(-3.5 * n_arc)                      # milder exponential preference for short arc_length\n    # log-based diminishing returns for processing rate (more sensitive at low rates)\n    s_proc = np.log1p(4.0 * n_proc) / np.log1p(4.0)\n    s_dist = 1.0 - np.power(n_dist, 1.3)              # prefer shorter physical distance with a power transform\n    # softened sigmoid queue penalty: center shifted to 0.3, gentler slope\n    p_queue = 1.0 / (1.0 + np.exp(-6.0 * (n_queue - 0.30)))\n\n    # Re-tuned weights (still modest so offsets refine Q-values)\n    w_arc = 0.30\n    w_proc = 0.20\n    w_dist = 0.15\n    w_queue = 0.75\n\n    raw = (w_arc * s_arc) + (w_proc * s_proc) + (w_dist * s_dist) - (w_queue * p_queue)\n\n    # Center and bound to modest magnitude (reduced scale from original)\n    raw_centered = raw - np.median(raw)\n    scale = 0.08\n    offsets = scale * np.tanh(raw_centered)\n\n    return offsets",
          "objective": 1.4915,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6109051978010089,
               "avg_dropped_ratio": 0.088855421686747
          }
     }
]