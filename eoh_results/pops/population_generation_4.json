[
     {
          "algorithm": "A robust percentile-normalized offset combining a steep exponential reward for short arc lengths, diminishing returns for high processing rates, inverse-distance preference, and a strong sigmoid queue penalty, with tuned weights and a modest tanh scaling (scale=0.12) so offsets refine \u2014 not overpower \u2014 learned Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    def robust_minmax(x, lowp=5.0, highp=95.0):\n        # Percentile-based min-max to reduce outlier influence\n        try:\n            p_low = np.nanpercentile(x, lowp)\n            p_high = np.nanpercentile(x, highp)\n        except IndexError:\n            p_low = np.nanmin(x)\n            p_high = np.nanmax(x)\n        rng = p_high - p_low\n        if rng < 1e-8:\n            # Fallback to global min-max\n            mn = np.nanmin(x)\n            mx = np.nanmax(x)\n            rng2 = mx - mn\n            if rng2 < 1e-8:\n                return np.zeros_like(x, dtype=float)\n            return np.clip((x - mn) / rng2, 0.0, 1.0)\n        return np.clip((x - p_low) / rng, 0.0, 1.0)\n\n    n_dist = robust_minmax(distances)\n    n_arc = robust_minmax(arc_lengths)\n    n_proc = robust_minmax(processing_rates)\n    n_queue = robust_minmax(queue_lengths)\n\n    # Transforms (all produce values roughly in [0,1])\n    s_arc = np.exp(-5.0 * n_arc)                 # strongly prefer small arc_length\n    s_proc = np.sqrt(n_proc)                     # reward higher processing rates with diminishing returns\n    s_dist = 1.0 / (1.0 + 4.0 * n_dist)          # prefer shorter physical distance (inverse-like)\n    p_queue = 1.0 / (1.0 + np.exp(-8.0 * (n_queue - 0.4)))  # steep sigmoid penalty for moderate+ queues\n\n    # Tuned weights (kept modest so offsets refine Q-values)\n    w_arc = 0.40\n    w_proc = 0.12\n    w_dist = 0.08\n    w_queue = 0.90\n\n    raw = (w_arc * s_arc) + (w_proc * s_proc) + (w_dist * s_dist) - (w_queue * p_queue)\n\n    # Remove global bias and bound to modest magnitude\n    raw_centered = raw - np.median(raw)\n    scale = 0.12\n    offsets = scale * np.tanh(raw_centered)\n\n    return offsets",
          "objective": 1.5636,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.5884834103597613,
               "avg_dropped_ratio": 0.07981927710843373
          }
     },
     {
          "algorithm": "A robust, percentile-normalized offset that uses an exponential preference for short arc lengths, a log-compression reward for higher processing rates, a power-law inverse-distance preference, and a strong power-law queue penalty, combined with median-centering and a modest tanh scaling so offsets gently refine the agent Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    def robust_norm(x, lowp=10.0, highp=90.0):\n        \"\"\"Percentile-based robust min-max normalization to [0,1].\"\"\"\n        x_flat = np.asarray(x, dtype=float)\n        try:\n            p_low = np.nanpercentile(x_flat, lowp)\n            p_high = np.nanpercentile(x_flat, highp)\n        except Exception:\n            p_low = np.nanmin(x_flat)\n            p_high = np.nanmax(x_flat)\n        rng = p_high - p_low\n        if rng < 1e-8:\n            mn = np.nanmin(x_flat)\n            mx = np.nanmax(x_flat)\n            rng2 = mx - mn\n            if rng2 < 1e-8:\n                return np.zeros_like(x_flat, dtype=float)\n            return np.clip((x_flat - mn) / rng2, 0.0, 1.0)\n        return np.clip((x_flat - p_low) / rng, 0.0, 1.0)\n\n    n_dist = robust_norm(distances)\n    n_arc = robust_norm(arc_lengths)\n    n_proc = robust_norm(processing_rates)\n    n_queue = robust_norm(queue_lengths)\n\n    # Feature transforms (all roughly in [0,1])\n    s_arc = np.exp(-4.5 * n_arc)                         # strong preference for small arc_length\n    s_proc = np.log1p(6.0 * n_proc) / np.log1p(6.0)      # diminishing returns for processing rate\n    s_dist = (1.0 - n_dist) ** 1.2                        # prefer shorter physical distance\n    p_queue = n_queue ** 2.2                              # strong, increasing penalty for queue\n\n    # Weights chosen to refine, not overpower, learned Q-values\n    w_arc = 0.42\n    w_proc = 0.14\n    w_dist = 0.09\n    w_queue = 0.85\n\n    raw = (w_arc * s_arc) + (w_proc * s_proc) + (w_dist * s_dist) - (w_queue * p_queue)\n\n    # Center to remove global bias and apply gentle nonlinearity so offsets are modest\n    raw_centered = raw - np.median(raw)\n    scale = 0.11\n    offsets = scale * np.tanh(raw_centered)\n\n    # Preserve input shape\n    return offsets",
          "objective": 1.5506,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.5953697254862066,
               "avg_dropped_ratio": 0.07680722891566265
          }
     },
     {
          "algorithm": "A percentile-robust offset that more strongly penalizes queue congestion, uses a steeper exponential preference for very short arc lengths, logarithmic diminishing returns for processing rates, an inverse-square distance benefit, and a reduced tanh output scale so offsets refine learned Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    def robust_minmax(x, lowp=2.0, highp=98.0):\n        # Percentile-based min-max to reduce outlier influence\n        try:\n            p_low = np.nanpercentile(x, lowp)\n            p_high = np.nanpercentile(x, highp)\n        except Exception:\n            p_low = np.nanmin(x)\n            p_high = np.nanmax(x)\n        rng = p_high - p_low\n        if rng < 1e-8:\n            mn = np.nanmin(x)\n            mx = np.nanmax(x)\n            rng2 = mx - mn\n            if rng2 < 1e-8:\n                return np.zeros_like(x, dtype=float)\n            return np.clip((x - mn) / rng2, 0.0, 1.0)\n        return np.clip((x - p_low) / rng, 0.0, 1.0)\n\n    n_dist = robust_minmax(distances)\n    n_arc = robust_minmax(arc_lengths)\n    n_proc = robust_minmax(processing_rates)\n    n_queue = robust_minmax(queue_lengths)\n\n    # Transforms (map roughly into [0,1] range)\n    s_arc = np.exp(-6.5 * (n_arc ** 1.15))                     # stronger preference for very small arc_length\n    # Logarithmic diminishing returns for processing rate, robust to zeros\n    s_proc = np.log1p(6.0 * n_proc) / np.log1p(6.0)\n    s_dist = 1.0 / (1.0 + 9.0 * (n_dist ** 2.0))               # inverse-square style distance preference\n    p_queue = 1.0 / (1.0 + np.exp(-10.0 * (n_queue - 0.28)))  # steeper sigmoid penalty shifted slightly left\n\n    # Tuned weights (different from original)\n    w_arc = 0.35\n    w_proc = 0.18\n    w_dist = 0.06\n    w_queue = 1.10\n\n    raw = (w_arc * s_arc) + (w_proc * s_proc) + (w_dist * s_dist) - (w_queue * p_queue)\n\n    # Center to remove global bias and apply modest final scaling so offsets refine Q-values\n    raw_centered = raw - np.mean(raw)\n    scale = 0.09\n    offsets = scale * np.tanh(raw_centered)\n\n    # Ensure numeric stability and same shape\n    offsets = np.asarray(offsets, dtype=float)\n    return offsets",
          "objective": 1.5267,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.604693773226471,
               "avg_dropped_ratio": 0.07680722891566265
          }
     },
     {
          "algorithm": "A balanced offset combining milder exponential preference for short arc lengths, increased weight on processing rate with a log-like diminishing return, stronger inverse-distance preference, and a slightly reduced but still steep sigmoid queue penalty, all percentile-normalized and modestly scaled via tanh to gently refine agent Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    def robust_minmax(x, lowp=5.0, highp=95.0):\n        # Percentile-based min-max normalization to reduce outlier influence\n        if x.size == 0:\n            return x.astype(float)\n        try:\n            p_low = np.nanpercentile(x, lowp)\n            p_high = np.nanpercentile(x, highp)\n        except Exception:\n            p_low = np.nanmin(x)\n            p_high = np.nanmax(x)\n        rng = p_high - p_low\n        if rng < 1e-8:\n            mn = np.nanmin(x)\n            mx = np.nanmax(x)\n            rng2 = mx - mn\n            if rng2 < 1e-8:\n                return np.zeros_like(x, dtype=float)\n            return np.clip((x - mn) / rng2, 0.0, 1.0)\n        return np.clip((x - p_low) / rng, 0.0, 1.0)\n\n    n_dist = robust_minmax(distances)\n    n_arc = robust_minmax(arc_lengths)\n    n_proc = robust_minmax(processing_rates)\n    n_queue = robust_minmax(queue_lengths)\n\n    # Feature transforms (produce values roughly in [0,1])\n    s_arc = np.exp(-3.0 * n_arc)                     # milder preference for small arc_length\n    # log-like diminishing returns for processing rate\n    s_proc = np.log1p(3.0 * n_proc) / np.log1p(3.0)\n    s_dist = 1.0 / (1.0 + 2.5 * n_dist)              # stronger inverse-distance preference\n    p_queue = 1.0 / (1.0 + np.exp(-10.0 * (n_queue - 0.5)))  # steep sigmoid penalty centered at 0.5\n\n    # Tuned weights (different from original; offsets remain modest)\n    w_arc = 0.30\n    w_proc = 0.20\n    w_dist = 0.15\n    w_queue = 0.70\n\n    raw = (w_arc * s_arc) + (w_proc * s_proc) + (w_dist * s_dist) - (w_queue * p_queue)\n\n    # Remove global bias and bound to modest magnitude so offsets refine learned Q-values\n    raw_centered = raw - np.median(raw)\n    scale = 0.10\n    offsets = scale * np.tanh(raw_centered)\n\n    return offsets",
          "objective": 1.521,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.5970823151030327,
               "avg_dropped_ratio": 0.09186746987951808
          }
     }
]