[
     {
          "algorithm": "I compute small additive offsets by min-max normalizing per-neighbor features, rewarding small arc_length, high processing_rate and short distance while penalizing high queue_length, then combining them with tuned weights and a modest global scale so offsets refine (not overpower) learned Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    def minmax_norm(x):\n        mn = np.nanmin(x)\n        mx = np.nanmax(x)\n        rng = mx - mn\n        if rng < 1e-8:\n            return np.zeros_like(x, dtype=float)\n        return (x - mn) / rng\n\n    n_dist = minmax_norm(distances)\n    n_arc = minmax_norm(arc_lengths)\n    n_proc = minmax_norm(processing_rates)\n    n_queue = minmax_norm(queue_lengths)\n\n    # Scores: higher is better\n    score_arc = 1.0 - n_arc       # closer to destination is better\n    score_proc = n_proc           # higher processing rate is better\n    score_dist = 1.0 - n_dist     # shorter physical distance is better\n    score_queue_pen = n_queue     # higher queue -> larger penalty\n\n    # Weighted combination (tuned to produce modest offsets)\n    w_arc = 0.50\n    w_proc = 0.20\n    w_dist = 0.10\n    w_queue = 1.00  # penalty weight\n\n    raw = (w_arc * score_arc) + (w_proc * score_proc) + (w_dist * score_dist) - (w_queue * score_queue_pen)\n\n    # Global scale to keep offsets small relative to Q-values seen in examples\n    scale = 0.22\n    offsets = raw * scale\n\n    return offsets",
          "objective": 1.4794,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6087701727443906,
               "avg_dropped_ratio": 0.09939759036144578
          }
     },
     {
          "algorithm": "Combine normalized, robust feature scores (distance, arc length, processing rate, queue length) into a small bounded offset that rewards short hops, small remaining arc, and high processing rate while penalizing high queue occupancy, with missing (zero-padded) neighbours strongly discouraged.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n      - offsets: numpy array of same shape\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    # detect padded/missing neighbour entries (all features zero)\n    present = ~((distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0))\n\n    # prepare output\n    offsets = np.zeros_like(distances, dtype=float)\n    if not np.any(present):\n        # nothing present, return zeros\n        return offsets\n\n    eps = 1e-8\n\n    # robust normalization function: returns array in [0,1] for present entries, zeros for absent;\n    # if feature is constant among present entries, treat its contribution as neutral (zeros)\n    def normalize_feature(x):\n        x = np.asarray(x, dtype=float)\n        out = np.zeros_like(x)\n        xm = x[present]\n        mn = np.min(xm)\n        mx = np.max(xm)\n        if mx - mn < eps:\n            # constant -> neutral (zero)\n            out[present] = 0.0\n        else:\n            out = (x - mn) / (mx - mn + eps)\n            out[~present] = 0.0\n        return out\n\n    nd = normalize_feature(distances)          # larger -> worse\n    na = normalize_feature(arc_lengths)        # larger -> worse (farther from dest)\n    nproc = normalize_feature(processing_rates) # larger -> better\n    nq = normalize_feature(queue_lengths)      # larger -> worse\n\n    # weights chosen to refine (not overpower) agent Q-values; final offsets clipped to small range\n    w_arc = 0.06      # reward being closer to destination (smaller arc_length)\n    w_proc = 0.05     # reward higher processing rate\n    w_dist = 0.02     # penalize long propagation distance\n    w_queue = 0.12    # penalize long queues (nonlinear)\n\n    # components:\n    arc_comp = w_arc * (1.0 - na)                  # closer => larger positive\n    proc_comp = w_proc * nproc                     # faster service => positive\n    dist_comp = w_dist * (1.0 - nd)                # shorter distance => positive\n    queue_comp = - w_queue * (nq ** 1.5)           # high queue => increasing negative penalty\n\n    offsets = arc_comp + proc_comp + dist_comp + queue_comp\n\n    # strongly discourage missing neighbours (matches -inf Q-values in agent)\n    offsets[~present] = -1e6\n\n    # keep offsets modest so they refine but do not dominate learned Q-values\n    offsets = np.clip(offsets, -0.15, 0.15)\n\n    return offsets",
          "objective": 1.4642,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6274240384134825,
               "avg_dropped_ratio": 0.08132530120481929
          }
     },
     {
          "algorithm": "I compute per-neighbour offsets by combining a normalized negative congestion penalty (queue length), a positive proximity bonus (inverse normalized arc length), a positive service-rate bonus (normalized processing rate), and a small distance propagation penalty, with a strong negative mask for zero-padded (missing) neighbours, scaled so offsets refine but do not overpower learned Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n      - offsets: numpy array of same shape\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n    \n    eps = 1e-8\n    # Detect missing (zero-padded) neighbours: all features zero -> treat as missing\n    missing = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n    \n    # Normalize features robustly (per-sample relative scales)\n    # Use max() per array to avoid division by tiny numbers\n    max_dist = max(distances.max(), eps)\n    max_arc = max(arc_lengths.max(), eps)\n    max_proc = max(processing_rates.max(), eps)\n    max_queue = max(queue_lengths.max(), 1.0)  # avoid amplifying when all zeros\n    \n    # Terms (kept small so they refine Q-values)\n    # Congestion penalty: stronger negative effect\n    queue_penalty = -0.03 * (queue_lengths / max(1.0, max_queue)) * (queue_lengths)  # quadratic-ish to emphasize large queues\n    # Processing bonus: higher service rate -> positive\n    proc_bonus = 0.06 * (processing_rates / max_proc)\n    # Arc (proximity) bonus: closer to destination -> positive\n    arc_closeness = 1.0 - (arc_lengths / max_arc)\n    arc_bonus = 0.12 * arc_closeness\n    # Distance penalty: longer propagation -> small negative\n    distance_penalty = -0.02 * (distances / max_dist)\n    \n    offsets = queue_penalty + proc_bonus + arc_bonus + distance_penalty\n    \n    # Clip offsets to a modest range so they don't overpower agent Q-values\n    offsets = np.clip(offsets, -0.6, 0.18)\n    \n    # Strong negative for missing neighbours (to be safe), but keep finite\n    offsets = np.where(missing, -10.0, offsets)\n    \n    return offsets",
          "objective": 1.4275,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6351195229447919,
               "avg_dropped_ratio": 0.09337349397590362
          }
     },
     {
          "algorithm": "I compute a small bounded offset by normalizing each feature, giving positive credit to small arc_length, high processing_rate, and short distance while applying a stronger (quadratic) negative penalty for high queue_length, and combine them with modest weights so the offset refines but does not overpower the agent Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n      - offsets: numpy array of same shape\n    \"\"\"\n    # Ensure numpy arrays\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n\n    def normalize(x):\n        xmin = np.nanmin(x)\n        xmax = np.nanmax(x)\n        denom = xmax - xmin\n        if denom == 0 or np.isnan(denom):\n            return np.zeros_like(x)\n        return (x - xmin) / denom\n\n    # Normalized scores in [0,1]\n    arc_norm = normalize(a)        # larger => farther from dest (worse)\n    proc_norm = normalize(p)       # larger => faster service (better)\n    dist_norm = normalize(d)       # larger => farther (worse)\n    queue_norm = normalize(q)      # larger => more congested (worse)\n\n    # Transformations: higher is better for arc_score, proc_score, dist_score\n    arc_score = 1.0 - arc_norm     # closer to destination -> higher score\n    proc_score = proc_norm         # higher processing rate -> higher score\n    dist_score = 1.0 - dist_norm   # shorter distance -> higher score\n    queue_score = queue_norm**2    # quadratic penalty for congestion (0..1)\n\n    # Weights chosen to be modest so offsets refine learned Q-values\n    w_arc  = 0.12\n    w_proc = 0.10\n    w_dist = 0.06\n    w_q    = 0.30   # applied as a penalty\n\n    offsets = (w_arc * arc_score) + (w_proc * proc_score) + (w_dist * dist_score) - (w_q * queue_score)\n\n    # Preserve shape and return\n    return offsets",
          "objective": 1.426,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6400086080899638,
               "avg_dropped_ratio": 0.08734939759036145
          }
     }
]