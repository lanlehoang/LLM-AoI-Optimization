{
     "algorithm": "I compute a modest, interpretable offset by normalizing each feature and combining them with tuned weights that penalize high queue lengths and long distances while rewarding low arc lengths and high processing rates, then clip the result so it refines but does not overpower the agent's Q-values.",
     "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Compute small offsets to add to agent Q-values based on features.\n    Inputs: numpy arrays of same shape.\n    Output: numpy array 'offsets' of same shape.\n    \"\"\"\n    # Ensure numpy arrays\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n    \n    eps = 1e-8\n\n    def normalize(x):\n        mn = np.nanmin(x)\n        mx = np.nanmax(x)\n        if mx - mn < eps:\n            return np.full_like(x, 0.5)\n        return (x - mn) / (mx - mn + eps)\n\n    # Normalized scores in [0,1]\n    # For features where lower is better (arc_lengths, distances) we invert later\n    norm_dist = normalize(distances)            # higher = worse\n    norm_arc = normalize(arc_lengths)           # higher = worse\n    norm_proc = normalize(processing_rates)     # higher = better\n    norm_queue = normalize(queue_lengths)       # higher = worse\n\n    # Convert to desirability scores in [0,1]\n    dist_score = 1.0 - norm_dist       # higher = better (closer)\n    arc_score = 1.0 - norm_arc         # higher = better (closer to dest)\n    proc_score = norm_proc             # higher = better (faster processing)\n    queue_penalty = norm_queue         # higher = worse (more queue)\n\n    # Weights chosen to refine (not overpower) Q-values; magnitudes compatible with observed Q scale\n    w_queue = -0.35   # strong negative for congestion/drop risk\n    w_arc   = +0.16   # reward being closer to destination\n    w_proc  = +0.12   # reward faster service\n    w_dist  = -0.07   # small penalty for longer propagation distance\n\n    offset_raw = (w_queue * queue_penalty +\n                  w_arc   * arc_score +\n                  w_proc  * proc_score +\n                  w_dist  * dist_score)\n\n    # If an entry is a zero-padding (all features zero), softly discourage it\n    all_zero_mask = (np.isclose(distances, 0.0) &\n                     np.isclose(arc_lengths, 0.0) &\n                     np.isclose(processing_rates, 0.0) &\n                     np.isclose(queue_lengths, 0.0))\n    # Clip to modest range so offsets refine but do not dominate learned Q-values\n    max_offset = 0.25\n    offsets = np.clip(offset_raw, -max_offset, max_offset)\n\n    # Apply a slightly stronger negative offset for padded/missing neighbours, but keep moderate\n    offsets = offsets.copy()\n    offsets[all_zero_mask] = np.minimum(offsets[all_zero_mask], -0.18)\n\n    return offsets",
     "objective": 1.4107,
     "other_inf": null,
     "eval_metrics": {
          "avg_aoi": 0.5914232362110091,
          "avg_dropped_ratio": 0.16566265060240964
     }
}