{
     "algorithm": "Combine multiplicative exponential feature transforms (distance propagation decay, arc-length closeness, normalized throughput) with queue-length damping to produce a probabilistic score, then map score to a small centered tanh-scaled offset so offsets are modest, zero-median, and strongly negative for fully-missing neighbours.",
     "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n      - offsets: numpy array of same shape\n    \"\"\"\n    eps = 1e-9\n    # ensure numpy arrays\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n\n    # preserve shape\n    shape = d.shape\n    d = d.reshape(-1)\n    a = a.reshape(-1)\n    p = p.reshape(-1)\n    q = q.reshape(-1)\n\n    # Detect fully-missing neighbors (all-zero feature vector) to strongly penalize them\n    missing_mask = (np.abs(d) <= eps) & (np.abs(a) <= eps) & (np.abs(p) <= eps) & (np.abs(q) <= eps)\n\n    # Robust scale parameters (use medians to avoid outlier effects)\n    D0 = np.median(d[~missing_mask]) if np.any(~missing_mask) else 1.0\n    A0 = np.median(a[~missing_mask]) if np.any(~missing_mask) else 1.0\n    P0 = np.median(p[~missing_mask]) if np.any(~missing_mask) else 1.0\n    Q0 = np.median(q[~missing_mask]) if np.any(~missing_mask) else 1.0\n\n    # Avoid degenerate scales\n    D0 = max(D0, 1.0)\n    A0 = max(A0, 1.0)\n    P0 = max(P0, 1.0)\n    Q0 = max(Q0, 1.0)\n\n    # Exponential closeness term: neighbors much closer to destination get exponentially higher score\n    closeness_term = np.exp(-a / (A0 * 0.8 + eps))  # smaller arc -> larger value\n\n    # Propagation delay penalty (distance): longer distance reduces score\n    propagation_term = np.exp(-d / (D0 * 3.5 + eps))\n\n    # Throughput advantage: normalized processing rate (favor higher \u03bc), soft-scaled\n    proc_norm = (p + eps) / (P0 + eps)\n    proc_term = np.log1p(proc_norm)  # gentle concave reward for higher \u03bc\n\n    # Congestion damping: queue length reduces score multiplicatively (stronger effect as queue grows)\n    # use soft damping so small queues are allowed but large queues heavily penalized\n    queue_damping = np.exp(- (q / (Q0 + eps)) * 1.2)\n\n    # Combine multiplicatively to form a positive score\n    base_score = closeness_term * propagation_term * proc_term * queue_damping\n\n    # if base_score is all zeros (possible if proc_term=0), add tiny floor to keep transforms stable\n    base_score = base_score + eps\n\n    # Log-transform to compress dynamic range, center by median, then apply tanh to produce bounded offsets\n    log_score = np.log(base_score + 1.0)  # log1p-like compression\n    center = np.median(log_score[~missing_mask]) if np.any(~missing_mask) else np.median(log_score)\n    centered = log_score - center\n\n    # Scaling: choose a gain that keeps offsets modest while allowing meaningful differentiation\n    gain = 0.9\n    raw_offsets = np.tanh(gain * centered)\n\n    # Scale final offsets to a modest amplitude (refinement only)\n    max_amplitude = 0.20\n    offsets = raw_offsets * max_amplitude\n\n    # Strong negative offset for missing neighbours so they are avoided\n    offsets[missing_mask] = -0.22\n\n    # ensure output shape matches input\n    offsets = offsets.reshape(shape)\n\n    return offsets",
     "objective": 1.562,
     "other_inf": null,
     "eval_metrics": {
          "avg_aoi": 0.6131950175099086,
          "avg_dropped_ratio": 0.04216867469879518
     }
}