{
     "algorithm": "A sharper scoring variant emphasizing arc-length closeness and queue penalties: stronger arc and distance exponentials, a square-root processing boost, steeper queue damping, multiplicative combination, and tanh-centered small offsets with a stronger penalty for missing neighbours.",
     "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n      - offsets: numpy array of same shape\n    \"\"\"\n    eps = 1e-9\n    d = np.asarray(distances, dtype=float).reshape(-1)\n    a = np.asarray(arc_lengths, dtype=float).reshape(-1)\n    p = np.asarray(processing_rates, dtype=float).reshape(-1)\n    q = np.asarray(queue_lengths, dtype=float).reshape(-1)\n\n    # Detect fully-missing neighbors (all-zero feature vector)\n    missing_mask = (np.abs(d) <= eps) & (np.abs(a) <= eps) & (np.abs(p) <= eps) & (np.abs(q) <= eps)\n\n    # Robust scales (medians excluding missing)\n    valid_idx = ~missing_mask\n    if np.any(valid_idx):\n        D0 = np.median(d[valid_idx])\n        A0 = np.median(a[valid_idx])\n        P0 = np.median(p[valid_idx])\n        Q0 = np.median(q[valid_idx])\n    else:\n        D0 = A0 = P0 = Q0 = 1.0\n\n    # Prevent degenerate small scales\n    D0 = max(D0, 1.0)\n    A0 = max(A0, 1.0)\n    P0 = max(P0, 1.0)\n    Q0 = max(Q0, 1.0)\n\n    # Parameter settings (different from original)\n    closeness_scale = 0.6    # emphasize arc closeness more strongly\n    propagation_scale = 2.0  # stronger distance penalty\n    queue_coeff = 1.9        # queue damping multiplier\n    proc_power = 0.5         # sqrt-like boost (use power 0.5)\n    closeness_exponent = 1.2 # slightly sharpen arc effect\n    gain = 1.10              # slightly stronger gain before tanh\n    max_amplitude = 0.18     # modest final amplitude\n    missing_penalty = -0.26  # stronger negative offset for missing neighbors\n\n    # Feature transforms\n    closeness_term = np.exp(-a / (A0 * closeness_scale + eps)) ** closeness_exponent\n    propagation_term = np.exp(-d / (D0 * propagation_scale + eps))\n    proc_norm = (p + eps) / (P0 + eps)\n    proc_term = np.power(proc_norm + eps, proc_power)  # sqrt-like boost\n    queue_damping = 1.0 / (1.0 + (q / (Q0 + eps)) * queue_coeff)\n\n    # Combine multiplicatively\n    base_score = closeness_term * propagation_term * proc_term * queue_damping\n    base_score = base_score + eps\n\n    # Compress and center\n    log_score = np.log1p(base_score)\n    if np.any(valid_idx):\n        center = np.median(log_score[valid_idx])\n    else:\n        center = np.median(log_score)\n    centered = log_score - center\n\n    # Map to bounded offsets\n    raw_offsets = np.tanh(gain * centered)\n    offsets = raw_offsets * max_amplitude\n\n    # Strong negative offset for missing neighbours\n    offsets[missing_mask] = missing_penalty\n\n    # Restore original shape\n    offsets = offsets.reshape(np.asarray(distances).shape)\n    return offsets",
     "objective": 1.5981,
     "other_inf": null,
     "eval_metrics": {
          "avg_aoi": 0.6087844272183531,
          "avg_dropped_ratio": 0.02710843373493976
     }
}