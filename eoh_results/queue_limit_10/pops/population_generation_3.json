[
     {
          "algorithm": "I compute a modest, interpretable offset by normalizing each feature and combining them with tuned weights that penalize high queue lengths and long distances while rewarding low arc lengths and high processing rates, then clip the result so it refines but does not overpower the agent's Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Compute small offsets to add to agent Q-values based on features.\n    Inputs: numpy arrays of same shape.\n    Output: numpy array 'offsets' of same shape.\n    \"\"\"\n    # Ensure numpy arrays\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n    \n    eps = 1e-8\n\n    def normalize(x):\n        mn = np.nanmin(x)\n        mx = np.nanmax(x)\n        if mx - mn < eps:\n            return np.full_like(x, 0.5)\n        return (x - mn) / (mx - mn + eps)\n\n    # Normalized scores in [0,1]\n    # For features where lower is better (arc_lengths, distances) we invert later\n    norm_dist = normalize(distances)            # higher = worse\n    norm_arc = normalize(arc_lengths)           # higher = worse\n    norm_proc = normalize(processing_rates)     # higher = better\n    norm_queue = normalize(queue_lengths)       # higher = worse\n\n    # Convert to desirability scores in [0,1]\n    dist_score = 1.0 - norm_dist       # higher = better (closer)\n    arc_score = 1.0 - norm_arc         # higher = better (closer to dest)\n    proc_score = norm_proc             # higher = better (faster processing)\n    queue_penalty = norm_queue         # higher = worse (more queue)\n\n    # Weights chosen to refine (not overpower) Q-values; magnitudes compatible with observed Q scale\n    w_queue = -0.35   # strong negative for congestion/drop risk\n    w_arc   = +0.16   # reward being closer to destination\n    w_proc  = +0.12   # reward faster service\n    w_dist  = -0.07   # small penalty for longer propagation distance\n\n    offset_raw = (w_queue * queue_penalty +\n                  w_arc   * arc_score +\n                  w_proc  * proc_score +\n                  w_dist  * dist_score)\n\n    # If an entry is a zero-padding (all features zero), softly discourage it\n    all_zero_mask = (np.isclose(distances, 0.0) &\n                     np.isclose(arc_lengths, 0.0) &\n                     np.isclose(processing_rates, 0.0) &\n                     np.isclose(queue_lengths, 0.0))\n    # Clip to modest range so offsets refine but do not dominate learned Q-values\n    max_offset = 0.25\n    offsets = np.clip(offset_raw, -max_offset, max_offset)\n\n    # Apply a slightly stronger negative offset for padded/missing neighbours, but keep moderate\n    offsets = offsets.copy()\n    offsets[all_zero_mask] = np.minimum(offsets[all_zero_mask], -0.18)\n\n    return offsets",
          "objective": 1.4107,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.5914232362110091,
               "avg_dropped_ratio": 0.16566265060240964
          }
     },
     {
          "algorithm": "Compute offsets by robustly normalizing features, applying nonlinear transforms to emphasize heavy queues and diminishing returns for processing rate, and linearly combining them with a tuned set of weights that penalize congestion and distance while rewarding proximity to destination and service speed.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Compute offsets to add to agent Q-values based on features.\n    Inputs: numpy arrays of same shape.\n    Output: numpy array 'offsets' of same shape.\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n    \n    eps = 1e-8\n\n    def normalize_min_max(x):\n        mn = np.nanmin(x)\n        mx = np.nanmax(x)\n        if mx - mn < eps:\n            return np.full_like(x, 0.5)\n        return (x - mn) / (mx - mn + eps)\n\n    # Basic min-max normalization\n    norm_dist = normalize_min_max(distances)        # higher = worse\n    norm_arc = normalize_min_max(arc_lengths)      # higher = worse\n    # For processing rates use log scaling before normalization to reduce skew\n    proc_logged = np.log1p(processing_rates)\n    norm_proc = normalize_min_max(proc_logged)      # higher = better\n    norm_queue = normalize_min_max(queue_lengths)   # higher = worse\n\n    # Convert to desirability / penalty scores in [0,1]\n    dist_score = 1.0 - norm_dist        # higher = better (closer)\n    arc_score = 1.0 - norm_arc          # higher = better (closer)\n    # Slightly diminishing returns for processing (sqrt)\n    proc_score = np.sqrt(np.clip(norm_proc, 0.0, 1.0))\n    # Emphasize high queues (nonlinear penalty)\n    queue_penalty = np.power(np.clip(norm_queue, 0.0, 1.0), 1.6)\n\n    # Tuned weights (different from original): stronger queue penalty, balanced rewards\n    w_queue = -0.45   # stronger negative for congestion/drop risk\n    w_arc   = +0.20   # reward being closer to destination\n    w_proc  = +0.14   # reward faster service\n    w_dist  = -0.06   # small penalty for longer propagation distance\n\n    offset_raw = (w_queue * queue_penalty +\n                  w_arc   * arc_score +\n                  w_proc  * proc_score +\n                  w_dist  * dist_score)\n\n    # Detect padded / missing neighbours (all features ~0)\n    all_zero_mask = (np.isclose(distances, 0.0) &\n                     np.isclose(arc_lengths, 0.0) &\n                     np.isclose(processing_rates, 0.0) &\n                     np.isclose(queue_lengths, 0.0))\n\n    # Clip to modest range so offsets refine but do not dominate learned Q-values\n    max_offset = 0.20\n    offsets = np.clip(offset_raw, -max_offset, max_offset)\n\n    # Apply a slightly stronger negative offset for padded/missing neighbours (bounded)\n    offsets = offsets.copy()\n    offsets[all_zero_mask] = np.minimum(offsets[all_zero_mask], -0.20)\n\n    return offsets",
          "objective": 1.3857,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.5846989671382691,
               "avg_dropped_ratio": 0.1897590361445783
          }
     },
     {
          "algorithm": "Compute modest offsets by combining nonlinearly transformed, min-max normalized feature scores with tuned weights that more strongly penalize queue congestion and more strongly reward processing rate and proximity to destination while keeping offsets clipped to a small range.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Compute small offsets to add to agent Q-values based on features.\n    Inputs: numpy arrays of same shape.\n    Output: numpy array 'offsets' of same shape.\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n    \n    eps = 1e-8\n\n    def normalize(x):\n        mn = np.nanmin(x)\n        mx = np.nanmax(x)\n        if mx - mn < eps:\n            return np.full_like(x, 0.5)\n        return (x - mn) / (mx - mn + eps)\n\n    # Basic normalized features in [0,1]\n    n_dist = normalize(distances)        # higher = farther (worse)\n    n_arc = normalize(arc_lengths)       # higher = farther from destination (worse)\n    n_proc = normalize(processing_rates) # higher = better\n    n_queue = normalize(queue_lengths)   # higher = worse\n\n    # Non-linear transforms to emphasize congestion and service effects\n    # Queue penalty grows super-linearly to reflect overflow risk\n    queue_penalty = n_queue ** 1.8       # [0,1], higher worse\n    # Arc desirability: closer -> better, slightly accentuate differences near destination\n    arc_score = 1.0 - (n_arc ** 1.2)\n    # Processing desirability: reward faster servers, mild sublinear gain to emphasize low->mid changes\n    proc_score = np.sqrt(n_proc)\n    # Distance desirability: closer is slightly better (propagation delay)\n    dist_score = 1.0 - n_dist\n\n    # Tuned weights (different from prior algorithm)\n    w_queue = -0.45   # stronger negative for congestion/drop risk\n    w_arc   = +0.22   # stronger reward for being closer to destination\n    w_proc  = +0.18   # stronger reward for faster processing\n    w_dist  = -0.03   # small penalty for longer physical distance\n\n    offset_raw = (w_queue * queue_penalty +\n                  w_arc   * arc_score +\n                  w_proc  * proc_score +\n                  w_dist  * dist_score)\n\n    # Detect padded/missing neighbour entries (all features zero)\n    all_zero_mask = (np.isclose(distances, 0.0) &\n                     np.isclose(arc_lengths, 0.0) &\n                     np.isclose(processing_rates, 0.0) &\n                     np.isclose(queue_lengths, 0.0))\n\n    # Clip to modest range so offsets refine but do not dominate learned Q-values\n    offsets = np.clip(offset_raw, -0.28, 0.22)\n\n    # Apply a stronger negative offset to padded/missing neighbours to discourage selection\n    offsets = offsets.copy()\n    offsets[all_zero_mask] = np.minimum(offsets[all_zero_mask], -0.20)\n\n    return offsets",
          "objective": 1.3793,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.5961447457203972,
               "avg_dropped_ratio": 0.17771084337349397
          }
     },
     {
          "algorithm": "I compute robust, bounded desirability scores per feature using a median/IQR-based sigmoid transform, combine them with multiplicative boost for being both close to destination and fast, penalize queue congestion strongly, include a small distance penalty, and clip to a modest range so offsets refine (not dominate) the agent Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Compute small offsets to add to agent Q-values based on features.\n    Inputs: numpy arrays (or array-like) of same shape:\n        distances, arc_lengths, processing_rates, queue_lengths\n    Output: numpy array 'offsets' of same shape.\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n    \n    eps = 1e-8\n\n    def robust_sigmoid_scale(x):\n        \"\"\"Map x to (0,1) using median/IQR normalization followed by a sigmoid.\n           If IQR is ~0, return constant 0.5 for stability.\"\"\"\n        x = np.asarray(x, dtype=float)\n        med = np.median(x)\n        q75 = np.percentile(x, 75)\n        q25 = np.percentile(x, 25)\n        iqr = q75 - q25\n        if iqr < eps:\n            return np.full_like(x, 0.5)\n        z = (x - med) / (iqr + eps)\n        # modest gain so outputs are smooth but separate values\n        return 1.0 / (1.0 + np.exp(-1.0 * z))\n\n    # Robust [0,1] scores where higher means \"worse\" for dist/arc/queue\n    dist_bad = robust_sigmoid_scale(distances)      # higher = farther = worse\n    arc_bad  = robust_sigmoid_scale(arc_lengths)    # higher = further from dest = worse\n    proc_good = robust_sigmoid_scale(processing_rates)  # higher = better\n    queue_bad = robust_sigmoid_scale(queue_lengths)  # higher = more congested = worse\n\n    # Convert to desirability where higher is better\n    dist_score = 1.0 - dist_bad\n    arc_score  = 1.0 - arc_bad\n    proc_score = proc_good\n    queue_score = queue_bad\n\n    # Combine features:\n    # - reward being close to destination and having good processing (weighted sum),\n    # - extra multiplicative boost if both arc and processing are good,\n    # - strong penalty for queue congestion,\n    # - small penalty for path distance (propagation).\n    w_base = 0.22\n    w_mult = 0.08\n    w_queue = 0.40\n    w_dist = 0.08\n\n    base_good = 0.6 * arc_score + 0.4 * proc_score  # mix closeness and speed\n    multiplicative_boost = arc_score * proc_score\n\n    offset_raw = (w_base * base_good +\n                  w_mult * multiplicative_boost -\n                  w_queue * queue_score -\n                  w_dist * (1.0 - dist_score))\n\n    # Extra modest discouragement for extremely congested neighbours\n    high_cong_mask = queue_score > 0.92\n    offset_raw = offset_raw.copy()\n    offset_raw[high_cong_mask] -= 0.06\n\n    # Clip to modest range so offsets refine but do not dominate Q-values\n    max_abs = 0.25\n    offsets = np.clip(offset_raw, -max_abs, max_abs)\n\n    # Softly penalize zero-padded / missing neighbours\n    all_zero_mask = (np.isclose(distances, 0.0) &\n                     np.isclose(arc_lengths, 0.0) &\n                     np.isclose(processing_rates, 0.0) &\n                     np.isclose(queue_lengths, 0.0))\n    offsets = offsets.copy()\n    offsets[all_zero_mask] = np.minimum(offsets[all_zero_mask], -0.20)\n\n    return offsets",
          "objective": 1.3753,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.614314484104825,
               "avg_dropped_ratio": 0.15512048192771083
          }
     }
]