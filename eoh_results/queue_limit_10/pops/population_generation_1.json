[
     {
          "algorithm": "I compute a modest, interpretable offset by normalizing each feature and combining them with tuned weights that penalize high queue lengths and long distances while rewarding low arc lengths and high processing rates, then clip the result so it refines but does not overpower the agent's Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Compute small offsets to add to agent Q-values based on features.\n    Inputs: numpy arrays of same shape.\n    Output: numpy array 'offsets' of same shape.\n    \"\"\"\n    # Ensure numpy arrays\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n    \n    eps = 1e-8\n\n    def normalize(x):\n        mn = np.nanmin(x)\n        mx = np.nanmax(x)\n        if mx - mn < eps:\n            return np.full_like(x, 0.5)\n        return (x - mn) / (mx - mn + eps)\n\n    # Normalized scores in [0,1]\n    # For features where lower is better (arc_lengths, distances) we invert later\n    norm_dist = normalize(distances)            # higher = worse\n    norm_arc = normalize(arc_lengths)           # higher = worse\n    norm_proc = normalize(processing_rates)     # higher = better\n    norm_queue = normalize(queue_lengths)       # higher = worse\n\n    # Convert to desirability scores in [0,1]\n    dist_score = 1.0 - norm_dist       # higher = better (closer)\n    arc_score = 1.0 - norm_arc         # higher = better (closer to dest)\n    proc_score = norm_proc             # higher = better (faster processing)\n    queue_penalty = norm_queue         # higher = worse (more queue)\n\n    # Weights chosen to refine (not overpower) Q-values; magnitudes compatible with observed Q scale\n    w_queue = -0.35   # strong negative for congestion/drop risk\n    w_arc   = +0.16   # reward being closer to destination\n    w_proc  = +0.12   # reward faster service\n    w_dist  = -0.07   # small penalty for longer propagation distance\n\n    offset_raw = (w_queue * queue_penalty +\n                  w_arc   * arc_score +\n                  w_proc  * proc_score +\n                  w_dist  * dist_score)\n\n    # If an entry is a zero-padding (all features zero), softly discourage it\n    all_zero_mask = (np.isclose(distances, 0.0) &\n                     np.isclose(arc_lengths, 0.0) &\n                     np.isclose(processing_rates, 0.0) &\n                     np.isclose(queue_lengths, 0.0))\n    # Clip to modest range so offsets refine but do not dominate learned Q-values\n    max_offset = 0.25\n    offsets = np.clip(offset_raw, -max_offset, max_offset)\n\n    # Apply a slightly stronger negative offset for padded/missing neighbours, but keep moderate\n    offsets = offsets.copy()\n    offsets[all_zero_mask] = np.minimum(offsets[all_zero_mask], -0.18)\n\n    return offsets",
          "objective": 1.4107,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.5914232362110091,
               "avg_dropped_ratio": 0.16566265060240964
          }
     },
     {
          "algorithm": "Combine normalized feature scores with nonlinear transforms (sqrt for processing, exponent for queue) and tuned weights that strongly penalize queue congestion, reward proximity to destination and higher processing, lightly penalize long distances, then clip to modest offsets and discourage zero-padded neighbours.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Compute offsets to add to agent Q-values based on features.\n    Inputs: numpy arrays of same shape.\n    Output: numpy array 'offsets' of same shape.\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    eps = 1e-8\n\n    def normalize(x):\n        mn = np.nanmin(x)\n        mx = np.nanmax(x)\n        if mx - mn < eps:\n            return np.full_like(x, 0.5)\n        return (x - mn) / (mx - mn + eps)\n\n    # Normalized in [0,1]; higher means larger raw value\n    norm_dist = normalize(distances)        # higher = farther (worse)\n    norm_arc = normalize(arc_lengths)       # higher = larger arc (worse)\n    norm_proc = normalize(processing_rates) # higher = faster (better)\n    norm_queue = normalize(queue_lengths)   # higher = more queued (worse)\n\n    # Derived desirability / risk scores\n    arc_score = 1.0 - norm_arc               # higher = closer to destination (better)\n    proc_score = np.sqrt(np.clip(norm_proc, 0.0, 1.0))   # diminishing returns for high rates\n    queue_risk = np.power(np.clip(norm_queue, 0.0, 1.0), 1.6)  # emphasize high congestion\n    dist_norm = norm_dist                    # directly use normalized distance as small penalty\n\n    # Tuned weights (different from original): strong negative for queue, reward arc & proc, small distance penalty\n    w_arc   = 0.26\n    w_proc  = 0.16\n    w_queue = -0.50\n    w_dist  = -0.04\n\n    offset_raw = (w_arc * arc_score +\n                  w_proc * proc_score +\n                  w_queue * queue_risk +\n                  w_dist * dist_norm)\n\n    # Clip to modest range so offsets refine but do not dominate learned Q-values\n    max_offset = 0.18\n    offsets = np.clip(offset_raw, -max_offset, max_offset)\n\n    # If an entry is a zero-padding (all features zero), discourage it moderately\n    all_zero_mask = (np.isclose(distances, 0.0) &\n                     np.isclose(arc_lengths, 0.0) &\n                     np.isclose(processing_rates, 0.0) &\n                     np.isclose(queue_lengths, 0.0))\n    if np.any(all_zero_mask):\n        offsets = offsets.copy()\n        offsets[all_zero_mask] = np.minimum(offsets[all_zero_mask], -0.16)\n\n    return offsets",
          "objective": 1.3683,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.5899438050839698,
               "avg_dropped_ratio": 0.19277108433734938
          }
     },
     {
          "algorithm": "I compute normalized desirability scores, apply nonlinear transforms to emphasize high queue congestion and diminishing returns on processing rate, combine them with tuned weights (stronger penalty for queue, moderate reward for proximity and processing, mild distance penalty), and clip the final offsets to a small range with extra discouragement for zero-padded neighbours.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Compute offsets to add to agent Q-values based on features.\n    Inputs: numpy arrays of same shape.\n    Output: numpy array 'offsets' of same shape.\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    eps = 1e-8\n\n    def normalize(x):\n        mn = np.nanmin(x)\n        mx = np.nanmax(x)\n        if mx - mn < eps:\n            return np.full_like(x, 0.5)\n        return (x - mn) / (mx - mn + eps)\n\n    # Normalize features to [0,1]\n    norm_dist = normalize(distances)        # higher = worse\n    norm_arc = normalize(arc_lengths)       # higher = worse\n    norm_proc = normalize(processing_rates) # higher = better\n    norm_queue = normalize(queue_lengths)   # higher = worse\n\n    # Convert to desirability (higher better)\n    dist_score = 1.0 - norm_dist\n    arc_score = 1.0 - norm_arc\n    # Diminishing returns for processing rate\n    proc_score = np.sqrt(norm_proc)\n    # Emphasize high queue occupancy (super-linear)\n    queue_penalty = norm_queue ** 1.5\n\n    # Tuned weights (different from baseline)\n    w_queue = -0.45   # stronger negative for congestion/drop risk\n    w_arc   = +0.22   # moderate reward for being closer to destination\n    w_proc  = +0.14   # moderate reward for faster service with diminishing returns\n    w_dist  = -0.05   # mild penalty for longer propagation distance\n\n    offset_raw = (w_queue * queue_penalty +\n                  w_arc   * arc_score +\n                  w_proc  * proc_score +\n                  w_dist  * dist_score)\n\n    # Identify zero-padded/missing neighbours\n    all_zero_mask = (np.isclose(distances, 0.0) &\n                     np.isclose(arc_lengths, 0.0) &\n                     np.isclose(processing_rates, 0.0) &\n                     np.isclose(queue_lengths, 0.0))\n\n    # Clip to modest range so offsets refine but do not dominate learned Q-values\n    max_offset = 0.18\n    offsets = np.clip(offset_raw, -max_offset, max_offset)\n\n    # Apply a slightly stronger negative offset for padded/missing neighbours\n    offsets = offsets.copy()\n    offsets[all_zero_mask] = np.minimum(offsets[all_zero_mask], -0.20)\n\n    return offsets",
          "objective": 1.3657,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.5822594802551253,
               "avg_dropped_ratio": 0.20481927710843373
          }
     },
     {
          "algorithm": "Combine robustly-normalized, nonlinear feature transforms (sigmoid and inverse transforms) into a small weighted sum that strongly penalizes queue congestion, rewards proximity to destination and higher service rates, and modestly penalizes long distances, with clipping and extra discouragement for zero-padded neighbours.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Compute small offsets to add to agent Q-values based on features.\n    Inputs: numpy arrays of same shape.\n    Output: numpy array 'offsets' of same shape.\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    eps = 1e-8\n\n    def robust_std(x):\n        # Use scaled IQR to be robust to outliers; fallback to std if needed\n        q75, q25 = np.percentile(x, [75, 25])\n        iqr = q75 - q25\n        if iqr < eps:\n            s = np.std(x)\n            return s if s > eps else 1.0\n        # scale IQR to approximate std for normal-like data\n        return max(iqr / 1.349, eps)\n\n    def sigmoid(x):\n        return 1.0 / (1.0 + np.exp(-x))\n\n    # Robust centering & scaling\n    def robust_z(x):\n        med = np.median(x)\n        s = robust_std(x)\n        return (x - med) / s\n\n    # If all zeros for an input vector (padding), we'll detect later via all_zero_mask\n    # Compute robust z-scores\n    zd = robust_z(distances)\n    za = robust_z(arc_lengths)\n    zp = robust_z(processing_rates)\n    zq = robust_z(queue_lengths)\n\n    # Map to (0,1) via sigmoid (monotonic), then invert where smaller-is-better\n    dist_score = 1.0 - sigmoid(zd)        # closer distances -> larger score\n    arc_score  = 1.0 - sigmoid(za)        # closer to destination -> larger score\n    proc_score = sigmoid(zp)              # higher processing -> larger score\n    queue_score = sigmoid(zq)             # larger queue -> larger (bad) score\n\n    # Emphasize higher queue penalty with a convex transform (squares strongly penalize big queues)\n    queue_penalty = queue_score ** 2\n\n    # Tuned small weights to refine learned Q-values (kept modest relative to Q scale)\n    w_proc = 0.14      # reward faster processing\n    w_arc  = 0.17      # reward being closer to destination\n    w_queue = -0.40    # strong negative for congestion/drop risk\n    w_dist =  -0.06    # slight penalty for larger propagation distance\n\n    # Combine\n    offset_raw = (w_proc * proc_score +\n                  w_arc  * arc_score +\n                  w_queue * queue_penalty +\n                  w_dist * dist_score)\n\n    # Scale down if raw values are unexpectedly large (keeps offsets modest)\n    # Compute a robust scale based on median absolute deviation of offset_raw\n    mad = np.median(np.abs(offset_raw - np.median(offset_raw))) + eps\n    # aim for typical magnitude ~0.12-0.18; scale factor reduces extremes\n    scale = 0.18 / (mad * 1.0)\n    # but avoid huge scaling; clamp scale between 0.5 and 2.5\n    scale = float(np.clip(scale, 0.5, 2.5))\n    offsets = offset_raw * scale\n\n    # Clip to modest range so offsets refine but do not dominate learned Q-values\n    offsets = np.clip(offsets, -0.25, 0.25)\n\n    # If an entry is a zero-padding (all features zero), softly discourage it\n    all_zero_mask = (np.isclose(distances, 0.0) &\n                     np.isclose(arc_lengths, 0.0) &\n                     np.isclose(processing_rates, 0.0) &\n                     np.isclose(queue_lengths, 0.0))\n    if np.any(all_zero_mask):\n        # push these down but not to extreme; ensure at least -0.18\n        offsets[all_zero_mask] = np.minimum(offsets[all_zero_mask], -0.18)\n\n    return offsets",
          "objective": 1.3549,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.587996035680122,
               "avg_dropped_ratio": 0.2033132530120482
          }
     }
]