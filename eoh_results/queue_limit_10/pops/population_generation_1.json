[
     {
          "algorithm": "Combine multiplicative exponential feature transforms (distance propagation decay, arc-length closeness, normalized throughput) with queue-length damping to produce a probabilistic score, then map score to a small centered tanh-scaled offset so offsets are modest, zero-median, and strongly negative for fully-missing neighbours.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n      - offsets: numpy array of same shape\n    \"\"\"\n    eps = 1e-9\n    # ensure numpy arrays\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n\n    # preserve shape\n    shape = d.shape\n    d = d.reshape(-1)\n    a = a.reshape(-1)\n    p = p.reshape(-1)\n    q = q.reshape(-1)\n\n    # Detect fully-missing neighbors (all-zero feature vector) to strongly penalize them\n    missing_mask = (np.abs(d) <= eps) & (np.abs(a) <= eps) & (np.abs(p) <= eps) & (np.abs(q) <= eps)\n\n    # Robust scale parameters (use medians to avoid outlier effects)\n    D0 = np.median(d[~missing_mask]) if np.any(~missing_mask) else 1.0\n    A0 = np.median(a[~missing_mask]) if np.any(~missing_mask) else 1.0\n    P0 = np.median(p[~missing_mask]) if np.any(~missing_mask) else 1.0\n    Q0 = np.median(q[~missing_mask]) if np.any(~missing_mask) else 1.0\n\n    # Avoid degenerate scales\n    D0 = max(D0, 1.0)\n    A0 = max(A0, 1.0)\n    P0 = max(P0, 1.0)\n    Q0 = max(Q0, 1.0)\n\n    # Exponential closeness term: neighbors much closer to destination get exponentially higher score\n    closeness_term = np.exp(-a / (A0 * 0.8 + eps))  # smaller arc -> larger value\n\n    # Propagation delay penalty (distance): longer distance reduces score\n    propagation_term = np.exp(-d / (D0 * 3.5 + eps))\n\n    # Throughput advantage: normalized processing rate (favor higher \u03bc), soft-scaled\n    proc_norm = (p + eps) / (P0 + eps)\n    proc_term = np.log1p(proc_norm)  # gentle concave reward for higher \u03bc\n\n    # Congestion damping: queue length reduces score multiplicatively (stronger effect as queue grows)\n    # use soft damping so small queues are allowed but large queues heavily penalized\n    queue_damping = np.exp(- (q / (Q0 + eps)) * 1.2)\n\n    # Combine multiplicatively to form a positive score\n    base_score = closeness_term * propagation_term * proc_term * queue_damping\n\n    # if base_score is all zeros (possible if proc_term=0), add tiny floor to keep transforms stable\n    base_score = base_score + eps\n\n    # Log-transform to compress dynamic range, center by median, then apply tanh to produce bounded offsets\n    log_score = np.log(base_score + 1.0)  # log1p-like compression\n    center = np.median(log_score[~missing_mask]) if np.any(~missing_mask) else np.median(log_score)\n    centered = log_score - center\n\n    # Scaling: choose a gain that keeps offsets modest while allowing meaningful differentiation\n    gain = 0.9\n    raw_offsets = np.tanh(gain * centered)\n\n    # Scale final offsets to a modest amplitude (refinement only)\n    max_amplitude = 0.20\n    offsets = raw_offsets * max_amplitude\n\n    # Strong negative offset for missing neighbours so they are avoided\n    offsets[missing_mask] = -0.22\n\n    # ensure output shape matches input\n    offsets = offsets.reshape(shape)\n\n    return offsets",
          "objective": 1.562,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6131950175099086,
               "avg_dropped_ratio": 0.04216867469879518
          }
     },
     {
          "algorithm": "Multiply carefully-shaped nonlinear feature scores (exponential decay for propagation delay, sigmoid-based congestion penalty, exponential responsiveness to processing rate, and progress toward destination) into a composite desirability, standardize it, and squash to a small bounded offset so the learned Q-values remain dominant.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs: distances, arc_lengths, processing_rates, queue_lengths - numpy arrays of identical shape\n    Output: offsets - numpy array of same shape\n    \"\"\"\n    eps = 1e-8\n    # ensure numpy arrays\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n\n    # -------- feature transforms (nonlinear, multiplicative combination) --------\n    # propagation delay score: exponential decay of travel time (km -> seconds)\n    c_km_s = 299792.458  # speed of light in km/s\n    travel_time = d / (c_km_s + eps)        # seconds\n    # use a moderate decay constant so typical distances modestly affect score\n    travel_score = np.exp(-travel_time * 8.0)  # in (0,1]; smaller travel_time -> closer to 1\n\n    # progress toward destination: larger when arc_length is small\n    a_max = np.nanmax(a) if np.isfinite(np.nanmax(a)) else 0.0\n    if a_max <= eps:\n        progress = np.ones_like(a) * 0.5\n    else:\n        progress = (a_max - a) / (a_max + eps)   # in [0,1] (closer -> larger)\n\n    # processing rate responsiveness: diminishing returns via 1 - exp(-p/scale)\n    p_scale = np.clip(np.nanmean(p), 1.0, None)  # avoid too small scale\n    proc_score = 1.0 - np.exp(-p / (p_scale + eps))  # in [0,1)\n\n    # queue-based penalty: sigmoid around typical queue length\n    q_mean = np.nanmean(q)\n    q_std = np.nanstd(q)\n    if q_std < eps:\n        q_std = 1.0\n    # higher queues -> larger sigmoid value; convert to a penalty in (0,1) where high queue -> small score\n    q_sig = 1.0 / (1.0 + np.exp(-(q - q_mean) / (q_std + eps)))\n    queue_score = 1.0 - q_sig  # high queue => near 0, low queue => near 1\n\n    # small bonus if queue is zero (distinct boost for empty queues)\n    empty_bonus = (q == 0).astype(float) * 0.08\n\n    # combine multiplicatively to emphasize that a single bad factor can reduce desirability\n    # give progress a mild additive bias so being closer to destination matters but not overwhelmingly\n    combined = travel_score * (0.7 * progress + 0.3) * proc_score * queue_score + empty_bonus\n\n    # -------- standardize and squash to modest offset range --------\n    # center and scale so offsets refine learned Q-values (not overpower)\n    mean_comb = np.nanmean(combined)\n    std_comb = np.nanstd(combined)\n    std_comb = std_comb if std_comb > eps else 1.0\n\n    z = (combined - mean_comb) / (std_comb + eps)   # roughly zero-mean, unit-std\n    # squash with tanh to keep bounded and then scale to desired max magnitude (~0.18)\n    scale = 0.16\n    offsets = np.tanh(z) * scale\n\n    # final clip to be safe\n    offsets = np.clip(offsets, -0.18, 0.18)\n\n    return offsets",
          "objective": 1.5144,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.631483704397511,
               "avg_dropped_ratio": 0.04367469879518072
          }
     },
     {
          "algorithm": "Compute small, normalized feature-based offsets that reward short arc_length and high processing_rate, penalize long distances and high queue_lengths, and clip to a modest range so the learned Q-values remain dominant.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs: NumPy arrays of same shape for distances, arc_lengths, processing_rates, queue_lengths\n    Output: NumPy array 'offsets' of same shape\n    \"\"\"\n    eps = 1e-8\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    def min_max_norm(x):\n        x = np.asarray(x, dtype=float)\n        xmin = np.nanmin(x)\n        xmax = np.nanmax(x)\n        denom = xmax - xmin\n        if denom <= eps:\n            # if constant, return zeros to avoid creating artificial preference\n            return np.zeros_like(x)\n        return (x - xmin) / (denom + eps)\n\n    # Normalizations in [0,1]\n    nd = min_max_norm(distances)         # higher = farther (worse)\n    na = min_max_norm(arc_lengths)       # higher = farther from destination (worse)\n    np_rate = min_max_norm(processing_rates)  # higher = better\n    nq = min_max_norm(queue_lengths)     # higher = more congested (worse)\n\n    # Weights chosen to be small so offsets refine learned Q-values\n    w_proc = 0.10    # reward higher processing rate\n    w_arc  = 0.08    # reward shorter arc (we'll use 1 - na)\n    w_queue= -0.16   # penalize higher queue lengths\n    w_dist = -0.03   # penalize longer propagation distance\n\n    # Small bonus for empty queue (encourages low-congestion paths)\n    empty_queue_bonus = 0.03 * (queue_lengths == 0).astype(float)\n\n    offsets = (\n        w_proc * np_rate +\n        w_arc  * (1.0 - na) +\n        w_queue * nq +\n        w_dist * nd +\n        empty_queue_bonus\n    )\n\n    # Clip offsets to keep them modest compared to learned Q-values\n    offsets = np.clip(offsets, -0.20, 0.20)\n\n    return offsets",
          "objective": 1.4932,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6414727286653467,
               "avg_dropped_ratio": 0.042168674698795185
          }
     },
     {
          "algorithm": "",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n      - offsets: numpy array of same shape\n    \"\"\"\n    eps = 1e-9\n\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n\n    shape = d.shape\n    d = d.reshape(-1)\n    a = a.reshape(-1)\n    p = p.reshape(-1)\n    q = q.reshape(-1)\n\n    # detect missing (all-zero) entries\n    missing_mask = (np.abs(d) <= eps) & (np.abs(a) <= eps) & (np.abs(p) <= eps) & (np.abs(q) <= eps)\n    valid = ~missing_mask\n\n    # If nothing valid, return zeros\n    if not np.any(valid):\n        return np.zeros(shape, dtype=float)\n\n    # Robust scales (medians) with minimal epsilon to avoid div-by-zero\n    D0 = np.median(np.abs(d[valid])) + eps\n    A0 = np.median(np.abs(a[valid])) + eps\n    P0 = np.median(np.abs(p[valid])) + eps\n    Q0 = np.median(np.abs(q[valid])) + eps\n\n    # Normalize features relative to their robust scales\n    dn = d / D0\n    an = a / A0\n    pn = p / P0\n    qn = q / Q0\n\n    # Simple, monotonic component functions that generalize:\n    closeness = 1.0 / (1.0 + an)        # smaller arc -> larger value\n    propagation = 1.0 / (1.0 + dn)      # shorter distance -> larger value\n    proc = pn / (1.0 + np.abs(pn))      # bounded, favors larger processing rates\n    congestion = 1.0 / (1.0 + qn)       # larger queue -> smaller value\n\n    # Combine multiplicatively to keep signal in (0,1]\n    score = closeness * propagation * proc * congestion\n    score = np.maximum(score, eps)\n\n    # Log-compress and center by median of valid entries\n    log_score = np.log(score + eps)\n    center = np.median(log_score[valid])\n    centered = log_score - center\n\n    # Data-driven scaling: map typical spread to a modest amplitude\n    target_amp = 0.20\n    spread95 = np.percentile(np.abs(centered[valid]), 95)\n    scale = target_amp / (spread95 + eps)\n\n    # Bound offsets and scale to target amplitude\n    offsets = np.tanh(centered * scale) * target_amp\n\n    # Assign a negative offset for missing neighbours slightly below the minimum valid offset\n    min_valid = np.min(offsets[valid])\n    margin = max(target_amp * 0.05, 1e-3)\n    offsets[missing_mask] = min_valid - margin\n\n    offsets = offsets.reshape(shape)\n    return offsets",
          "objective": 1.493,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6496031572527932,
               "avg_dropped_ratio": 0.030120481927710843
          }
     }
]