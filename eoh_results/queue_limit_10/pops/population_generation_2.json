[
     {
          "algorithm": "Combine multiplicative exponential feature transforms (distance propagation decay, arc-length closeness, normalized throughput) with queue-length damping to produce a probabilistic score, then map score to a small centered tanh-scaled offset so offsets are modest, zero-median, and strongly negative for fully-missing neighbours.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n      - offsets: numpy array of same shape\n    \"\"\"\n    eps = 1e-9\n    # ensure numpy arrays\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n\n    # preserve shape\n    shape = d.shape\n    d = d.reshape(-1)\n    a = a.reshape(-1)\n    p = p.reshape(-1)\n    q = q.reshape(-1)\n\n    # Detect fully-missing neighbors (all-zero feature vector) to strongly penalize them\n    missing_mask = (np.abs(d) <= eps) & (np.abs(a) <= eps) & (np.abs(p) <= eps) & (np.abs(q) <= eps)\n\n    # Robust scale parameters (use medians to avoid outlier effects)\n    D0 = np.median(d[~missing_mask]) if np.any(~missing_mask) else 1.0\n    A0 = np.median(a[~missing_mask]) if np.any(~missing_mask) else 1.0\n    P0 = np.median(p[~missing_mask]) if np.any(~missing_mask) else 1.0\n    Q0 = np.median(q[~missing_mask]) if np.any(~missing_mask) else 1.0\n\n    # Avoid degenerate scales\n    D0 = max(D0, 1.0)\n    A0 = max(A0, 1.0)\n    P0 = max(P0, 1.0)\n    Q0 = max(Q0, 1.0)\n\n    # Exponential closeness term: neighbors much closer to destination get exponentially higher score\n    closeness_term = np.exp(-a / (A0 * 0.8 + eps))  # smaller arc -> larger value\n\n    # Propagation delay penalty (distance): longer distance reduces score\n    propagation_term = np.exp(-d / (D0 * 3.5 + eps))\n\n    # Throughput advantage: normalized processing rate (favor higher \u03bc), soft-scaled\n    proc_norm = (p + eps) / (P0 + eps)\n    proc_term = np.log1p(proc_norm)  # gentle concave reward for higher \u03bc\n\n    # Congestion damping: queue length reduces score multiplicatively (stronger effect as queue grows)\n    # use soft damping so small queues are allowed but large queues heavily penalized\n    queue_damping = np.exp(- (q / (Q0 + eps)) * 1.2)\n\n    # Combine multiplicatively to form a positive score\n    base_score = closeness_term * propagation_term * proc_term * queue_damping\n\n    # if base_score is all zeros (possible if proc_term=0), add tiny floor to keep transforms stable\n    base_score = base_score + eps\n\n    # Log-transform to compress dynamic range, center by median, then apply tanh to produce bounded offsets\n    log_score = np.log(base_score + 1.0)  # log1p-like compression\n    center = np.median(log_score[~missing_mask]) if np.any(~missing_mask) else np.median(log_score)\n    centered = log_score - center\n\n    # Scaling: choose a gain that keeps offsets modest while allowing meaningful differentiation\n    gain = 0.9\n    raw_offsets = np.tanh(gain * centered)\n\n    # Scale final offsets to a modest amplitude (refinement only)\n    max_amplitude = 0.20\n    offsets = raw_offsets * max_amplitude\n\n    # Strong negative offset for missing neighbours so they are avoided\n    offsets[missing_mask] = -0.22\n\n    # ensure output shape matches input\n    offsets = offsets.reshape(shape)\n\n    return offsets",
          "objective": 1.562,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6131950175099086,
               "avg_dropped_ratio": 0.04216867469879518
          }
     },
     {
          "algorithm": "A multiplicative scoring offset that uses a tighter arc-length closeness, a stronger propagation distance penalty, a sigmoid-shaped processing-rate reward, amplified queue-length damping with a small bonus for empty queues, and tanh-compressed, modestly scaled offsets with strong penalties for missing neighbours.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n      - offsets: numpy array of same shape\n    \"\"\"\n    eps = 1e-9\n    d = np.asarray(distances, dtype=float).reshape(-1)\n    a = np.asarray(arc_lengths, dtype=float).reshape(-1)\n    p = np.asarray(processing_rates, dtype=float).reshape(-1)\n    q = np.asarray(queue_lengths, dtype=float).reshape(-1)\n\n    # Detect fully-missing neighbors (all-zero feature vector)\n    missing_mask = (np.abs(d) <= eps) & (np.abs(a) <= eps) & (np.abs(p) <= eps) & (np.abs(q) <= eps)\n\n    # Robust scale parameters (medians)\n    valid = ~missing_mask\n    if np.any(valid):\n        D0 = np.median(d[valid])\n        A0 = np.median(a[valid])\n        P0 = np.median(p[valid])\n        Q0 = np.median(q[valid])\n    else:\n        D0 = A0 = P0 = Q0 = 1.0\n\n    # Avoid degenerate scales\n    D0 = max(D0, 1.0)\n    A0 = max(A0, 1.0)\n    P0 = max(P0, 1.0)\n    Q0 = max(Q0, 1.0)\n\n    # Exponential closeness term: favor much smaller arc lengths (more aggressive than baseline)\n    closeness_term = np.exp(-a / (A0 * 0.6 + eps))\n\n    # Propagation delay penalty (distance): stronger penalty than baseline\n    propagation_term = np.exp(-d / (D0 * 2.5 + eps))\n\n    # Processing-rate reward: sigmoid-shaped normalized advantage, mapped to [0.8, 2.0]\n    proc_scale = 0.35\n    proc_norm = (p / (P0 + eps)) - 1.0\n    sigmoid = 1.0 / (1.0 + np.exp(-proc_norm / proc_scale))\n    proc_term = 0.8 + 1.2 * sigmoid  # range ~[0.8, 2.0]\n\n    # Queue-length damping: stronger exponential damping for congestion\n    queue_damping = np.exp(- (q / (Q0 + eps)) * 1.8)\n\n    # Small bonus for very low queues (prefer empty/near-empty queues)\n    low_queue_bonus = np.where(q <= 1.0, 1.08, 1.0)\n\n    # Combine multiplicatively\n    base_score = closeness_term * propagation_term * proc_term * queue_damping * low_queue_bonus\n\n    # Stability floor\n    base_score = base_score + eps\n\n    # Compress dynamic range and center\n    log_score = np.log1p(base_score)\n    if np.any(valid):\n        center = np.median(log_score[valid])\n    else:\n        center = np.median(log_score)\n    centered = log_score - center\n\n    # Gain and bounded tanh scaling to produce modest offsets\n    gain = 1.2\n    raw_offsets = np.tanh(gain * centered)\n\n    max_amplitude = 0.18\n    offsets = raw_offsets * max_amplitude\n\n    # Strong negative offset for missing neighbours\n    offsets[missing_mask] = -0.26\n\n    # Reshape to original input shape\n    offsets = offsets.reshape(np.asarray(distances).shape)\n    return offsets",
          "objective": 1.5436,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.621478449545646,
               "avg_dropped_ratio": 0.04066265060240964
          }
     },
     {
          "algorithm": "An adaptive additive utility uses per-feature normalized scores with attention-like softmax weights (based on relative feature variation), converts utilities into Plackett\u2013Luce style probabilities, maps log-probabilities to zero-centered bounded offsets, and strongly penalizes fully-missing neighbours.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n      - offsets: numpy array of same shape\n    \"\"\"\n    eps = 1e-9\n    c_km_s = 299792.458  # speed of light in km/s\n\n    # ensure numpy arrays and flatten for computation\n    d = np.asarray(distances, dtype=float).reshape(-1)\n    a = np.asarray(arc_lengths, dtype=float).reshape(-1)\n    p = np.asarray(processing_rates, dtype=float).reshape(-1)\n    q = np.asarray(queue_lengths, dtype=float).reshape(-1)\n    n = d.size\n\n    # detect fully-missing neighbours (zero-padded)\n    missing_mask = (np.abs(d) <= eps) & (np.abs(a) <= eps) & (np.abs(p) <= eps) & (np.abs(q) <= eps)\n\n    # robust scales (medians) computed over non-missing entries\n    valid = ~missing_mask\n    if np.any(valid):\n        D0 = max(np.median(d[valid]), 1.0)\n        A0 = max(np.median(a[valid]), 1.0)\n        P0 = max(np.median(p[valid]), 1.0)\n        Q0 = max(np.median(q[valid]), 1.0)\n    else:\n        # all missing: return strong negatives\n        return np.full_like(d, -0.22).reshape(distances.shape)\n\n    # ---- per-feature monotone desirability transforms (range roughly in (0,1]) ----\n    # propagation/travel score (shorter distance -> higher)\n    travel_time = d / (c_km_s + eps)                        # seconds\n    travel_score = np.exp(-np.maximum(travel_time, 0.0) * 7.0)  # sharper decay factor\n\n    # progress toward destination: smaller arc length -> higher\n    arc_score = 1.0 / (1.0 + (a / (A0 + eps)))  # in (0,1]\n\n    # processing rate responsiveness: saturation but favors higher rates\n    proc_score = (p + eps) / (p + P0 + eps)  # in (0,1), softer than linear\n\n    # queue penalty: steep penalty for large queues, gentle for small\n    queue_score = 1.0 / (1.0 + (q / (Q0 + eps))**1.7)  # in (0,1], steeper exponent\n\n    # Clamp scores to safe ranges\n    travel_score = np.clip(travel_score, 1e-6, 1.0)\n    arc_score = np.clip(arc_score, 1e-6, 1.0)\n    proc_score = np.clip(proc_score, 1e-6, 1.0)\n    queue_score = np.clip(queue_score, 1e-6, 1.0)\n\n    # ---- compute per-feature relative variability to form attention-like weights ----\n    # Use coefficient of variation (std/mean) across non-missing neighbours to measure informativeness\n    def relvar(x):\n        xm = x[valid]\n        mean = np.mean(xm) if xm.size > 0 else 0.0\n        std = np.std(xm) if xm.size > 0 else 0.0\n        return (std / (abs(mean) + eps))\n\n    rv_travel = relvar(travel_score)\n    rv_arc = relvar(arc_score)\n    rv_proc = relvar(proc_score)\n    rv_queue = relvar(queue_score)\n\n    rels = np.array([rv_travel, rv_arc, rv_proc, rv_queue], dtype=float)\n    # if all rels are tiny (no variation), default to equal importance\n    if np.all(rels <= eps):\n        feat_weights = np.ones(4, dtype=float) / 4.0\n    else:\n        # softmax to produce positive weights summing to 1; temperature to control concentration\n        temp = 1.0\n        ex = np.exp((rels / (np.max(rels) + eps)) / (temp + eps))\n        feat_weights = ex / (np.sum(ex) + eps)\n\n    # ---- combine additively using the feature weights (different from multiplicative designs) ----\n    utilities = (feat_weights[0] * travel_score +\n                 feat_weights[1] * arc_score +\n                 feat_weights[2] * proc_score +\n                 feat_weights[3] * queue_score)\n\n    # give a small deterministic bonus to completely empty queues (helps avoid congestion)\n    empty_bonus = (q == 0).astype(float) * 0.03\n    utilities = utilities + empty_bonus\n\n    # Mask utilities for missing neighbours so they do not skew normalization\n    masked_util = utilities.copy()\n    masked_util[missing_mask] = -1e6  # very small logit for missing\n\n    # ---- convert utilities to Plackett-Luce style probabilities (softmax over neighbors) ----\n    beta = 6.0  # sharpness parameter: higher -> more discriminative\n    logits = masked_util * beta\n\n    # numerical stable log-sum-exp to compute log probabilities\n    # handle case where all logits are very small/large\n    max_logit = np.max(logits)\n    stable = logits - max_logit\n    exp_stable = np.exp(stable)\n    sum_exp = np.sum(exp_stable)\n    # if sum_exp is zero (all masked), set uniform tiny probabilities\n    if sum_exp <= eps:\n        # all missing neighbours or degenerate -> give strong negatives and return\n        offsets = np.full_like(d, -0.22)\n        return offsets.reshape(distances.shape)\n\n    log_probs = stable - np.log(sum_exp + eps)  # log P_i\n\n    # center by the median of log-probs among non-missing neighbours so median offset is zero\n    valid_logp = log_probs[~missing_mask]\n    if valid_logp.size > 0:\n        center = np.median(valid_logp)\n    else:\n        center = np.median(log_probs)\n\n    centered = log_probs - center\n\n    # ---- map centered log-probabilities to small bounded offsets ----\n    # Use a smooth arctangent mapping (different from tanh used previously) to bound outputs\n    gamma = 1.8  # gain\n    max_amp = 0.18\n    offsets = np.arctan(centered * gamma) / (np.pi / 2.0)  # maps to (-1,1)\n    offsets = offsets * max_amp\n\n    # Strong negative offset for missing neighbours\n    offsets[missing_mask] = -0.22\n\n    # final safety clipping\n    offsets = np.clip(offsets, -0.22, 0.18)\n\n    # restore original shape\n    return offsets",
          "objective": 1.5359,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6305057043578938,
               "avg_dropped_ratio": 0.03162650602409639
          }
     },
     {
          "algorithm": "Multiply carefully-shaped nonlinear feature scores (exponential decay for propagation delay, sigmoid-based congestion penalty, exponential responsiveness to processing rate, and progress toward destination) into a composite desirability, standardize it, and squash to a small bounded offset so the learned Q-values remain dominant.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs: distances, arc_lengths, processing_rates, queue_lengths - numpy arrays of identical shape\n    Output: offsets - numpy array of same shape\n    \"\"\"\n    eps = 1e-8\n    # ensure numpy arrays\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n\n    # -------- feature transforms (nonlinear, multiplicative combination) --------\n    # propagation delay score: exponential decay of travel time (km -> seconds)\n    c_km_s = 299792.458  # speed of light in km/s\n    travel_time = d / (c_km_s + eps)        # seconds\n    # use a moderate decay constant so typical distances modestly affect score\n    travel_score = np.exp(-travel_time * 8.0)  # in (0,1]; smaller travel_time -> closer to 1\n\n    # progress toward destination: larger when arc_length is small\n    a_max = np.nanmax(a) if np.isfinite(np.nanmax(a)) else 0.0\n    if a_max <= eps:\n        progress = np.ones_like(a) * 0.5\n    else:\n        progress = (a_max - a) / (a_max + eps)   # in [0,1] (closer -> larger)\n\n    # processing rate responsiveness: diminishing returns via 1 - exp(-p/scale)\n    p_scale = np.clip(np.nanmean(p), 1.0, None)  # avoid too small scale\n    proc_score = 1.0 - np.exp(-p / (p_scale + eps))  # in [0,1)\n\n    # queue-based penalty: sigmoid around typical queue length\n    q_mean = np.nanmean(q)\n    q_std = np.nanstd(q)\n    if q_std < eps:\n        q_std = 1.0\n    # higher queues -> larger sigmoid value; convert to a penalty in (0,1) where high queue -> small score\n    q_sig = 1.0 / (1.0 + np.exp(-(q - q_mean) / (q_std + eps)))\n    queue_score = 1.0 - q_sig  # high queue => near 0, low queue => near 1\n\n    # small bonus if queue is zero (distinct boost for empty queues)\n    empty_bonus = (q == 0).astype(float) * 0.08\n\n    # combine multiplicatively to emphasize that a single bad factor can reduce desirability\n    # give progress a mild additive bias so being closer to destination matters but not overwhelmingly\n    combined = travel_score * (0.7 * progress + 0.3) * proc_score * queue_score + empty_bonus\n\n    # -------- standardize and squash to modest offset range --------\n    # center and scale so offsets refine learned Q-values (not overpower)\n    mean_comb = np.nanmean(combined)\n    std_comb = np.nanstd(combined)\n    std_comb = std_comb if std_comb > eps else 1.0\n\n    z = (combined - mean_comb) / (std_comb + eps)   # roughly zero-mean, unit-std\n    # squash with tanh to keep bounded and then scale to desired max magnitude (~0.18)\n    scale = 0.16\n    offsets = np.tanh(z) * scale\n\n    # final clip to be safe\n    offsets = np.clip(offsets, -0.18, 0.18)\n\n    return offsets",
          "objective": 1.5144,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.631483704397511,
               "avg_dropped_ratio": 0.04367469879518072
          }
     }
]