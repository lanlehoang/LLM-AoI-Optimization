[
     {
          "algorithm": "A sharper scoring variant emphasizing arc-length closeness and queue penalties: stronger arc and distance exponentials, a square-root processing boost, steeper queue damping, multiplicative combination, and tanh-centered small offsets with a stronger penalty for missing neighbours.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n      - offsets: numpy array of same shape\n    \"\"\"\n    eps = 1e-9\n    d = np.asarray(distances, dtype=float).reshape(-1)\n    a = np.asarray(arc_lengths, dtype=float).reshape(-1)\n    p = np.asarray(processing_rates, dtype=float).reshape(-1)\n    q = np.asarray(queue_lengths, dtype=float).reshape(-1)\n\n    # Detect fully-missing neighbors (all-zero feature vector)\n    missing_mask = (np.abs(d) <= eps) & (np.abs(a) <= eps) & (np.abs(p) <= eps) & (np.abs(q) <= eps)\n\n    # Robust scales (medians excluding missing)\n    valid_idx = ~missing_mask\n    if np.any(valid_idx):\n        D0 = np.median(d[valid_idx])\n        A0 = np.median(a[valid_idx])\n        P0 = np.median(p[valid_idx])\n        Q0 = np.median(q[valid_idx])\n    else:\n        D0 = A0 = P0 = Q0 = 1.0\n\n    # Prevent degenerate small scales\n    D0 = max(D0, 1.0)\n    A0 = max(A0, 1.0)\n    P0 = max(P0, 1.0)\n    Q0 = max(Q0, 1.0)\n\n    # Parameter settings (different from original)\n    closeness_scale = 0.6    # emphasize arc closeness more strongly\n    propagation_scale = 2.0  # stronger distance penalty\n    queue_coeff = 1.9        # queue damping multiplier\n    proc_power = 0.5         # sqrt-like boost (use power 0.5)\n    closeness_exponent = 1.2 # slightly sharpen arc effect\n    gain = 1.10              # slightly stronger gain before tanh\n    max_amplitude = 0.18     # modest final amplitude\n    missing_penalty = -0.26  # stronger negative offset for missing neighbors\n\n    # Feature transforms\n    closeness_term = np.exp(-a / (A0 * closeness_scale + eps)) ** closeness_exponent\n    propagation_term = np.exp(-d / (D0 * propagation_scale + eps))\n    proc_norm = (p + eps) / (P0 + eps)\n    proc_term = np.power(proc_norm + eps, proc_power)  # sqrt-like boost\n    queue_damping = 1.0 / (1.0 + (q / (Q0 + eps)) * queue_coeff)\n\n    # Combine multiplicatively\n    base_score = closeness_term * propagation_term * proc_term * queue_damping\n    base_score = base_score + eps\n\n    # Compress and center\n    log_score = np.log1p(base_score)\n    if np.any(valid_idx):\n        center = np.median(log_score[valid_idx])\n    else:\n        center = np.median(log_score)\n    centered = log_score - center\n\n    # Map to bounded offsets\n    raw_offsets = np.tanh(gain * centered)\n    offsets = raw_offsets * max_amplitude\n\n    # Strong negative offset for missing neighbours\n    offsets[missing_mask] = missing_penalty\n\n    # Restore original shape\n    offsets = offsets.reshape(np.asarray(distances).shape)\n    return offsets",
          "objective": 1.5981,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6087844272183531,
               "avg_dropped_ratio": 0.02710843373493976
          }
     },
     {
          "algorithm": "Combine multiplicative exponential feature transforms (distance propagation decay, arc-length closeness, normalized throughput) with queue-length damping to produce a probabilistic score, then map score to a small centered tanh-scaled offset so offsets are modest, zero-median, and strongly negative for fully-missing neighbours.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n      - offsets: numpy array of same shape\n    \"\"\"\n    eps = 1e-9\n    # ensure numpy arrays\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n\n    # preserve shape\n    shape = d.shape\n    d = d.reshape(-1)\n    a = a.reshape(-1)\n    p = p.reshape(-1)\n    q = q.reshape(-1)\n\n    # Detect fully-missing neighbors (all-zero feature vector) to strongly penalize them\n    missing_mask = (np.abs(d) <= eps) & (np.abs(a) <= eps) & (np.abs(p) <= eps) & (np.abs(q) <= eps)\n\n    # Robust scale parameters (use medians to avoid outlier effects)\n    D0 = np.median(d[~missing_mask]) if np.any(~missing_mask) else 1.0\n    A0 = np.median(a[~missing_mask]) if np.any(~missing_mask) else 1.0\n    P0 = np.median(p[~missing_mask]) if np.any(~missing_mask) else 1.0\n    Q0 = np.median(q[~missing_mask]) if np.any(~missing_mask) else 1.0\n\n    # Avoid degenerate scales\n    D0 = max(D0, 1.0)\n    A0 = max(A0, 1.0)\n    P0 = max(P0, 1.0)\n    Q0 = max(Q0, 1.0)\n\n    # Exponential closeness term: neighbors much closer to destination get exponentially higher score\n    closeness_term = np.exp(-a / (A0 * 0.8 + eps))  # smaller arc -> larger value\n\n    # Propagation delay penalty (distance): longer distance reduces score\n    propagation_term = np.exp(-d / (D0 * 3.5 + eps))\n\n    # Throughput advantage: normalized processing rate (favor higher \u03bc), soft-scaled\n    proc_norm = (p + eps) / (P0 + eps)\n    proc_term = np.log1p(proc_norm)  # gentle concave reward for higher \u03bc\n\n    # Congestion damping: queue length reduces score multiplicatively (stronger effect as queue grows)\n    # use soft damping so small queues are allowed but large queues heavily penalized\n    queue_damping = np.exp(- (q / (Q0 + eps)) * 1.2)\n\n    # Combine multiplicatively to form a positive score\n    base_score = closeness_term * propagation_term * proc_term * queue_damping\n\n    # if base_score is all zeros (possible if proc_term=0), add tiny floor to keep transforms stable\n    base_score = base_score + eps\n\n    # Log-transform to compress dynamic range, center by median, then apply tanh to produce bounded offsets\n    log_score = np.log(base_score + 1.0)  # log1p-like compression\n    center = np.median(log_score[~missing_mask]) if np.any(~missing_mask) else np.median(log_score)\n    centered = log_score - center\n\n    # Scaling: choose a gain that keeps offsets modest while allowing meaningful differentiation\n    gain = 0.9\n    raw_offsets = np.tanh(gain * centered)\n\n    # Scale final offsets to a modest amplitude (refinement only)\n    max_amplitude = 0.20\n    offsets = raw_offsets * max_amplitude\n\n    # Strong negative offset for missing neighbours so they are avoided\n    offsets[missing_mask] = -0.22\n\n    # ensure output shape matches input\n    offsets = offsets.reshape(shape)\n\n    return offsets",
          "objective": 1.562,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6131950175099086,
               "avg_dropped_ratio": 0.04216867469879518
          }
     },
     {
          "algorithm": "A multiplicative scoring offset that uses a tighter arc-length closeness, a stronger propagation distance penalty, a sigmoid-shaped processing-rate reward, amplified queue-length damping with a small bonus for empty queues, and tanh-compressed, modestly scaled offsets with strong penalties for missing neighbours.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n      - offsets: numpy array of same shape\n    \"\"\"\n    eps = 1e-9\n    d = np.asarray(distances, dtype=float).reshape(-1)\n    a = np.asarray(arc_lengths, dtype=float).reshape(-1)\n    p = np.asarray(processing_rates, dtype=float).reshape(-1)\n    q = np.asarray(queue_lengths, dtype=float).reshape(-1)\n\n    # Detect fully-missing neighbors (all-zero feature vector)\n    missing_mask = (np.abs(d) <= eps) & (np.abs(a) <= eps) & (np.abs(p) <= eps) & (np.abs(q) <= eps)\n\n    # Robust scale parameters (medians)\n    valid = ~missing_mask\n    if np.any(valid):\n        D0 = np.median(d[valid])\n        A0 = np.median(a[valid])\n        P0 = np.median(p[valid])\n        Q0 = np.median(q[valid])\n    else:\n        D0 = A0 = P0 = Q0 = 1.0\n\n    # Avoid degenerate scales\n    D0 = max(D0, 1.0)\n    A0 = max(A0, 1.0)\n    P0 = max(P0, 1.0)\n    Q0 = max(Q0, 1.0)\n\n    # Exponential closeness term: favor much smaller arc lengths (more aggressive than baseline)\n    closeness_term = np.exp(-a / (A0 * 0.6 + eps))\n\n    # Propagation delay penalty (distance): stronger penalty than baseline\n    propagation_term = np.exp(-d / (D0 * 2.5 + eps))\n\n    # Processing-rate reward: sigmoid-shaped normalized advantage, mapped to [0.8, 2.0]\n    proc_scale = 0.35\n    proc_norm = (p / (P0 + eps)) - 1.0\n    sigmoid = 1.0 / (1.0 + np.exp(-proc_norm / proc_scale))\n    proc_term = 0.8 + 1.2 * sigmoid  # range ~[0.8, 2.0]\n\n    # Queue-length damping: stronger exponential damping for congestion\n    queue_damping = np.exp(- (q / (Q0 + eps)) * 1.8)\n\n    # Small bonus for very low queues (prefer empty/near-empty queues)\n    low_queue_bonus = np.where(q <= 1.0, 1.08, 1.0)\n\n    # Combine multiplicatively\n    base_score = closeness_term * propagation_term * proc_term * queue_damping * low_queue_bonus\n\n    # Stability floor\n    base_score = base_score + eps\n\n    # Compress dynamic range and center\n    log_score = np.log1p(base_score)\n    if np.any(valid):\n        center = np.median(log_score[valid])\n    else:\n        center = np.median(log_score)\n    centered = log_score - center\n\n    # Gain and bounded tanh scaling to produce modest offsets\n    gain = 1.2\n    raw_offsets = np.tanh(gain * centered)\n\n    max_amplitude = 0.18\n    offsets = raw_offsets * max_amplitude\n\n    # Strong negative offset for missing neighbours\n    offsets[missing_mask] = -0.26\n\n    # Reshape to original input shape\n    offsets = offsets.reshape(np.asarray(distances).shape)\n    return offsets",
          "objective": 1.5436,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.621478449545646,
               "avg_dropped_ratio": 0.04066265060240964
          }
     },
     {
          "algorithm": "Combine stronger closeness and distance exponentials with a tempered processing-rate boost and steeper queue-power damping, center by median log-score, and output small tanh-scaled offsets with strong negative for missing neighbours.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n      - offsets: numpy array of same shape\n    \"\"\"\n    eps = 1e-9\n    d = np.asarray(distances, dtype=float).reshape(-1)\n    a = np.asarray(arc_lengths, dtype=float).reshape(-1)\n    p = np.asarray(processing_rates, dtype=float).reshape(-1)\n    q = np.asarray(queue_lengths, dtype=float).reshape(-1)\n\n    # detect fully-missing (zero-padded) neighbours\n    missing_mask = (np.abs(d) <= eps) & (np.abs(a) <= eps) & (np.abs(p) <= eps) & (np.abs(q) <= eps)\n\n    # Robust scale estimates (medians) excluding missing entries\n    valid = ~missing_mask\n    if np.any(valid):\n        D0 = float(np.median(d[valid]))\n        A0 = float(np.median(a[valid]))\n        P0 = float(np.median(p[valid]))\n        Q0 = float(np.median(q[valid]))\n    else:\n        D0 = A0 = P0 = Q0 = 1.0\n\n    # Avoid degenerate scales\n    D0 = max(D0, 1.0)\n    A0 = max(A0, 1.0)\n    P0 = max(P0, 1.0)\n    Q0 = max(Q0, 1.0)\n\n    # Exponential closeness (stronger emphasis on reducing arc length)\n    closeness_term = np.exp(-a / (A0 * 0.6 + eps))\n\n    # Propagation delay penalty (distance) with increased sensitivity\n    propagation_term = np.exp(-d / (D0 * 2.0 + eps))\n\n    # Processing-rate advantage: tempered, smooth boost for higher \u03bc\n    proc_ratio = (p + eps) / (P0 + eps)\n    proc_term = 1.0 + 0.8 * np.tanh((proc_ratio - 1.0) * 0.9)  # in approx [0.2,1.8]\n\n    # Queue damping: steeper power penalty so large queues are strongly discouraged\n    queue_damping = 1.0 / (1.0 + np.power((q / (Q0 + eps)), 1.8))\n\n    # Combine multiplicatively to form a positive score (keeps interactions interpretable)\n    base_score = closeness_term**1.2 * propagation_term**0.9 * proc_term * queue_damping\n    base_score = base_score + eps\n\n    # Compress dynamic range and center by median to produce refinement offsets\n    log_score = np.log1p(base_score)\n    if np.any(valid):\n        center = float(np.median(log_score[valid]))\n    else:\n        center = float(np.median(log_score))\n    centered = log_score - center\n\n    # Map to bounded offsets with moderate gain and amplitude\n    gain = 1.1\n    raw_offsets = np.tanh(gain * centered)\n    max_amplitude = 0.16\n    offsets = raw_offsets * max_amplitude\n\n    # Strong negative offset for missing neighbours (ensure they are avoided)\n    offsets[missing_mask] = -0.26\n\n    # restore original shape\n    offsets = offsets.reshape(np.asarray(distances).shape)\n    return offsets",
          "objective": 1.5374,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6337823224617015,
               "avg_dropped_ratio": 0.025602409638554216
          }
     }
]