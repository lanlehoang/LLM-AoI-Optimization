[
     {
          "algorithm": "Compute small, normalized feature-based offsets that reward short arc_length and high processing_rate, penalize long distances and high queue_lengths, and clip to a modest range so the learned Q-values remain dominant.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs: NumPy arrays of same shape for distances, arc_lengths, processing_rates, queue_lengths\n    Output: NumPy array 'offsets' of same shape\n    \"\"\"\n    eps = 1e-8\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    def min_max_norm(x):\n        x = np.asarray(x, dtype=float)\n        xmin = np.nanmin(x)\n        xmax = np.nanmax(x)\n        denom = xmax - xmin\n        if denom <= eps:\n            # if constant, return zeros to avoid creating artificial preference\n            return np.zeros_like(x)\n        return (x - xmin) / (denom + eps)\n\n    # Normalizations in [0,1]\n    nd = min_max_norm(distances)         # higher = farther (worse)\n    na = min_max_norm(arc_lengths)       # higher = farther from destination (worse)\n    np_rate = min_max_norm(processing_rates)  # higher = better\n    nq = min_max_norm(queue_lengths)     # higher = more congested (worse)\n\n    # Weights chosen to be small so offsets refine learned Q-values\n    w_proc = 0.10    # reward higher processing rate\n    w_arc  = 0.08    # reward shorter arc (we'll use 1 - na)\n    w_queue= -0.16   # penalize higher queue lengths\n    w_dist = -0.03   # penalize longer propagation distance\n\n    # Small bonus for empty queue (encourages low-congestion paths)\n    empty_queue_bonus = 0.03 * (queue_lengths == 0).astype(float)\n\n    offsets = (\n        w_proc * np_rate +\n        w_arc  * (1.0 - na) +\n        w_queue * nq +\n        w_dist * nd +\n        empty_queue_bonus\n    )\n\n    # Clip offsets to keep them modest compared to learned Q-values\n    offsets = np.clip(offsets, -0.20, 0.20)\n\n    return offsets",
          "objective": 1.4932,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6414727286653467,
               "avg_dropped_ratio": 0.042168674698795185
          }
     },
     {
          "algorithm": "Compute small normalized offsets by rewarding neighbours closer to the destination and with higher processing rates while penalizing long distances and high queue lengths, with weights chosen to keep offsets within \u00b10.15 so they refine but don't overpower learned Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      - distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    Output:\n      - offsets: numpy array of same shape\n    \"\"\"\n    eps = 1e-6\n    # ensure arrays are numpy arrays\n    d = np.array(distances, dtype=float)\n    a = np.array(arc_lengths, dtype=float)\n    p = np.array(processing_rates, dtype=float)\n    q = np.array(queue_lengths, dtype=float)\n\n    def min_max_norm(x):\n        xmin = np.nanmin(x)\n        xmax = np.nanmax(x)\n        denom = (xmax - xmin) if (xmax - xmin) > 0 else eps\n        return (x - xmin) / denom\n\n    # normalize features to [0,1]\n    dist_n = min_max_norm(d)        # larger -> worse (higher delay)\n    arc_n = min_max_norm(a)         # larger arc -> further from destination\n    proc_n = min_max_norm(p)        # larger -> better\n    queue_n = min_max_norm(q)       # larger -> worse (congestion)\n\n    # closeness to destination: smaller arc_length is better\n    closeness = 1.0 - arc_n\n\n    # weights chosen to keep offsets in roughly +/-0.15 range\n    w_closeness = 0.09\n    w_proc = 0.06\n    w_queue = 0.12\n    w_dist = 0.03\n\n    offsets = (w_closeness * closeness\n               + w_proc * proc_n\n               - w_queue * queue_n\n               - w_dist * dist_n)\n\n    # clip just in case to keep within intended refinement bounds\n    offsets = np.clip(offsets, -0.15, 0.15)\n\n    return offsets",
          "objective": 1.4919,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.642014129968399,
               "avg_dropped_ratio": 0.04216867469879518
          }
     },
     {
          "algorithm": "I compute small, centered offsets by min-max normalizing features, combining positive incentives for low arc_length, high processing_rate and short distance and negative penalties for high queue and queue-per-service util, then mean-centering and clipping to keep offsets small.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n      distances, arc_lengths, processing_rates, queue_lengths : numpy arrays (same shape)\n    Output:\n      offsets : numpy array (same shape)\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    eps = 1e-8\n\n    def minmax_norm(x):\n        mn = np.min(x)\n        mx = np.max(x)\n        denom = mx - mn\n        if denom < eps:\n            return np.zeros_like(x, dtype=float)\n        return (x - mn) / denom\n\n    # Normalize features to [0,1] (higher=better for norm_proc; lower is better for distances/arc -> invert)\n    norm_dist = 1.0 - minmax_norm(distances)         # shorter distance -> higher score\n    norm_arc = 1.0 - minmax_norm(arc_lengths)       # closer to destination -> higher score\n    norm_proc = minmax_norm(processing_rates)       # higher processing rate -> higher score\n    norm_queue = minmax_norm(queue_lengths)         # higher queue -> worse (used as penalty)\n\n    util = queue_lengths / (processing_rates + eps) # queue per service unit -> higher is worse\n    norm_util = minmax_norm(util)\n\n    # Weights chosen to keep offsets small (relative to observed Q-values) and to refine agent preferences\n    w_arc = 0.035\n    w_proc = 0.035\n    w_dist = 0.010\n    w_q   = 0.060\n    w_u   = 0.030\n\n    positive = w_arc * norm_arc + w_proc * norm_proc + w_dist * norm_dist\n    penalty = w_q * norm_queue + w_u * norm_util\n\n    combined = positive - penalty\n\n    # Center offsets around zero so they refine (not overwhelmingly shift) agent Q-values\n    combined = combined - np.mean(combined)\n\n    # Clip to keep offsets small and bounded\n    max_offset = 0.06\n    offsets = np.clip(combined, -max_offset, max_offset)\n\n    return offsets",
          "objective": 1.4806,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.636749540287018,
               "avg_dropped_ratio": 0.057228915662650606
          }
     },
     {
          "algorithm": "I compute small normalized offsets that reward proximity (low arc length) and high processing rate while penalizing high queue occupancy and long propagation distance, scaled so offsets refine rather than dominate learned Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs:\n        distances, arc_lengths, processing_rates, queue_lengths : numpy arrays of same shape\n    Output:\n        offsets : numpy array of same shape\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    # shape and validity mask (missing neighbours are zero-padded)\n    shape = distances.shape\n    valid = (distances > 0) | (arc_lengths > 0) | (processing_rates > 0) | (queue_lengths > 0)\n    eps = 1e-8\n\n    def minmax_norm(x):\n        # normalize using only valid entries; if no variance, return zeros\n        if np.any(valid):\n            xv = x.copy()\n            xmin = np.min(xv[valid])\n            xmax = np.max(xv[valid])\n        else:\n            xmin = np.min(x)\n            xmax = np.max(x)\n        denom = xmax - xmin\n        if denom < eps:\n            return np.zeros_like(x, dtype=float)\n        out = (x - xmin) / denom\n        out = np.clip(out, 0.0, 1.0)\n        return out\n\n    # normalized features in [0,1] using min-max on valid entries\n    proc_norm = minmax_norm(processing_rates)           # higher is better\n    arc_norm = minmax_norm(arc_lengths)                 # lower arc -> closer; we will invert\n    dist_norm = minmax_norm(distances)                  # higher distance is worse\n    queue_norm = minmax_norm(queue_lengths)             # higher queue is worse\n\n    # feature transforms (accentuate important effects mildly)\n    proc_term = np.sqrt(proc_norm)                      # boost higher service rates a bit\n    arc_term = 1.0 - arc_norm                           # closeness to destination\n    queue_term = queue_norm ** 1.5                      # penalize high queues nonlinearly\n    dist_term = dist_norm                               # linear penalty for long hops\n\n    # weights chosen so combined offset is a mild refinement (~\u00b10.15)\n    w_proc = 0.06\n    w_arc = 0.08\n    w_queue = 0.14\n    w_dist = 0.03\n\n    offsets = (w_proc * proc_term) + (w_arc * arc_term) - (w_queue * queue_term) - (w_dist * dist_term)\n\n    # keep offsets for invalid (zero-padded) neighbours neutral (agent uses -inf Q for those anyway)\n    offsets = offsets.astype(float)\n    offsets[~valid] = 0.0\n\n    return offsets",
          "objective": 1.4678,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6505188839099939,
               "avg_dropped_ratio": 0.04518072289156627
          }
     }
]