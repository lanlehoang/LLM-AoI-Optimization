[
     {
          "algorithm": "I compute a modest, interpretable offset by normalizing each feature and combining them with tuned weights that penalize high queue lengths and long distances while rewarding low arc lengths and high processing rates, then clip the result so it refines but does not overpower the agent's Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Compute small offsets to add to agent Q-values based on features.\n    Inputs: numpy arrays of same shape.\n    Output: numpy array 'offsets' of same shape.\n    \"\"\"\n    # Ensure numpy arrays\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n    \n    eps = 1e-8\n\n    def normalize(x):\n        mn = np.nanmin(x)\n        mx = np.nanmax(x)\n        if mx - mn < eps:\n            return np.full_like(x, 0.5)\n        return (x - mn) / (mx - mn + eps)\n\n    # Normalized scores in [0,1]\n    # For features where lower is better (arc_lengths, distances) we invert later\n    norm_dist = normalize(distances)            # higher = worse\n    norm_arc = normalize(arc_lengths)           # higher = worse\n    norm_proc = normalize(processing_rates)     # higher = better\n    norm_queue = normalize(queue_lengths)       # higher = worse\n\n    # Convert to desirability scores in [0,1]\n    dist_score = 1.0 - norm_dist       # higher = better (closer)\n    arc_score = 1.0 - norm_arc         # higher = better (closer to dest)\n    proc_score = norm_proc             # higher = better (faster processing)\n    queue_penalty = norm_queue         # higher = worse (more queue)\n\n    # Weights chosen to refine (not overpower) Q-values; magnitudes compatible with observed Q scale\n    w_queue = -0.35   # strong negative for congestion/drop risk\n    w_arc   = +0.16   # reward being closer to destination\n    w_proc  = +0.12   # reward faster service\n    w_dist  = -0.07   # small penalty for longer propagation distance\n\n    offset_raw = (w_queue * queue_penalty +\n                  w_arc   * arc_score +\n                  w_proc  * proc_score +\n                  w_dist  * dist_score)\n\n    # If an entry is a zero-padding (all features zero), softly discourage it\n    all_zero_mask = (np.isclose(distances, 0.0) &\n                     np.isclose(arc_lengths, 0.0) &\n                     np.isclose(processing_rates, 0.0) &\n                     np.isclose(queue_lengths, 0.0))\n    # Clip to modest range so offsets refine but do not dominate learned Q-values\n    max_offset = 0.25\n    offsets = np.clip(offset_raw, -max_offset, max_offset)\n\n    # Apply a slightly stronger negative offset for padded/missing neighbours, but keep moderate\n    offsets = offsets.copy()\n    offsets[all_zero_mask] = np.minimum(offsets[all_zero_mask], -0.18)\n\n    return offsets",
          "objective": 1.4107,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.5914232362110091,
               "avg_dropped_ratio": 0.16566265060240964
          }
     },
     {
          "algorithm": "I compute a small additive offset by combining normalized closeness (inverse arc length), neighbor distance, processing rate, and a negative congestion penalty from queue lengths, scaled so offsets refine but don't overpower the agent's Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Compute small offsets to be added to agent Q-values for routing decisions.\n    Inputs: numpy arrays of same shape for distances, arc_lengths, processing_rates, queue_lengths\n    Output: numpy array of offsets (same shape)\n    \"\"\"\n    eps = 1e-6\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n\n    def norm01(x):\n        xmin = np.min(x)\n        rng = np.ptp(x)\n        if rng <= eps:\n            return np.zeros_like(x)\n        return (x - xmin) / (rng + eps)\n\n    # Normalized scores in [0,1]\n    norm_dist = norm01(d)        # 0 = closest, 1 = farthest\n    norm_arc  = norm01(a)       # 0 = nearest to dest, 1 = farthest\n    norm_proc = norm01(p)       # 0 = slowest, 1 = fastest\n    norm_q    = norm01(q)       # 0 = empty, 1 = fullest\n\n    # Feature interpretations:\n    # - arc_score: smaller arc_length is better -> inverse of normalized arc\n    arc_score = 1.0 - norm_arc\n    # - dist_score: smaller distance is better -> inverse of normalized distance\n    dist_score = 1.0 - norm_dist\n    # - proc_score: higher processing rate is better\n    proc_score = norm_proc\n    # - queue_score: higher queue length is worse (will be subtracted)\n    queue_score = norm_q\n\n    # Small weights so offsets refine (not overpower) learned Q-values\n    w_arc  = 0.06\n    w_proc = 0.05\n    w_dist = 0.03\n    w_q    = 0.09  # penalty weight for congestion\n\n    offsets = w_arc * arc_score + w_proc * proc_score + w_dist * dist_score - w_q * queue_score\n\n    # Bound offsets to a small range to avoid overpowering agent Q-values\n    max_abs_offset = 0.12\n    offsets = np.clip(offsets, -max_abs_offset, max_abs_offset)\n\n    return offsets",
          "objective": 1.327,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6105794790886263,
               "avg_dropped_ratio": 0.18975903614457829
          }
     },
     {
          "algorithm": "Combine normalized signals for queue congestion (penalize), proximity to destination (reward), processing speed (reward), and propagation distance (penalize) with modest weights to produce small offsets that refine the agent Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Compute small Q-value offsets from routing features.\n\n    Inputs (all numpy arrays, same shape):\n      - distances: physical distances (km)\n      - arc_lengths: great-circle distance remaining to destination (km)\n      - processing_rates: service rates (packets/sec)\n      - queue_lengths: current queue occupancies (packets)\n\n    Output:\n      - offsets: numpy array of same shape with small additive offsets\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    eps = 1e-8\n\n    # Normalize queue lengths to [0,1] (0 = empty, 1 = max observed or at least 1)\n    q_denom = max(queue_lengths.max(), 1.0)\n    q_norm = queue_lengths / (q_denom + eps)\n    q_term = q_norm ** 1.5              # non-linear: penalize high queues more\n\n    # Normalize arc lengths so that closer-to-destination -> value near 1\n    a_min, a_max = float(arc_lengths.min()), float(arc_lengths.max())\n    if a_max - a_min < eps:\n        a_term = np.ones_like(arc_lengths) * 0.5\n    else:\n        a_term = 1.0 - (arc_lengths - a_min) / (a_max - a_min)\n        a_term = np.clip(a_term, 0.0, 1.0) ** 1.2\n\n    # Normalize processing rate (higher is better)\n    p_min, p_max = float(processing_rates.min()), float(processing_rates.max())\n    if p_max - p_min < eps:\n        p_term = np.zeros_like(processing_rates)\n    else:\n        p_term = (processing_rates - p_min) / (p_max - p_min)\n        p_term = np.clip(p_term, 0.0, 1.0)\n\n    # Normalize distances: larger distance -> worse (penalize)\n    d_min, d_max = float(distances.min()), float(distances.max())\n    if d_max - d_min < eps:\n        d_term = np.zeros_like(distances)\n    else:\n        d_term = (distances - d_min) / (d_max - d_min)\n        d_term = np.clip(d_term, 0.0, 1.0)\n\n    # Modest weights so offsets refine (not overpower) learned Q-values\n    w_q = -0.20   # queue penalty (strong negative)\n    w_a = +0.18   # proximity reward (positive)\n    w_p = +0.10   # processing speed reward (positive)\n    w_d = -0.05   # distance penalty (negative)\n\n    offsets = w_q * q_term + w_a * a_term + w_p * p_term + w_d * d_term\n\n    # Keep offsets small and bounded to avoid overpowering Q-values\n    offsets = np.clip(offsets, -0.3, 0.3)\n\n    return offsets",
          "objective": 1.3143,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6073035030528087,
               "avg_dropped_ratio": 0.20180722891566263
          }
     },
     {
          "algorithm": "Compute a small, bounded additive offset by normalizing each feature into [0,1], rewarding low arc_length, high processing_rate, and short distance while penalizing high queue_length (with extra penalty for very congested queues), and disable zero-padded neighbours.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    Inputs: numpy arrays (same shape)\n      - distances, arc_lengths, processing_rates, queue_lengths\n    Output:\n      - offsets: numpy array (same shape)\n    \"\"\"\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    # mask for zero-padded / non-existent neighbours\n    mask_empty = (distances == 0) & (arc_lengths == 0) & (processing_rates == 0) & (queue_lengths == 0)\n\n    eps = 1e-8\n\n    # Normalizers using only non-empty entries to avoid degenerate scales\n    def normalize01(x):\n        x = np.asarray(x, dtype=float)\n        if np.all(mask_empty):\n            return np.zeros_like(x)\n        valid = ~mask_empty\n        mn = np.min(x[valid]) if np.any(valid) else np.min(x)\n        mx = np.max(x[valid]) if np.any(valid) else np.max(x)\n        rng = mx - mn\n        if rng < eps:\n            return np.zeros_like(x)\n        return (x - mn) / rng\n\n    # queue fraction: 0 (empty) .. 1 (max observed)\n    if np.any(~mask_empty):\n        max_q = max(1.0, float(np.max(queue_lengths[~mask_empty])))\n    else:\n        max_q = 1.0\n    q_frac = queue_lengths / max_q\n    q_frac = np.clip(q_frac, 0.0, 1.0)\n\n    # arc: lower is better -> invert normalized value\n    arc_frac = normalize01(arc_lengths)\n    arc_score = 1.0 - arc_frac\n\n    # processing rate: higher is better\n    p_frac = normalize01(processing_rates)\n\n    # distance: shorter is better\n    d_frac = 1.0 - normalize01(distances)\n\n    # Weights chosen to nudge Q-values without overpowering them\n    w_q   = -0.18   # penalize high queue lengths\n    w_arc =  0.08   # reward proximity to destination\n    w_p   =  0.12   # reward higher processing rate\n    w_d   =  0.02   # small reward for shorter propagation distance\n\n    offsets = w_q * q_frac + w_arc * arc_score + w_p * p_frac + w_d * d_frac\n\n    # Extra nonlinear penalty for very congested queues to reduce drop risk\n    severe_penalty = -0.05 * np.clip((q_frac - 0.8) / 0.2, 0.0, 1.0)\n    offsets = offsets + severe_penalty\n\n    # Disable offsets for zero-padded neighbours so they remain unreachable\n    offsets = offsets.astype(float)\n    offsets[mask_empty] = -np.inf\n\n    # Keep offsets bounded so they refine rather than dominate learned Q-values\n    offsets = np.where(np.isfinite(offsets), np.clip(offsets, -0.25, 0.25), offsets)\n\n    return offsets",
          "objective": 1.3133,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6192394107646466,
               "avg_dropped_ratio": 0.18674698795180722
          }
     }
]