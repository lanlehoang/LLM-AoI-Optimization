{
     "algorithm": "Compute bounded, small-magnitude offsets by robustly scaling features using median/IQR-based normalization, applying a strong nonlinear penalty to queue occupancy, rewarding proximity and processing rate with power/log transforms, lightly penalizing distance, combining with tuned weights, and mapping to a tanh-bounded offset while forcing a large negative value for missing neighbours.",
     "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    returns: offsets numpy array of same shape\n    \"\"\"\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n    \n    eps = 1e-8\n    offsets = np.zeros_like(d, dtype=float)\n    \n    # Detect missing neighbours (zero-padded)\n    missing = (d == 0) & (a == 0) & (p == 0) & (q == 0)\n    valid = ~missing\n    \n    # If no valid entries, return strong negative for all (or zeros if none missing)\n    if not np.any(valid):\n        offsets[:] = -1e6 if np.any(missing) else 0.0\n        return offsets\n    \n    # Robust scales using 75th percentile (less sensitive to outliers) with fallbacks\n    def robust_scale(x):\n        xv = x[valid]\n        q75 = np.percentile(xv, 75) if xv.size > 0 else 0.0\n        q25 = np.percentile(xv, 25) if xv.size > 0 else 0.0\n        iqr = max(q75 - q25, eps)\n        scale = max(q75, eps)\n        return scale, iqr\n    \n    d_scale, d_iqr = robust_scale(d)\n    a_scale, a_iqr = robust_scale(a)\n    p_scale, p_iqr = robust_scale(p)\n    q_scale, q_iqr = robust_scale(q)\n    \n    # Normalize to roughly [0,1], but be robust\n    d_norm = np.clip(d / (d_scale + eps), 0.0, 1.0)\n    a_norm = np.clip(a / (a_scale + eps), 0.0, 1.0)\n    p_norm = np.clip(p / (p_scale + eps), 0.0, 1.0)\n    q_norm = np.clip(q / (q_scale + eps), 0.0, 1.0)\n    \n    # Feature nonlinear scores (higher is better except distance and queue)\n    # Queue: strong nonlinear penalty to reflect overflow/drop risk (steep for medium->high occupancy)\n    queue_score = - (q_norm ** 2.4)\n    \n    # Arc length: reward closeness; slightly concave to emphasize near-destination\n    arc_score = np.clip(1.0 - (a_norm ** 1.08), 0.0, 1.0)\n    \n    # Processing: use log to reduce sensitivity, then mild power to sharpen differences\n    proc_score = (np.log1p(p) / np.log1p(p_scale + eps))\n    proc_score = np.clip(proc_score ** 0.9, 0.0, 1.0)\n    \n    # Distance: light negative penalty (propagation delay), sublinear so very long hops aren't punished excessively\n    dist_score = - (d_norm ** 0.55)\n    \n    # Weights (strongly avoid queues, reward proximity and processing, distance minor)\n    w_q = 0.70\n    w_arc = 0.17\n    w_proc = 0.10\n    w_dist = 0.03\n    \n    raw = (w_q * queue_score) + (w_arc * arc_score) + (w_proc * proc_score) + (w_dist * dist_score)\n    \n    # Bound offsets so they refine but do not dominate learned Q-values\n    max_offset = 0.15\n    scale = 2.0\n    offsets = np.tanh(raw * scale) * max_offset\n    \n    # Enforce strong negative offset for missing neighbours\n    offsets = offsets.astype(float)\n    offsets[missing] = -1e6\n    \n    return offsets",
     "objective": 1.5909,
     "other_inf": null,
     "eval_metrics": {
          "avg_aoi": 0.6039607155271691,
          "avg_dropped_ratio": 0.0391566265060241
     }
}