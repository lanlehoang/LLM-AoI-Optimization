{
     "algorithm": "I compute per-neighbor bounded offsets by robustly scaling each feature with different nonlinear transforms (sqrt/log/exponent), combining them with tuned weights that strongly penalize queue occupancy, reward proximity and processing rate, lightly penalize distance, and force a large negative offset for missing neighbours.",
     "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    returns: offsets numpy array of same shape\n    \"\"\"\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n    \n    # Output\n    offsets = np.zeros_like(d, dtype=float)\n    \n    # Missing neighbours (zero-padded)\n    missing = (d == 0) & (a == 0) & (p == 0) & (q == 0)\n    \n    # Robust per-sample scales computed over non-missing entries\n    valid = ~missing\n    eps = 1e-8\n    if np.any(valid):\n        d_max = np.max(d[valid])\n        a_max = np.max(a[valid])\n        p_max = np.max(p[valid])\n        q_max = np.max(q[valid])\n    else:\n        d_max = a_max = p_max = q_max = 1.0\n    \n    d_max = max(d_max, eps)\n    a_max = max(a_max, eps)\n    p_max = max(p_max, eps)\n    q_max = max(q_max, eps)\n    \n    # Normalize to [0,1]\n    d_norm = np.clip(d / d_max, 0.0, 1.0)\n    a_norm = np.clip(a / a_max, 0.0, 1.0)\n    p_norm = np.clip(p / p_max, 0.0, 1.0)\n    q_norm = np.clip(q / q_max, 0.0, 1.0)\n    \n    # Per-feature nonlinear scores (higher is better except distance and queue)\n    # Queue: strong negative, amplify heavy occupancy\n    queue_score = - (q_norm ** 1.6)\n    # Arc: closer to destination -> positive, emphasize very close nodes via sqrt\n    arc_score = np.sqrt(np.clip(1.0 - a_norm, 0.0, 1.0))\n    # Processing: reward higher rates, use log1p to reduce sensitivity to outliers\n    proc_score = np.log1p(p) / np.log1p(p_max + eps)\n    # Distance: small negative penalty, sqrt to emphasize longer hops slightly\n    dist_score = - np.sqrt(d_norm)\n    \n    # Weights prioritize queue avoidance, then proximity and processing; distance small\n    w_q = 0.60\n    w_arc = 0.22\n    w_proc = 0.14\n    w_dist = 0.04\n    \n    raw = (w_q * queue_score) + (w_arc * arc_score) + (w_proc * proc_score) + (w_dist * dist_score)\n    \n    # Map to bounded offsets using tanh nonlinearity so we refine, not overpower Q-values\n    max_offset = 0.12\n    scale = 1.8\n    offsets = np.tanh(raw * scale) * max_offset\n    \n    # Strong negative for missing neighbours\n    offsets = offsets.astype(float)\n    offsets[missing] = -1e3\n    \n    return offsets",
     "objective": 1.5546,
     "other_inf": null,
     "eval_metrics": {
          "avg_aoi": 0.6180528533093089,
          "avg_dropped_ratio": 0.03915662650602409
     }
}