[
     {
          "algorithm": "I compute per-neighbor bounded offsets by robustly scaling each feature with different nonlinear transforms (sqrt/log/exponent), combining them with tuned weights that strongly penalize queue occupancy, reward proximity and processing rate, lightly penalize distance, and force a large negative offset for missing neighbours.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    returns: offsets numpy array of same shape\n    \"\"\"\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n    \n    # Output\n    offsets = np.zeros_like(d, dtype=float)\n    \n    # Missing neighbours (zero-padded)\n    missing = (d == 0) & (a == 0) & (p == 0) & (q == 0)\n    \n    # Robust per-sample scales computed over non-missing entries\n    valid = ~missing\n    eps = 1e-8\n    if np.any(valid):\n        d_max = np.max(d[valid])\n        a_max = np.max(a[valid])\n        p_max = np.max(p[valid])\n        q_max = np.max(q[valid])\n    else:\n        d_max = a_max = p_max = q_max = 1.0\n    \n    d_max = max(d_max, eps)\n    a_max = max(a_max, eps)\n    p_max = max(p_max, eps)\n    q_max = max(q_max, eps)\n    \n    # Normalize to [0,1]\n    d_norm = np.clip(d / d_max, 0.0, 1.0)\n    a_norm = np.clip(a / a_max, 0.0, 1.0)\n    p_norm = np.clip(p / p_max, 0.0, 1.0)\n    q_norm = np.clip(q / q_max, 0.0, 1.0)\n    \n    # Per-feature nonlinear scores (higher is better except distance and queue)\n    # Queue: strong negative, amplify heavy occupancy\n    queue_score = - (q_norm ** 1.6)\n    # Arc: closer to destination -> positive, emphasize very close nodes via sqrt\n    arc_score = np.sqrt(np.clip(1.0 - a_norm, 0.0, 1.0))\n    # Processing: reward higher rates, use log1p to reduce sensitivity to outliers\n    proc_score = np.log1p(p) / np.log1p(p_max + eps)\n    # Distance: small negative penalty, sqrt to emphasize longer hops slightly\n    dist_score = - np.sqrt(d_norm)\n    \n    # Weights prioritize queue avoidance, then proximity and processing; distance small\n    w_q = 0.60\n    w_arc = 0.22\n    w_proc = 0.14\n    w_dist = 0.04\n    \n    raw = (w_q * queue_score) + (w_arc * arc_score) + (w_proc * proc_score) + (w_dist * dist_score)\n    \n    # Map to bounded offsets using tanh nonlinearity so we refine, not overpower Q-values\n    max_offset = 0.12\n    scale = 1.8\n    offsets = np.tanh(raw * scale) * max_offset\n    \n    # Strong negative for missing neighbours\n    offsets = offsets.astype(float)\n    offsets[missing] = -1e3\n    \n    return offsets",
          "objective": 1.5546,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6180528533093089,
               "avg_dropped_ratio": 0.03915662650602409
          }
     },
     {
          "algorithm": "A robust offset function that sharply penalizes queue congestion with an exponential term, rewards proximity via a concave proximity boost, favors high processing rates with sublinear scaling, lightly penalizes propagation distance, and maps the combined score into bounded offsets via tanh while marking zero-padded neighbours as missing.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    returns: offsets numpy array of same shape\n    \"\"\"\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n\n    # initialize\n    offsets = np.zeros_like(d, dtype=float)\n\n    # detect missing neighbours (zero-padded)\n    missing = (d == 0) & (a == 0) & (p == 0) & (q == 0)\n    valid = ~missing\n    eps = 1e-8\n\n    # robust maxima over valid entries (guard against all-missing)\n    if np.any(valid):\n        d_max = np.max(d[valid])\n        a_max = np.max(a[valid])\n        p_max = np.max(p[valid])\n        q_max = np.max(q[valid])\n    else:\n        d_max = a_max = p_max = q_max = 1.0\n\n    d_max = max(d_max, eps)\n    a_max = max(a_max, eps)\n    p_max = max(p_max, eps)\n    q_max = max(q_max, eps)\n\n    # normalized features in [0,1]\n    d_norm = np.clip(d / d_max, 0.0, 1.0)\n    a_norm = np.clip(a / a_max, 0.0, 1.0)\n    p_norm = np.clip(p / p_max, 0.0, 1.0)\n    q_norm = np.clip(q / q_max, 0.0, 1.0)\n\n    # Feature-specific scores (higher is better except distance and queue)\n    # Queue: stronger exponential penalty for occupancy (sharply negative for heavy queues)\n    queue_score = - (q_norm ** 2.2)\n\n    # Arc length: emphasize very close neighbours via 1.5 exponent (concave near 0)\n    arc_score = np.clip((1.0 - a_norm) ** 1.5, 0.0, 1.0)\n\n    # Processing: sublinear reward to favor higher rates but reduce outlier influence\n    proc_score = p_norm ** 0.7\n\n    # Distance: light negative penalty proportional to normalized distance with mild concavity\n    dist_score = - (d_norm ** 1.05)\n\n    # Weights (different from original): prioritize queue avoidance, then proximity, then processing, small distance penalty\n    w_q = 0.65\n    w_arc = 0.20\n    w_proc = 0.10\n    w_dist = 0.05\n\n    raw = (w_q * queue_score) + (w_arc * arc_score) + (w_proc * proc_score) + (w_dist * dist_score)\n\n    # Bound offsets so they refine learned Q-values (smaller magnitude than agent Q-values)\n    max_offset = 0.10\n    scale = 2.0\n    offsets = np.tanh(raw * scale) * max_offset\n\n    # Strong negative for missing neighbours\n    offsets = offsets.astype(float)\n    offsets[missing] = -1e3\n\n    return offsets",
          "objective": 1.5292,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6293083371187551,
               "avg_dropped_ratio": 0.03765060240963856
          }
     },
     {
          "algorithm": "A per-step, robust-offset that clips each feature to its 5\u201395 percentile range, maps them to bounded scores (penalizing high queue occupancy strongly, rewarding short arc and high processing rate, slightly penalizing long distance), combines weighted contributions, squashes the result with tanh to keep offsets small, and forces a very large negative value for zero-padded neighbours.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    returns: offsets numpy array of same shape\n    \"\"\"\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n    \n    # Prepare output\n    offsets = np.zeros_like(d, dtype=float)\n    eps = 1e-8\n    \n    # Missing neighbours detection (zero-padded)\n    missing = (d == 0) & (a == 0) & (p == 0) & (q == 0)\n    \n    def robust_norm(x):\n        # Clip to 5-95 percentile to reduce influence of outliers, then scale to [0,1]\n        if x.size == 0:\n            return x.astype(float)\n        p05 = np.percentile(x, 5)\n        p95 = np.percentile(x, 95)\n        if np.isclose(p95, p05):\n            # fallback to min-max\n            mn = x.min()\n            mx = x.max()\n            if np.isclose(mx, mn):\n                return np.zeros_like(x, dtype=float)\n            return (x - mn) / (mx - mn + eps)\n        x_clipped = np.clip(x, p05, p95)\n        return (x_clipped - p05) / (p95 - p05 + eps)\n    \n    d_n = robust_norm(d)   # larger is worse (longer propagation)\n    a_n = robust_norm(a)   # smaller is better => will use (1 - a_n)\n    p_n = robust_norm(p)   # larger is better\n    q_n = robust_norm(q)   # larger is worse\n    \n    # Transform scores into [-1,1] style contributions\n    # Queue: strong negative for high occupancy (use convex penalty)\n    q_score = - (q_n ** 1.9)          # in [-1,0]\n    # Arc: closeness positive\n    arc_score = (1.0 - a_n)           # in [0,1]\n    # Processing: positive, but saturating with sqrt to reduce outlier effect\n    proc_score = np.sqrt(p_n)         # in [0,1]\n    # Distance: small negative influence, use gentle convex penalty\n    dist_score = - (d_n ** 1.2)       # in [-1,0]\n    \n    # Weights emphasizing queue avoidance, then proximity and processing, tiny weight on distance\n    w_q = 0.58\n    w_arc = 0.24\n    w_proc = 0.15\n    w_dist = 0.03\n    \n    raw = (w_q * q_score) + (w_arc * arc_score) + (w_proc * proc_score) + (w_dist * dist_score)\n    \n    # Squash to a small bounded offset range so we refine but don't overpower Q-values\n    max_offset = 0.12\n    # use tanh to ensure smooth bounding; scale factor tuned to keep typical values modest\n    offsets = np.tanh(raw * 1.1) * max_offset\n    \n    # Strong negative for missing neighbours\n    offsets = offsets.astype(float)\n    offsets[missing] = -1e3\n    \n    return offsets",
          "objective": 1.5261,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6226936288554553,
               "avg_dropped_ratio": 0.04969879518072289
          }
     },
     {
          "algorithm": "I compute a small bounded offset by normalizing each feature per-step, giving a strong negative contribution for high queue occupancy, positive contributions for being closer to destination and for higher processing rate, a small penalty for longer distance, and a very large negative value for zero-padded (missing) neighbours so the offset refines but does not overpower learned Q-values.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    returns: offsets numpy array of same shape\n    \"\"\"\n    # Ensure numpy arrays\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n    \n    # Prepare output\n    offsets = np.zeros_like(d, dtype=float)\n    \n    # Detect missing (zero-padded) neighbours: all features zero -> strongly negative offset\n    missing = (d == 0) & (a == 0) & (p == 0) & (q == 0)\n    \n    # Normalize features robustly (per-sample scaling)\n    eps = 1e-6\n    # For each vector, normalize to [0,1] by dividing by max (if max==0, result stays 0)\n    d_max = d.max() if d.size else 0.0\n    a_max = a.max() if a.size else 0.0\n    p_max = p.max() if p.size else 0.0\n    q_max = q.max() if q.size else 0.0\n    \n    d_norm = d / (d_max + eps)\n    a_norm = a / (a_max + eps)\n    p_norm = p / (p_max + eps)\n    q_norm = q / (q_max + eps)\n    \n    # Feature contributions (signed): higher is better\n    # - queue: negative contribution, penalize heavier for high q (exponentiate)\n    # - arc: closer (smaller arc) -> positive contribution (use 1 - a_norm)\n    # - processing: positive contribution\n    # - distance: small negative contribution\n    q_contrib = - (q_norm ** 1.25)\n    arc_contrib = (1.0 - a_norm)\n    proc_contrib = p_norm\n    dist_contrib = - d_norm\n    \n    # Weights chosen to prioritize queue avoidance, then proximity and service rate, small weight on distance\n    w_q = 0.50\n    w_arc = 0.28\n    w_proc = 0.18\n    w_dist = 0.04  # small\n    \n    raw_score = (w_q * q_contrib) + (w_arc * arc_contrib) + (w_proc * proc_contrib) + (w_dist * dist_contrib)\n    \n    # Bound the final offsets to be small compared to typical Q-values (~ +/-0.5).\n    max_offset = 0.12  # small refinement scale\n    offsets = raw_score * max_offset  # now typically in [-max_offset, +max_offset]\n    \n    # Assign strong negative for missing neighbours (so they won't be selected even if Q-values are malformed)\n    offsets = offsets.astype(float)\n    offsets[missing] = -1e3\n    \n    return offsets",
          "objective": 1.5176,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6251895413182893,
               "avg_dropped_ratio": 0.05120481927710843
          }
     }
]