[
     {
          "algorithm": "Compute bounded, small-magnitude offsets by robustly scaling features using median/IQR-based normalization, applying a strong nonlinear penalty to queue occupancy, rewarding proximity and processing rate with power/log transforms, lightly penalizing distance, combining with tuned weights, and mapping to a tanh-bounded offset while forcing a large negative value for missing neighbours.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    returns: offsets numpy array of same shape\n    \"\"\"\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n    \n    eps = 1e-8\n    offsets = np.zeros_like(d, dtype=float)\n    \n    # Detect missing neighbours (zero-padded)\n    missing = (d == 0) & (a == 0) & (p == 0) & (q == 0)\n    valid = ~missing\n    \n    # If no valid entries, return strong negative for all (or zeros if none missing)\n    if not np.any(valid):\n        offsets[:] = -1e6 if np.any(missing) else 0.0\n        return offsets\n    \n    # Robust scales using 75th percentile (less sensitive to outliers) with fallbacks\n    def robust_scale(x):\n        xv = x[valid]\n        q75 = np.percentile(xv, 75) if xv.size > 0 else 0.0\n        q25 = np.percentile(xv, 25) if xv.size > 0 else 0.0\n        iqr = max(q75 - q25, eps)\n        scale = max(q75, eps)\n        return scale, iqr\n    \n    d_scale, d_iqr = robust_scale(d)\n    a_scale, a_iqr = robust_scale(a)\n    p_scale, p_iqr = robust_scale(p)\n    q_scale, q_iqr = robust_scale(q)\n    \n    # Normalize to roughly [0,1], but be robust\n    d_norm = np.clip(d / (d_scale + eps), 0.0, 1.0)\n    a_norm = np.clip(a / (a_scale + eps), 0.0, 1.0)\n    p_norm = np.clip(p / (p_scale + eps), 0.0, 1.0)\n    q_norm = np.clip(q / (q_scale + eps), 0.0, 1.0)\n    \n    # Feature nonlinear scores (higher is better except distance and queue)\n    # Queue: strong nonlinear penalty to reflect overflow/drop risk (steep for medium->high occupancy)\n    queue_score = - (q_norm ** 2.4)\n    \n    # Arc length: reward closeness; slightly concave to emphasize near-destination\n    arc_score = np.clip(1.0 - (a_norm ** 1.08), 0.0, 1.0)\n    \n    # Processing: use log to reduce sensitivity, then mild power to sharpen differences\n    proc_score = (np.log1p(p) / np.log1p(p_scale + eps))\n    proc_score = np.clip(proc_score ** 0.9, 0.0, 1.0)\n    \n    # Distance: light negative penalty (propagation delay), sublinear so very long hops aren't punished excessively\n    dist_score = - (d_norm ** 0.55)\n    \n    # Weights (strongly avoid queues, reward proximity and processing, distance minor)\n    w_q = 0.70\n    w_arc = 0.17\n    w_proc = 0.10\n    w_dist = 0.03\n    \n    raw = (w_q * queue_score) + (w_arc * arc_score) + (w_proc * proc_score) + (w_dist * dist_score)\n    \n    # Bound offsets so they refine but do not dominate learned Q-values\n    max_offset = 0.15\n    scale = 2.0\n    offsets = np.tanh(raw * scale) * max_offset\n    \n    # Enforce strong negative offset for missing neighbours\n    offsets = offsets.astype(float)\n    offsets[missing] = -1e6\n    \n    return offsets",
          "objective": 1.5909,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6039607155271691,
               "avg_dropped_ratio": 0.0391566265060241
          }
     },
     {
          "algorithm": "I compute per-neighbor bounded offsets by robustly scaling each feature with different nonlinear transforms (sqrt/log/exponent), combining them with tuned weights that strongly penalize queue occupancy, reward proximity and processing rate, lightly penalize distance, and force a large negative offset for missing neighbours.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    returns: offsets numpy array of same shape\n    \"\"\"\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n    \n    # Output\n    offsets = np.zeros_like(d, dtype=float)\n    \n    # Missing neighbours (zero-padded)\n    missing = (d == 0) & (a == 0) & (p == 0) & (q == 0)\n    \n    # Robust per-sample scales computed over non-missing entries\n    valid = ~missing\n    eps = 1e-8\n    if np.any(valid):\n        d_max = np.max(d[valid])\n        a_max = np.max(a[valid])\n        p_max = np.max(p[valid])\n        q_max = np.max(q[valid])\n    else:\n        d_max = a_max = p_max = q_max = 1.0\n    \n    d_max = max(d_max, eps)\n    a_max = max(a_max, eps)\n    p_max = max(p_max, eps)\n    q_max = max(q_max, eps)\n    \n    # Normalize to [0,1]\n    d_norm = np.clip(d / d_max, 0.0, 1.0)\n    a_norm = np.clip(a / a_max, 0.0, 1.0)\n    p_norm = np.clip(p / p_max, 0.0, 1.0)\n    q_norm = np.clip(q / q_max, 0.0, 1.0)\n    \n    # Per-feature nonlinear scores (higher is better except distance and queue)\n    # Queue: strong negative, amplify heavy occupancy\n    queue_score = - (q_norm ** 1.6)\n    # Arc: closer to destination -> positive, emphasize very close nodes via sqrt\n    arc_score = np.sqrt(np.clip(1.0 - a_norm, 0.0, 1.0))\n    # Processing: reward higher rates, use log1p to reduce sensitivity to outliers\n    proc_score = np.log1p(p) / np.log1p(p_max + eps)\n    # Distance: small negative penalty, sqrt to emphasize longer hops slightly\n    dist_score = - np.sqrt(d_norm)\n    \n    # Weights prioritize queue avoidance, then proximity and processing; distance small\n    w_q = 0.60\n    w_arc = 0.22\n    w_proc = 0.14\n    w_dist = 0.04\n    \n    raw = (w_q * queue_score) + (w_arc * arc_score) + (w_proc * proc_score) + (w_dist * dist_score)\n    \n    # Map to bounded offsets using tanh nonlinearity so we refine, not overpower Q-values\n    max_offset = 0.12\n    scale = 1.8\n    offsets = np.tanh(raw * scale) * max_offset\n    \n    # Strong negative for missing neighbours\n    offsets = offsets.astype(float)\n    offsets[missing] = -1e3\n    \n    return offsets",
          "objective": 1.5546,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6180528533093089,
               "avg_dropped_ratio": 0.03915662650602409
          }
     },
     {
          "algorithm": "A robust offset function that sharply penalizes queue congestion with an exponential term, rewards proximity via a concave proximity boost, favors high processing rates with sublinear scaling, lightly penalizes propagation distance, and maps the combined score into bounded offsets via tanh while marking zero-padded neighbours as missing.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    returns: offsets numpy array of same shape\n    \"\"\"\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n\n    # initialize\n    offsets = np.zeros_like(d, dtype=float)\n\n    # detect missing neighbours (zero-padded)\n    missing = (d == 0) & (a == 0) & (p == 0) & (q == 0)\n    valid = ~missing\n    eps = 1e-8\n\n    # robust maxima over valid entries (guard against all-missing)\n    if np.any(valid):\n        d_max = np.max(d[valid])\n        a_max = np.max(a[valid])\n        p_max = np.max(p[valid])\n        q_max = np.max(q[valid])\n    else:\n        d_max = a_max = p_max = q_max = 1.0\n\n    d_max = max(d_max, eps)\n    a_max = max(a_max, eps)\n    p_max = max(p_max, eps)\n    q_max = max(q_max, eps)\n\n    # normalized features in [0,1]\n    d_norm = np.clip(d / d_max, 0.0, 1.0)\n    a_norm = np.clip(a / a_max, 0.0, 1.0)\n    p_norm = np.clip(p / p_max, 0.0, 1.0)\n    q_norm = np.clip(q / q_max, 0.0, 1.0)\n\n    # Feature-specific scores (higher is better except distance and queue)\n    # Queue: stronger exponential penalty for occupancy (sharply negative for heavy queues)\n    queue_score = - (q_norm ** 2.2)\n\n    # Arc length: emphasize very close neighbours via 1.5 exponent (concave near 0)\n    arc_score = np.clip((1.0 - a_norm) ** 1.5, 0.0, 1.0)\n\n    # Processing: sublinear reward to favor higher rates but reduce outlier influence\n    proc_score = p_norm ** 0.7\n\n    # Distance: light negative penalty proportional to normalized distance with mild concavity\n    dist_score = - (d_norm ** 1.05)\n\n    # Weights (different from original): prioritize queue avoidance, then proximity, then processing, small distance penalty\n    w_q = 0.65\n    w_arc = 0.20\n    w_proc = 0.10\n    w_dist = 0.05\n\n    raw = (w_q * queue_score) + (w_arc * arc_score) + (w_proc * proc_score) + (w_dist * dist_score)\n\n    # Bound offsets so they refine learned Q-values (smaller magnitude than agent Q-values)\n    max_offset = 0.10\n    scale = 2.0\n    offsets = np.tanh(raw * scale) * max_offset\n\n    # Strong negative for missing neighbours\n    offsets = offsets.astype(float)\n    offsets[missing] = -1e3\n\n    return offsets",
          "objective": 1.5292,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6293083371187551,
               "avg_dropped_ratio": 0.03765060240963856
          }
     },
     {
          "algorithm": "A per-step, robust-offset that clips each feature to its 5\u201395 percentile range, maps them to bounded scores (penalizing high queue occupancy strongly, rewarding short arc and high processing rate, slightly penalizing long distance), combines weighted contributions, squashes the result with tanh to keep offsets small, and forces a very large negative value for zero-padded neighbours.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    returns: offsets numpy array of same shape\n    \"\"\"\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n    \n    # Prepare output\n    offsets = np.zeros_like(d, dtype=float)\n    eps = 1e-8\n    \n    # Missing neighbours detection (zero-padded)\n    missing = (d == 0) & (a == 0) & (p == 0) & (q == 0)\n    \n    def robust_norm(x):\n        # Clip to 5-95 percentile to reduce influence of outliers, then scale to [0,1]\n        if x.size == 0:\n            return x.astype(float)\n        p05 = np.percentile(x, 5)\n        p95 = np.percentile(x, 95)\n        if np.isclose(p95, p05):\n            # fallback to min-max\n            mn = x.min()\n            mx = x.max()\n            if np.isclose(mx, mn):\n                return np.zeros_like(x, dtype=float)\n            return (x - mn) / (mx - mn + eps)\n        x_clipped = np.clip(x, p05, p95)\n        return (x_clipped - p05) / (p95 - p05 + eps)\n    \n    d_n = robust_norm(d)   # larger is worse (longer propagation)\n    a_n = robust_norm(a)   # smaller is better => will use (1 - a_n)\n    p_n = robust_norm(p)   # larger is better\n    q_n = robust_norm(q)   # larger is worse\n    \n    # Transform scores into [-1,1] style contributions\n    # Queue: strong negative for high occupancy (use convex penalty)\n    q_score = - (q_n ** 1.9)          # in [-1,0]\n    # Arc: closeness positive\n    arc_score = (1.0 - a_n)           # in [0,1]\n    # Processing: positive, but saturating with sqrt to reduce outlier effect\n    proc_score = np.sqrt(p_n)         # in [0,1]\n    # Distance: small negative influence, use gentle convex penalty\n    dist_score = - (d_n ** 1.2)       # in [-1,0]\n    \n    # Weights emphasizing queue avoidance, then proximity and processing, tiny weight on distance\n    w_q = 0.58\n    w_arc = 0.24\n    w_proc = 0.15\n    w_dist = 0.03\n    \n    raw = (w_q * q_score) + (w_arc * arc_score) + (w_proc * proc_score) + (w_dist * dist_score)\n    \n    # Squash to a small bounded offset range so we refine but don't overpower Q-values\n    max_offset = 0.12\n    # use tanh to ensure smooth bounding; scale factor tuned to keep typical values modest\n    offsets = np.tanh(raw * 1.1) * max_offset\n    \n    # Strong negative for missing neighbours\n    offsets = offsets.astype(float)\n    offsets[missing] = -1e3\n    \n    return offsets",
          "objective": 1.5261,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6226936288554553,
               "avg_dropped_ratio": 0.04969879518072289
          }
     }
]