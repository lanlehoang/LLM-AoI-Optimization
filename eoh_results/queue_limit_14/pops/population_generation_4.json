[
     {
          "algorithm": "Compute bounded, small-magnitude offsets by robustly scaling features using median/IQR-based normalization, applying a strong nonlinear penalty to queue occupancy, rewarding proximity and processing rate with power/log transforms, lightly penalizing distance, combining with tuned weights, and mapping to a tanh-bounded offset while forcing a large negative value for missing neighbours.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    returns: offsets numpy array of same shape\n    \"\"\"\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n    \n    eps = 1e-8\n    offsets = np.zeros_like(d, dtype=float)\n    \n    # Detect missing neighbours (zero-padded)\n    missing = (d == 0) & (a == 0) & (p == 0) & (q == 0)\n    valid = ~missing\n    \n    # If no valid entries, return strong negative for all (or zeros if none missing)\n    if not np.any(valid):\n        offsets[:] = -1e6 if np.any(missing) else 0.0\n        return offsets\n    \n    # Robust scales using 75th percentile (less sensitive to outliers) with fallbacks\n    def robust_scale(x):\n        xv = x[valid]\n        q75 = np.percentile(xv, 75) if xv.size > 0 else 0.0\n        q25 = np.percentile(xv, 25) if xv.size > 0 else 0.0\n        iqr = max(q75 - q25, eps)\n        scale = max(q75, eps)\n        return scale, iqr\n    \n    d_scale, d_iqr = robust_scale(d)\n    a_scale, a_iqr = robust_scale(a)\n    p_scale, p_iqr = robust_scale(p)\n    q_scale, q_iqr = robust_scale(q)\n    \n    # Normalize to roughly [0,1], but be robust\n    d_norm = np.clip(d / (d_scale + eps), 0.0, 1.0)\n    a_norm = np.clip(a / (a_scale + eps), 0.0, 1.0)\n    p_norm = np.clip(p / (p_scale + eps), 0.0, 1.0)\n    q_norm = np.clip(q / (q_scale + eps), 0.0, 1.0)\n    \n    # Feature nonlinear scores (higher is better except distance and queue)\n    # Queue: strong nonlinear penalty to reflect overflow/drop risk (steep for medium->high occupancy)\n    queue_score = - (q_norm ** 2.4)\n    \n    # Arc length: reward closeness; slightly concave to emphasize near-destination\n    arc_score = np.clip(1.0 - (a_norm ** 1.08), 0.0, 1.0)\n    \n    # Processing: use log to reduce sensitivity, then mild power to sharpen differences\n    proc_score = (np.log1p(p) / np.log1p(p_scale + eps))\n    proc_score = np.clip(proc_score ** 0.9, 0.0, 1.0)\n    \n    # Distance: light negative penalty (propagation delay), sublinear so very long hops aren't punished excessively\n    dist_score = - (d_norm ** 0.55)\n    \n    # Weights (strongly avoid queues, reward proximity and processing, distance minor)\n    w_q = 0.70\n    w_arc = 0.17\n    w_proc = 0.10\n    w_dist = 0.03\n    \n    raw = (w_q * queue_score) + (w_arc * arc_score) + (w_proc * proc_score) + (w_dist * dist_score)\n    \n    # Bound offsets so they refine but do not dominate learned Q-values\n    max_offset = 0.15\n    scale = 2.0\n    offsets = np.tanh(raw * scale) * max_offset\n    \n    # Enforce strong negative offset for missing neighbours\n    offsets = offsets.astype(float)\n    offsets[missing] = -1e6\n    \n    return offsets",
          "objective": 1.5909,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6039607155271691,
               "avg_dropped_ratio": 0.0391566265060241
          }
     },
     {
          "algorithm": "I compute per-neighbor bounded offsets by robustly scaling each feature with different nonlinear transforms (sqrt/log/exponent), combining them with tuned weights that strongly penalize queue occupancy, reward proximity and processing rate, lightly penalize distance, and force a large negative offset for missing neighbours.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    returns: offsets numpy array of same shape\n    \"\"\"\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n    \n    # Output\n    offsets = np.zeros_like(d, dtype=float)\n    \n    # Missing neighbours (zero-padded)\n    missing = (d == 0) & (a == 0) & (p == 0) & (q == 0)\n    \n    # Robust per-sample scales computed over non-missing entries\n    valid = ~missing\n    eps = 1e-8\n    if np.any(valid):\n        d_max = np.max(d[valid])\n        a_max = np.max(a[valid])\n        p_max = np.max(p[valid])\n        q_max = np.max(q[valid])\n    else:\n        d_max = a_max = p_max = q_max = 1.0\n    \n    d_max = max(d_max, eps)\n    a_max = max(a_max, eps)\n    p_max = max(p_max, eps)\n    q_max = max(q_max, eps)\n    \n    # Normalize to [0,1]\n    d_norm = np.clip(d / d_max, 0.0, 1.0)\n    a_norm = np.clip(a / a_max, 0.0, 1.0)\n    p_norm = np.clip(p / p_max, 0.0, 1.0)\n    q_norm = np.clip(q / q_max, 0.0, 1.0)\n    \n    # Per-feature nonlinear scores (higher is better except distance and queue)\n    # Queue: strong negative, amplify heavy occupancy\n    queue_score = - (q_norm ** 1.6)\n    # Arc: closer to destination -> positive, emphasize very close nodes via sqrt\n    arc_score = np.sqrt(np.clip(1.0 - a_norm, 0.0, 1.0))\n    # Processing: reward higher rates, use log1p to reduce sensitivity to outliers\n    proc_score = np.log1p(p) / np.log1p(p_max + eps)\n    # Distance: small negative penalty, sqrt to emphasize longer hops slightly\n    dist_score = - np.sqrt(d_norm)\n    \n    # Weights prioritize queue avoidance, then proximity and processing; distance small\n    w_q = 0.60\n    w_arc = 0.22\n    w_proc = 0.14\n    w_dist = 0.04\n    \n    raw = (w_q * queue_score) + (w_arc * arc_score) + (w_proc * proc_score) + (w_dist * dist_score)\n    \n    # Map to bounded offsets using tanh nonlinearity so we refine, not overpower Q-values\n    max_offset = 0.12\n    scale = 1.8\n    offsets = np.tanh(raw * scale) * max_offset\n    \n    # Strong negative for missing neighbours\n    offsets = offsets.astype(float)\n    offsets[missing] = -1e3\n    \n    return offsets",
          "objective": 1.5546,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6180528533093089,
               "avg_dropped_ratio": 0.03915662650602409
          }
     },
     {
          "algorithm": "",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    returns: offsets numpy array of same shape\n    \"\"\"\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n    \n    eps = 1e-8\n    offsets = np.zeros_like(d, dtype=float)\n    \n    # Detect missing neighbours (zero-padded)\n    missing = (d == 0) & (a == 0) & (p == 0) & (q == 0)\n    valid = ~missing\n    \n    # If no valid entries, return strong negative for all (or zeros if none missing)\n    if not np.any(valid):\n        offsets[:] = -1e6 if np.any(missing) else 0.0\n        return offsets\n    \n    # Robust scale: use median and MAD to avoid sensitivity to outliers\n    def robust_scale(x):\n        xv = x[valid]\n        if xv.size == 0:\n            return 1.0\n        med = np.median(xv)\n        mad = np.median(np.abs(xv - med))\n        # avoid zero scale\n        return float(max(med, mad, eps))\n    \n    d_scale = robust_scale(d)\n    a_scale = robust_scale(a)\n    p_scale = robust_scale(p)\n    q_scale = robust_scale(q)\n    \n    # Normalize into [0,1] using robust scales\n    d_norm = np.clip(d / (d_scale + eps), 0.0, 1.0)\n    a_norm = np.clip(a / (a_scale + eps), 0.0, 1.0)\n    p_norm = np.clip(p / (p_scale + eps), 0.0, 1.0)\n    q_norm = np.clip(q / (q_scale + eps), 0.0, 1.0)\n    \n    # Simple, more general feature transforms\n    queue_score = - (q_norm ** 2)          # stronger penalty for higher occupancy\n    arc_score = 1.0 - a_norm               # reward closeness linearly\n    proc_score = np.sqrt(p_norm)           # diminishing returns for higher processing\n    dist_score = - np.sqrt(d_norm)         # light penalty for distance\n    \n    # Balanced, simple weights that are less likely to overfit\n    w_q = 0.5\n    w_arc = 0.25\n    w_proc = 0.15\n    w_dist = 0.10\n    \n    raw = (w_q * queue_score) + (w_arc * arc_score) + (w_proc * proc_score) + (w_dist * dist_score)\n    \n    # Linear, bounded offsets to avoid dominating learned values\n    max_offset = 0.15\n    offsets = np.clip(raw * max_offset, -max_offset, max_offset)\n    \n    # Enforce strong negative offset for missing neighbours\n    offsets = offsets.astype(float)\n    offsets[missing] = -1e6\n    \n    return offsets",
          "objective": 1.5392,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6330581825951932,
               "avg_dropped_ratio": 0.025602409638554216
          }
     },
     {
          "algorithm": "Use robust median/IQR normalization, soften the queue penalty while boosting processing reward, adjust nonlinear exponents and weights, and produce small tanh-bounded offsets with a strong negative mask for missing neighbours.",
          "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    \"\"\"\n    distances, arc_lengths, processing_rates, queue_lengths: numpy arrays of same shape\n    returns: offsets numpy array of same shape\n    \"\"\"\n    d = np.asarray(distances, dtype=float)\n    a = np.asarray(arc_lengths, dtype=float)\n    p = np.asarray(processing_rates, dtype=float)\n    q = np.asarray(queue_lengths, dtype=float)\n    \n    eps = 1e-9\n    offsets = np.zeros_like(d, dtype=float)\n    \n    # Detect missing neighbours (zero-padded)\n    missing = (d == 0) & (a == 0) & (p == 0) & (q == 0)\n    valid = ~missing\n    \n    if not np.any(valid):\n        offsets[:] = -1e6 if np.any(missing) else 0.0\n        return offsets\n    \n    # Robust stats: median and IQR (less sensitive to outliers)\n    def med_iqr_scale(x):\n        xv = x[valid]\n        med = np.median(xv) if xv.size > 0 else 0.0\n        q75 = np.percentile(xv, 75) if xv.size > 0 else med\n        q25 = np.percentile(xv, 25) if xv.size > 0 else med\n        iqr = max(q75 - q25, eps)\n        # Use a conservative scale to cap most values near 1: med + 1.5*iqr (like robust range)\n        scale = max(abs(med) + 1.5 * iqr, eps)\n        return med, scale, iqr\n    \n    d_med, d_scale, d_iqr = med_iqr_scale(d)\n    a_med, a_scale, a_iqr = med_iqr_scale(a)\n    p_med, p_scale, p_iqr = med_iqr_scale(p)\n    q_med, q_scale, q_iqr = med_iqr_scale(q)\n    \n    # Normalizations (map to roughly [0,1] for typical ranges)\n    d_norm = np.clip(d / (d_scale + eps), 0.0, 1.0)\n    # arc: smaller is better => proximity ratio\n    arc_ratio = np.clip(a / (a_scale + eps), 0.0, 1.5)\n    # processing: higher is better\n    p_norm = np.clip(p / (p_scale + eps), 0.0, 2.0)\n    # queue: higher is worse\n    q_norm = np.clip(q / (q_scale + eps), 0.0, 2.0)\n    \n    # Nonlinear feature scorings (higher means better, except queue and distance)\n    # Queue: milder but still strong penalty for higher occupancy (steeper than linear but less than previous)\n    queue_score = - (q_norm ** 3.0) * 0.95  # slightly scaled\n    \n    # Arc length: emphasize closeness with sublinear exponent to favor near-destination\n    arc_score = np.clip(1.0 - (arc_ratio ** 0.9), 0.0, 1.0)\n    \n    # Processing: stronger reward than before using a mild power to accentuate good servers\n    proc_score = np.clip((p_norm ** 1.25) / (1.0 + (p_norm ** 1.25)), 0.0, 1.0)\n    \n    # Distance: light negative penalty, less severe for very long hops (sublinear)\n    dist_score = - (d_norm ** 0.4)\n    \n    # Weights: reduce queue dominance slightly, increase processing influence, keep distance minor\n    w_q = 0.55\n    w_arc = 0.20\n    w_proc = 0.20\n    w_dist = 0.05\n    \n    raw = (w_q * queue_score) + (w_arc * arc_score) + (w_proc * proc_score) + (w_dist * dist_score)\n    \n    # Bound offsets so they refine but do not dominate learned Q-values\n    max_offset = 0.12  # slightly different magnitude\n    scale = 1.8\n    offsets = np.tanh(raw * scale) * max_offset\n    \n    # Enforce strong negative offset for missing neighbours\n    offsets = offsets.astype(float)\n    offsets[missing] = -1e6\n    \n    return offsets",
          "objective": 1.5348,
          "other_inf": null,
          "eval_metrics": {
               "avg_aoi": 0.6260577427796432,
               "avg_dropped_ratio": 0.0391566265060241
          }
     }
]