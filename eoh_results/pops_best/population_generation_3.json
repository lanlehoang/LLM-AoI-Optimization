{
     "algorithm": "A robust percentile-normalized offset combining a steep exponential reward for short arc lengths, diminishing returns for high processing rates, inverse-distance preference, and a strong sigmoid queue penalty, with tuned weights and a modest tanh scaling (scale=0.12) so offsets refine \u2014 not overpower \u2014 learned Q-values.",
     "code": "import numpy as np\n\ndef compute_offset(distances, arc_lengths, processing_rates, queue_lengths):\n    distances = np.asarray(distances, dtype=float)\n    arc_lengths = np.asarray(arc_lengths, dtype=float)\n    processing_rates = np.asarray(processing_rates, dtype=float)\n    queue_lengths = np.asarray(queue_lengths, dtype=float)\n\n    def robust_minmax(x, lowp=5.0, highp=95.0):\n        # Percentile-based min-max to reduce outlier influence\n        try:\n            p_low = np.nanpercentile(x, lowp)\n            p_high = np.nanpercentile(x, highp)\n        except IndexError:\n            p_low = np.nanmin(x)\n            p_high = np.nanmax(x)\n        rng = p_high - p_low\n        if rng < 1e-8:\n            # Fallback to global min-max\n            mn = np.nanmin(x)\n            mx = np.nanmax(x)\n            rng2 = mx - mn\n            if rng2 < 1e-8:\n                return np.zeros_like(x, dtype=float)\n            return np.clip((x - mn) / rng2, 0.0, 1.0)\n        return np.clip((x - p_low) / rng, 0.0, 1.0)\n\n    n_dist = robust_minmax(distances)\n    n_arc = robust_minmax(arc_lengths)\n    n_proc = robust_minmax(processing_rates)\n    n_queue = robust_minmax(queue_lengths)\n\n    # Transforms (all produce values roughly in [0,1])\n    s_arc = np.exp(-5.0 * n_arc)                 # strongly prefer small arc_length\n    s_proc = np.sqrt(n_proc)                     # reward higher processing rates with diminishing returns\n    s_dist = 1.0 / (1.0 + 4.0 * n_dist)          # prefer shorter physical distance (inverse-like)\n    p_queue = 1.0 / (1.0 + np.exp(-8.0 * (n_queue - 0.4)))  # steep sigmoid penalty for moderate+ queues\n\n    # Tuned weights (kept modest so offsets refine Q-values)\n    w_arc = 0.40\n    w_proc = 0.12\n    w_dist = 0.08\n    w_queue = 0.90\n\n    raw = (w_arc * s_arc) + (w_proc * s_proc) + (w_dist * s_dist) - (w_queue * p_queue)\n\n    # Remove global bias and bound to modest magnitude\n    raw_centered = raw - np.median(raw)\n    scale = 0.12\n    offsets = scale * np.tanh(raw_centered)\n\n    return offsets",
     "objective": 1.5636,
     "other_inf": null,
     "eval_metrics": {
          "avg_aoi": 0.5884834103597613,
          "avg_dropped_ratio": 0.07981927710843373
     }
}